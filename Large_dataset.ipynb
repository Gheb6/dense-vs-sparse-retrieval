{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "064d3fcb",
   "metadata": {},
   "source": [
    "## Debug: Check Available Indexing Options\n",
    "\n",
    "Let's check what indexing options are available in pyserini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6295e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available indexing options\n",
    "import subprocess\n",
    "result = subprocess.run(\n",
    "    ['python', '-m', 'pyserini.index.lucene', '-options'],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "print(\"STDOUT:\")\n",
    "print(result.stdout)\n",
    "print(\"\\nSTDERR:\")\n",
    "print(result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ad26b0",
   "metadata": {},
   "source": [
    "# Paper Replication: Dense vs Sparse Retrieval on BEIR\n",
    "\n",
    "This notebook replicates results from **Table 1** of the paper comparing:\n",
    "- **Dense**: BGE (bge-base-en-v1.5) with HNSW and Flat indexes\n",
    "- **Sparse**: SPLADE++ EnsembleDistil and BM25 baseline\n",
    "- **Metrics**: Recall@10, nDCG@10, QPS (queries per second)\n",
    "\n",
    "## Key Implementation Details\n",
    "\n",
    "**Exact Paper Parameters:**\n",
    "- Library: Lucene 9.9.1 via Pyserini/Anserini\n",
    "- HNSW: M=16, efConstruction=100, efSearch=1000\n",
    "- Threads: 16 (indexing and search)\n",
    "- Retrieval: k=1000 hits\n",
    "- Evaluation: Recall@10, nDCG@10\n",
    "- QPS: Measured with 16 threads\n",
    "\n",
    "**Datasets:** The paper evaluates 29 BEIR datasets. Change `dataset_name` below to run on different datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237a7e6c",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "Install Pyserini (Anserini Python bindings), sentence-transformers (for BGE), BEIR, and FAISS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6b09e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sentence-transformers pyserini beir faiss-cpu pandas matplotlib seaborn\n",
    "# Install Java 21 for Lucene (class version 65)\n",
    "!apt-get -y install -qq openjdk-21-jdk-headless || true\n",
    "print(\"✅ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907e5dbc",
   "metadata": {},
   "source": [
    "## 2. Setup and Imports\n",
    "\n",
    "⚠️ **IMPORTANT**: After installing dependencies, restart the runtime/kernel before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d34b3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import subprocess\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Configure Java 21 for Lucene\n",
    "java_home = \"/usr/lib/jvm/java-21-openjdk-amd64\"\n",
    "if os.path.exists(java_home):\n",
    "    os.environ[\"JAVA_HOME\"] = java_home\n",
    "    os.environ[\"PATH\"] = f\"{java_home}/bin:\" + os.environ.get(\"PATH\", \"\")\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from beir import util\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "print(\"✅ Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2fa2db",
   "metadata": {},
   "source": [
    "## 3. Dataset Selection\n",
    "\n",
    "Select a BEIR dataset. The paper evaluates 29 datasets - here we can run on any individual dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497300ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select dataset from BEIR\n",
    "dataset_name = 'scifact'  # Change to: fiqa, trec-covid, nfcorpus, etc.\n",
    "\n",
    "dataset_urls = {\n",
    "    'scifact': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/scifact.zip',\n",
    "    'nfcorpus': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/nfcorpus.zip',\n",
    "    'fiqa': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/fiqa.zip',\n",
    "    'trec-covid': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/trec-covid.zip',\n",
    "    'arguana': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/arguana.zip',\n",
    "    'webis-touche2020': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/webis-touche2020.zip',\n",
    "    'quora': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/quora.zip',\n",
    "    'dbpedia-entity': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/dbpedia-entity.zip',\n",
    "    'scidocs': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/scidocs.zip',\n",
    "    'fever': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/fever.zip',\n",
    "    'climate-fever': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/climate-fever.zip',\n",
    "    'nq': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/nq.zip',\n",
    "}\n",
    "\n",
    "print(f\"Downloading {dataset_name} dataset...\")\n",
    "url = dataset_urls[dataset_name]\n",
    "data_path = util.download_and_unzip(url, \"datasets\")\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "corpus, queries, qrels = GenericDataLoader(data_folder=data_path).load(split=\"test\")\n",
    "\n",
    "doc_ids = list(corpus.keys())\n",
    "doc_texts = [corpus[did]['title'] + ' ' + corpus[did]['text'] for did in doc_ids]\n",
    "query_ids = list(queries.keys())\n",
    "query_texts = [queries[qid] for qid in query_ids]\n",
    "\n",
    "print(f\"\\n✅ Dataset: {dataset_name}\")\n",
    "print(f\"   Documents: {len(corpus):,}\")\n",
    "print(f\"   Queries: {len(queries):,}\")\n",
    "print(f\"   Relevance judgments: {len(qrels):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daef149",
   "metadata": {},
   "source": [
    "## 4. Dense Retrieval: BGE Model\n",
    "\n",
    "Load BGE (bge-base-en-v1.5) and encode documents and queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41371ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BGE model (bge-base-en-v1.5)\n",
    "model_name = 'BAAI/bge-base-en-v1.5'\n",
    "print(f\"Loading BGE model: {model_name}\")\n",
    "model = SentenceTransformer(model_name)\n",
    "dimension = model.get_sentence_embedding_dimension()\n",
    "print(f\"✅ Model loaded (dimension={dimension})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9cdf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode documents\n",
    "batch_size = 32 if len(doc_texts) <= 100_000 else 16\n",
    "print(f\"Encoding {len(doc_texts):,} documents (batch_size={batch_size})...\")\n",
    "\n",
    "doc_embeddings = model.encode(\n",
    "    doc_texts,\n",
    "    batch_size=batch_size,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True\n",
    ")\n",
    "\n",
    "print(f\"✅ Documents encoded: {doc_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce806da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode queries\n",
    "print(f\"Encoding {len(query_texts):,} queries...\")\n",
    "query_embeddings = model.encode(\n",
    "    query_texts,\n",
    "    batch_size=32,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True\n",
    ")\n",
    "\n",
    "print(f\"✅ Queries encoded: {query_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed83c88d",
   "metadata": {},
   "source": [
    "## 5. Build Lucene Indexes\n",
    "\n",
    "Build indexes for all retrieval methods:\n",
    "1. **BM25**: Inverted index\n",
    "2. **SPLADE++ ED**: Impact-based inverted index\n",
    "3. **BGE HNSW**: HNSW vector index (M=16, efC=100, efSearch=1000)\n",
    "4. **BGE Flat**: Flat vector index (brute-force search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd68661f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paper parameters\n",
    "M = 16  # HNSW M parameter\n",
    "ef_construction = 100  # HNSW efC\n",
    "ef_search = 1000  # HNSW efSearch\n",
    "threads = '16'  # 16 threads as per paper\n",
    "k_retrieve = 1000  # Retrieve 1000 hits\n",
    "k_eval = 10  # Evaluate at nDCG@10\n",
    "\n",
    "print(f\"Parameters: M={M}, efC={ef_construction}, efSearch={ef_search}, threads={threads}\")\n",
    "print(f\"Retrieval: k={k_retrieve}, evaluation@{k_eval}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c298789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare directory structure\n",
    "base_dir = f'indexes_{dataset_name}'\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "# 1. BM25 Index\n",
    "bm25_docs_dir = os.path.join(base_dir, 'bm25_docs')\n",
    "bm25_index_dir = os.path.join(base_dir, 'bm25_index')\n",
    "os.makedirs(bm25_docs_dir, exist_ok=True)\n",
    "\n",
    "print(\"Writing BM25 documents...\")\n",
    "bm25_jsonl = os.path.join(bm25_docs_dir, 'docs.jsonl')\n",
    "with open(bm25_jsonl, 'w', encoding='utf-8') as f:\n",
    "    for did, text in zip(doc_ids, doc_texts):\n",
    "        f.write(json.dumps({'id': did, 'contents': text}) + \"\\n\")\n",
    "\n",
    "print(\"Building BM25 index...\")\n",
    "subprocess.run([\n",
    "    'python', '-m', 'pyserini.index.lucene',\n",
    "    '--collection', 'JsonCollection',\n",
    "    '--input', bm25_docs_dir,\n",
    "    '--index', bm25_index_dir,\n",
    "    '--generator', 'DefaultLuceneDocumentGenerator',\n",
    "    '--threads', threads,\n",
    "    '--storePositions',\n",
    "    '--storeDocvectors',\n",
    "    '--storeRaw'\n",
    "], check=True)\n",
    "print(\"✅ BM25 index ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2802abe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. SPLADE++ ED Index\n",
    "splade_docs_dir = os.path.join(base_dir, 'splade_docs')\n",
    "splade_encoded_dir = os.path.join(base_dir, 'splade_encoded')\n",
    "splade_index_dir = os.path.join(base_dir, 'splade_index')\n",
    "os.makedirs(splade_docs_dir, exist_ok=True)\n",
    "os.makedirs(splade_encoded_dir, exist_ok=True)\n",
    "\n",
    "print(\"Writing SPLADE documents...\")\n",
    "splade_jsonl = os.path.join(splade_docs_dir, 'docs.jsonl')\n",
    "with open(splade_jsonl, 'w', encoding='utf-8') as f:\n",
    "    for did, text in zip(doc_ids, doc_texts):\n",
    "        f.write(json.dumps({'id': did, 'text': text}) + \"\\n\")\n",
    "\n",
    "print(\"Encoding with SPLADE++ EnsembleDistil (using GPU)...\")\n",
    "subprocess.run([\n",
    "    'python', '-m', 'pyserini.encode',\n",
    "    'input', '--corpus', splade_docs_dir,\n",
    "    '--fields', 'text',\n",
    "    'output', '--embeddings', splade_encoded_dir,\n",
    "    'encoder', '--encoder', 'naver/splade-cocondenser-ensembledistil',\n",
    "    '--device', 'cuda',\n",
    "    '--batch', '32'\n",
    "], check=True)\n",
    "\n",
    "print(\"Building SPLADE impact index...\")\n",
    "subprocess.run([\n",
    "    'python', '-m', 'pyserini.index.lucene',\n",
    "    '--collection', 'JsonVectorCollection',\n",
    "    '--input', splade_encoded_dir,\n",
    "    '--index', splade_index_dir,\n",
    "    '--generator', 'DefaultLuceneDocumentGenerator',\n",
    "    '--impact',\n",
    "    '--threads', threads,\n",
    "    '--storeRaw'\n",
    "], check=True)\n",
    "print(\"✅ SPLADE++ ED index ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e880228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. BGE HNSW Index (using FAISS)\n",
    "import faiss\n",
    "\n",
    "hnsw_index_path = os.path.join(base_dir, 'hnsw_index.faiss')\n",
    "\n",
    "print(f\"Building FAISS HNSW index (M={M}, efC={ef_construction}, efSearch={ef_search})...\")\n",
    "\n",
    "# Create HNSW index\n",
    "quantizer = faiss.IndexFlatIP(dimension)  # Inner product for cosine similarity (normalized vectors)\n",
    "hnsw_index = faiss.IndexHNSWFlat(dimension, M, faiss.METRIC_INNER_PRODUCT)\n",
    "hnsw_index.hnsw.efConstruction = ef_construction\n",
    "hnsw_index.hnsw.efSearch = ef_search\n",
    "\n",
    "# Add vectors to index\n",
    "print(f\"Adding {len(doc_embeddings):,} vectors to HNSW index...\")\n",
    "hnsw_index.add(doc_embeddings)\n",
    "\n",
    "# Save index\n",
    "faiss.write_index(hnsw_index, hnsw_index_path)\n",
    "print(f\"✅ HNSW index saved ({hnsw_index.ntotal:,} vectors)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd34c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. BGE Flat Index (using FAISS, brute-force search)\n",
    "flat_index_path = os.path.join(base_dir, 'flat_index.faiss')\n",
    "\n",
    "print(\"Building FAISS Flat index (brute-force)...\")\n",
    "\n",
    "# Create flat index for exact search\n",
    "flat_index = faiss.IndexFlatIP(dimension)  # Inner product for cosine similarity\n",
    "flat_index.add(doc_embeddings)\n",
    "\n",
    "# Save index\n",
    "faiss.write_index(flat_index, flat_index_path)\n",
    "print(f\"✅ Flat index saved ({flat_index.ntotal:,} vectors)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92266d2",
   "metadata": {},
   "source": [
    "## 6. Initialize Searchers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de01564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM25 searcher\n",
    "bm25_searcher = LuceneSearcher(bm25_index_dir)\n",
    "bm25_searcher.set_bm25(k1=0.9, b=0.4)\n",
    "\n",
    "# SPLADE searcher (built-in SPLADE query encoding)\n",
    "from pyserini.search.lucene import LuceneImpactSearcher\n",
    "\n",
    "# Initialize with encoder string - searcher will load and use the model internally\n",
    "splade_searcher = LuceneImpactSearcher(\n",
    "    splade_index_dir,\n",
    "    'naver/splade-cocondenser-ensembledistil',  # Model name as string\n",
    "    encoder_type='pytorch',  # Use PyTorch model\n",
    "    #device='cuda'\n",
    ")\n",
    "\n",
    "# Load FAISS indexes for dense retrieval\n",
    "import faiss\n",
    "hnsw_index_path = os.path.join(base_dir, 'hnsw_index.faiss')\n",
    "flat_index_path = os.path.join(base_dir, 'flat_index.faiss')\n",
    "\n",
    "hnsw_index = faiss.read_index(hnsw_index_path)\n",
    "flat_index = faiss.read_index(flat_index_path)\n",
    "\n",
    "print(f\"✅ All searchers initialized\")\n",
    "print(f\"   HNSW index: {hnsw_index.ntotal:,} vectors\")\n",
    "print(f\"   Flat index: {flat_index.ntotal:,} vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebce6ead",
   "metadata": {},
   "source": [
    "## 7. Search Functions\n",
    "\n",
    "Implement search with QPS measurement (16 threads)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b79f732",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id_to_idx = {did: i for i, did in enumerate(doc_ids)}\n",
    "\n",
    "def search_bm25(searcher, query_texts, k=1000):\n",
    "    \"\"\"BM25 search\"\"\"\n",
    "    all_indices = []\n",
    "    all_scores = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for q in tqdm(query_texts, desc=\"BM25 search\"):\n",
    "        hits = searcher.search(q, k)\n",
    "        docids = [h.docid for h in hits]\n",
    "        scores = [h.score for h in hits]\n",
    "        all_indices.append([doc_id_to_idx[d] for d in docids])\n",
    "        all_scores.append(scores)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    qps = len(query_texts) / elapsed\n",
    "    \n",
    "    return {\n",
    "        'name': 'BM25',\n",
    "        'indices': np.array(all_indices, dtype=object),\n",
    "        'scores': np.array(all_scores, dtype=object),\n",
    "        'qps': qps\n",
    "    }\n",
    "\n",
    "def search_splade(searcher, query_texts, k=1000):\n",
    "    \"\"\"SPLADE++ ED search\"\"\"\n",
    "    all_indices = []\n",
    "    all_scores = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for q in tqdm(query_texts, desc=\"SPLADE++ ED search\"):\n",
    "        hits = searcher.search(q, k)\n",
    "        docids = [h.docid for h in hits]\n",
    "        scores = [h.score for h in hits]\n",
    "        all_indices.append([doc_id_to_idx[d] for d in docids])\n",
    "        all_scores.append(scores)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    qps = len(query_texts) / elapsed\n",
    "    \n",
    "    return {\n",
    "        'name': 'SPLADE++ ED',\n",
    "        'indices': np.array(all_indices, dtype=object),\n",
    "        'scores': np.array(all_scores, dtype=object),\n",
    "        'qps': qps\n",
    "    }\n",
    "\n",
    "def search_dense(faiss_index, query_embeddings, name, k=1000):\n",
    "    \"\"\"Dense retrieval with FAISS (HNSW or Flat)\"\"\"\n",
    "    all_indices = []\n",
    "    all_scores = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for emb in tqdm(query_embeddings, desc=f\"{name} search\"):\n",
    "        # FAISS search returns (distances, indices)\n",
    "        scores, indices = faiss_index.search(emb.reshape(1, -1), k)\n",
    "        all_indices.append(indices[0].tolist())\n",
    "        all_scores.append(scores[0].tolist())\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    qps = len(query_embeddings) / elapsed\n",
    "    \n",
    "    return {\n",
    "        'name': name,\n",
    "        'indices': np.array(all_indices, dtype=object),\n",
    "        'scores': np.array(all_scores, dtype=object),\n",
    "        'qps': qps\n",
    "    }\n",
    "\n",
    "print(\"✅ Search functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd984c41",
   "metadata": {},
   "source": [
    "## 8. Run All Searches\n",
    "\n",
    "Retrieve 1000 hits per query using 16 threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e358b426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all searches\n",
    "results_bm25 = search_bm25(bm25_searcher, query_texts, k=k_retrieve)\n",
    "results_splade = search_splade(splade_searcher, query_texts, k=k_retrieve)\n",
    "results_hnsw = search_dense(hnsw_index, query_embeddings, 'BGE-HNSW', k=k_retrieve)\n",
    "results_flat = search_dense(flat_index, query_embeddings, 'BGE-Flat', k=k_retrieve)\n",
    "\n",
    "print(\"\\n✅ All searches complete\")\n",
    "print(f\"   BM25: {results_bm25['qps']:.2f} QPS\")\n",
    "print(f\"   SPLADE++ ED: {results_splade['qps']:.2f} QPS\")\n",
    "print(f\"   BGE-HNSW: {results_hnsw['qps']:.2f} QPS\")\n",
    "print(f\"   BGE-Flat: {results_flat['qps']:.2f} QPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50278285",
   "metadata": {},
   "source": [
    "## 9. Evaluation at nDCG@10\n",
    "\n",
    "Evaluate retrieval quality using nDCG@10 as per BEIR guidelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a719140e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall_at_k(retrieved_indices, qrels, query_ids, doc_ids, k=10):\n",
    "    \"\"\"Calculate Recall@k following BEIR guidelines\"\"\"\n",
    "    recalls = []\n",
    "    \n",
    "    for i, qid in enumerate(query_ids):\n",
    "        if qid not in qrels:\n",
    "            continue\n",
    "        \n",
    "        relevant_docs = set(qrels[qid].keys())\n",
    "        retrieved_docs = set([doc_ids[idx] for idx in retrieved_indices[i][:k] if idx >= 0])\n",
    "        \n",
    "        if len(relevant_docs) > 0:\n",
    "            recalls.append(len(relevant_docs & retrieved_docs) / len(relevant_docs))\n",
    "    \n",
    "    return np.mean(recalls) if recalls else 0.0\n",
    "\n",
    "def calculate_ndcg_at_k(retrieved_indices, qrels, query_ids, doc_ids, k=10):\n",
    "    \"\"\"Calculate nDCG@k following BEIR guidelines\"\"\"\n",
    "    ndcgs = []\n",
    "    \n",
    "    for i, qid in enumerate(query_ids):\n",
    "        if qid not in qrels:\n",
    "            continue\n",
    "        \n",
    "        relevant_docs = qrels[qid]\n",
    "        retrieved_docs = [doc_ids[idx] for idx in retrieved_indices[i][:k] if idx >= 0]\n",
    "        \n",
    "        # Calculate DCG\n",
    "        dcg = 0\n",
    "        for rank, doc_id in enumerate(retrieved_docs, 1):\n",
    "            rel = relevant_docs.get(doc_id, 0)\n",
    "            dcg += (2 ** rel - 1) / np.log2(rank + 1)\n",
    "        \n",
    "        # Calculate IDCG\n",
    "        ideal = sorted(relevant_docs.values(), reverse=True)[:k]\n",
    "        idcg = sum((2 ** r - 1) / np.log2(rank + 2) for rank, r in enumerate(ideal))\n",
    "        \n",
    "        ndcgs.append(dcg / idcg if idcg > 0 else 0)\n",
    "    \n",
    "    return np.mean(ndcgs) if ndcgs else 0.0\n",
    "\n",
    "# Evaluate all methods with both metrics\n",
    "for results in [results_bm25, results_splade, results_hnsw, results_flat]:\n",
    "    results['recall@10'] = calculate_recall_at_k(\n",
    "        results['indices'], qrels, query_ids, doc_ids, k=k_eval\n",
    "    )\n",
    "    results['ndcg@10'] = calculate_ndcg_at_k(\n",
    "        results['indices'], qrels, query_ids, doc_ids, k=k_eval\n",
    "    )\n",
    "\n",
    "print(\"✅ Evaluation complete (Recall@10 and nDCG@10)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fadaacf",
   "metadata": {},
   "source": [
    "## 10. Results Summary\n",
    "\n",
    "Display results in a table matching the paper format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1015379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results dataframe matching paper table format\n",
    "results_df = pd.DataFrame([\n",
    "    {\n",
    "        'Method': 'BM25',\n",
    "        'Type': 'Sparse (Baseline)',\n",
    "        'Recall@10': results_bm25['recall@10'],\n",
    "        'nDCG@10': results_bm25['ndcg@10'],\n",
    "        'QPS': results_bm25['qps'],\n",
    "    },\n",
    "    {\n",
    "        'Method': 'SPLADE++ ED',\n",
    "        'Type': 'Sparse (Learned)',\n",
    "        'Recall@10': results_splade['recall@10'],\n",
    "        'nDCG@10': results_splade['ndcg@10'],\n",
    "        'QPS': results_splade['qps'],\n",
    "    },\n",
    "    {\n",
    "        'Method': 'BGE-HNSW',\n",
    "        'Type': 'Dense (HNSW)',\n",
    "        'Recall@10': results_hnsw['recall@10'],\n",
    "        'nDCG@10': results_hnsw['ndcg@10'],\n",
    "        'QPS': results_hnsw['qps'],\n",
    "    },\n",
    "    {\n",
    "        'Method': 'BGE-Flat',\n",
    "        'Type': 'Dense (Flat)',\n",
    "        'Recall@10': results_flat['recall@10'],\n",
    "        'nDCG@10': results_flat['ndcg@10'],\n",
    "        'QPS': results_flat['qps'],\n",
    "    },\n",
    "])\n",
    "\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(f\"RESULTS: {dataset_name.upper()}\")\n",
    "print(f\"{'='*90}\")\n",
    "print(results_df.to_string(index=False))\n",
    "print(f\"{'='*90}\")\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  Name: {dataset_name}\")\n",
    "print(f\"  Documents (|C|): {len(corpus):,}\")\n",
    "print(f\"  Queries (|Q|): {len(queries):,}\")\n",
    "print(f\"  Relevance judgments: {len(qrels):,}\")\n",
    "print(f\"\\nIndexing Parameters:\")\n",
    "print(f\"  HNSW: M={M}, efC={ef_construction}, efSearch={ef_search}\")\n",
    "print(f\"  Threads: {threads}\")\n",
    "print(f\"\\nRetrieval & Evaluation:\")\n",
    "print(f\"  Retrieved: k={k_retrieve}\")\n",
    "print(f\"  Evaluated: Recall@{k_eval}, nDCG@{k_eval}\")\n",
    "print(f\"  QPS measured with {threads} threads\")\n",
    "print(f\"{'='*90}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab357754",
   "metadata": {},
   "source": [
    "## 11. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b65ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations matching paper analysis\n",
    "output_dir = f'results_{dataset_name}'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "colors = {'Sparse (Baseline)': 'orange', 'Sparse (Learned)': 'red', \n",
    "          'Dense (HNSW)': 'steelblue', 'Dense (Flat)': 'lightblue'}\n",
    "\n",
    "# Plot 1: Speed (QPS) vs Quality (nDCG@10)\n",
    "for _, row in results_df.iterrows():\n",
    "    ax1.scatter(row['QPS'], row['nDCG@10'], \n",
    "              s=200, alpha=0.7, color=colors[row['Type']], \n",
    "              edgecolors='black', linewidth=1.5)\n",
    "    ax1.annotate(row['Method'], \n",
    "               (row['QPS'], row['nDCG@10']), \n",
    "               xytext=(8, 8), textcoords='offset points', \n",
    "               fontsize=10, fontweight='bold')\n",
    "\n",
    "ax1.set_xlabel('QPS (queries per second, 16 threads)', fontsize=11)\n",
    "ax1.set_ylabel('nDCG@10', fontsize=11)\n",
    "ax1.set_title(f'Speed vs Quality (nDCG@10) — {dataset_name}', fontsize=12, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Speed (QPS) vs Quality (Recall@10)\n",
    "for _, row in results_df.iterrows():\n",
    "    ax2.scatter(row['QPS'], row['Recall@10'], \n",
    "              s=200, alpha=0.7, color=colors[row['Type']], \n",
    "              edgecolors='black', linewidth=1.5)\n",
    "    ax2.annotate(row['Method'], \n",
    "               (row['QPS'], row['Recall@10']), \n",
    "               xytext=(8, 8), textcoords='offset points', \n",
    "               fontsize=10, fontweight='bold')\n",
    "\n",
    "ax2.set_xlabel('QPS (queries per second, 16 threads)', fontsize=11)\n",
    "ax2.set_ylabel('Recall@10', fontsize=11)\n",
    "ax2.set_title(f'Speed vs Quality (Recall@10) — {dataset_name}', fontsize=12, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/speed_vs_quality.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Bar chart comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Quality metrics\n",
    "x = np.arange(len(results_df))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax1.bar(x - width/2, results_df['Recall@10'], width, label='Recall@10', alpha=0.8)\n",
    "bars2 = ax1.bar(x + width/2, results_df['nDCG@10'], width, label='nDCG@10', alpha=0.8)\n",
    "\n",
    "ax1.set_xlabel('Method', fontsize=11)\n",
    "ax1.set_ylabel('Score', fontsize=11)\n",
    "ax1.set_title(f'Quality Metrics Comparison — {dataset_name}', fontsize=12, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(results_df['Method'], rotation=15, ha='right')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# QPS comparison\n",
    "bars = ax2.bar(results_df['Method'], results_df['QPS'], alpha=0.8, \n",
    "               color=[colors[t] for t in results_df['Type']], edgecolor='black')\n",
    "ax2.set_xlabel('Method', fontsize=11)\n",
    "ax2.set_ylabel('QPS (16 threads)', fontsize=11)\n",
    "ax2.set_title(f'Query Performance — {dataset_name}', fontsize=12, fontweight='bold')\n",
    "ax2.set_xticklabels(results_df['Method'], rotation=15, ha='right')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/metrics_comparison.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✅ Visualizations complete and saved to {output_dir}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f8d6ff",
   "metadata": {},
   "source": [
    "## 12. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e2ee12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "output_dir = f'results_{dataset_name}'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "results_path = os.path.join(output_dir, f'{dataset_name}_results.csv')\n",
    "results_df.to_csv(results_path, index=False)\n",
    "\n",
    "# Save detailed results with metadata\n",
    "metadata = {\n",
    "    'dataset': dataset_name,\n",
    "    'num_documents': len(corpus),\n",
    "    'num_queries': len(queries),\n",
    "    'num_qrels': len(qrels),\n",
    "    'hnsw_M': M,\n",
    "    'hnsw_efC': ef_construction,\n",
    "    'hnsw_efSearch': ef_search,\n",
    "    'threads': threads,\n",
    "    'k_retrieve': k_retrieve,\n",
    "    'k_eval': k_eval,\n",
    "}\n",
    "\n",
    "metadata_path = os.path.join(output_dir, f'{dataset_name}_metadata.json')\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"✅ Results saved:\")\n",
    "print(f\"   - {results_path}\")\n",
    "print(f\"   - {metadata_path}\")\n",
    "print(f\"   - {output_dir}/speed_vs_quality.pdf\")\n",
    "print(f\"   - {output_dir}/metrics_comparison.pdf\")\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"COMPARISON WITH PAPER TABLE (table-main.tex):\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Your results can now be compared with the paper's Table 1.\")\n",
    "print(f\"\\nTo replicate full paper results across all 29 BEIR datasets:\")\n",
    "print(f\"  1. Change 'dataset_name' in the third cell\")\n",
    "print(f\"  2. Run all cells for each dataset\")\n",
    "print(f\"  3. Compile results from all datasets\")\n",
    "print(f\"\\nNote: QPS values may differ from paper due to:\")\n",
    "print(f\"  - Hardware differences (paper used Mac Studio M1 Ultra)\")\n",
    "print(f\"  - ONNX optimizations (paper reports 'QPS (ONNX)')\")\n",
    "print(f\"  - Caching effects (paper reports 'QPS (cached)')\")\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

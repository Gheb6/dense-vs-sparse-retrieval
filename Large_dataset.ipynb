{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "064d3fcb",
   "metadata": {},
   "source": [
    "## Debug: Check Available Indexing Options\n",
    "\n",
    "Let's check what indexing options are available in pyserini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6295e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check available indexing options\n",
    "# import subprocess\n",
    "# result = subprocess.run(\n",
    "#     ['python', '-m', 'pyserini.index.lucene', '-options'],\n",
    "#     capture_output=True,\n",
    "#     text=True\n",
    "# )\n",
    "# print(\"STDOUT:\")\n",
    "# print(result.stdout)\n",
    "# print(\"\\nSTDERR:\")\n",
    "# print(result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ad26b0",
   "metadata": {},
   "source": [
    "# Paper Replication: Dense vs Sparse Retrieval on BEIR\n",
    "\n",
    "This notebook replicates results from **Table 1** of the paper comparing:\n",
    "- **Dense**: BGE (bge-base-en-v1.5) with HNSW and Flat indexes\n",
    "- **Sparse**: SPLADE++ EnsembleDistil and BM25 baseline\n",
    "- **Metrics**: Recall@10, nDCG@10, QPS (queries per second)\n",
    "\n",
    "## Key Implementation Details\n",
    "\n",
    "**Exact Paper Parameters:**\n",
    "- Library: Lucene 9.9.1 via Pyserini/Anserini\n",
    "- HNSW: M=16, efConstruction=100, efSearch=1000\n",
    "- Threads: 16 (indexing and search)\n",
    "- Retrieval: k=1000 hits\n",
    "- Evaluation: Recall@10, nDCG@10\n",
    "- QPS: Measured with 16 threads\n",
    "\n",
    "**Datasets:** The paper evaluates 29 BEIR datasets. Change `dataset_name` below to run on different datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237a7e6c",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "Install Pyserini (Anserini Python bindings), sentence-transformers (for BGE), BEIR, and FAISS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6b09e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sentence-transformers pyserini beir faiss-cpu pandas matplotlib seaborn\n",
    "# Install Java 21 for Lucene (class version 65)\n",
    "!apt-get -y install -qq openjdk-21-jdk-headless || true\n",
    "print(\"✅ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907e5dbc",
   "metadata": {},
   "source": [
    "## 2. Setup and Imports\n",
    "\n",
    "⚠️ **IMPORTANT**: After installing dependencies, restart the runtime/kernel before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d34b3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import subprocess\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Configure Java 21 for Lucene\n",
    "java_home = \"/usr/lib/jvm/java-21-openjdk-amd64\"\n",
    "if os.path.exists(java_home):\n",
    "    os.environ[\"JAVA_HOME\"] = java_home\n",
    "    os.environ[\"PATH\"] = f\"{java_home}/bin:\" + os.environ.get(\"PATH\", \"\")\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from beir import util\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "print(\"✅ Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2fa2db",
   "metadata": {},
   "source": [
    "## 3. Dataset Selection\n",
    "\n",
    "Select a BEIR dataset. The paper evaluates 29 datasets - here we can run on any individual dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ec73f0",
   "metadata": {},
   "source": [
    "### Known Dataset Issues\n",
    "\n",
    "**⚠️ Some datasets have restricted access:**\n",
    "The most reliable publicly available datasets are: nfcorpus, scifact, arguana, scidocs, fiqa, trec-covid, webis-touche2020, nq, fever, climate-fever.\n",
    "\n",
    "Run the test cell above to check current availability before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497300ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select dataset from BEIR\n",
    "dataset_name = 'scifact'  # Change to: fiqa, trec-covid, nfcorpus, etc.\n",
    "\n",
    "# BEIR dataset URLs sorted by corpus cardinality (smallest to largest)\n",
    "dataset_urls = {\n",
    "    'nfcorpus': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/nfcorpus.zip',  # 3.6K docs\n",
    "    'scifact': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/scifact.zip',  # 5K docs\n",
    "    'arguana': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/arguana.zip',  # 8.7K docs\n",
    "    'scidocs': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/scidocs.zip',  # 25K docs\n",
    "    'fiqa': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/fiqa.zip',  # 57K docs\n",
    "    'trec-covid': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/trec-covid.zip',  # 171K docs\n",
    "    'webis-touche2020': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/webis-touche2020.zip',  # 382K docs\n",
    "    'quora': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/quora.zip',  # 523K docs\n",
    "    'robust04': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/robust04.zip',  # 528K docs\n",
    "    'trec-news': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/trec-news.zip',  # 595K docs\n",
    "    'nq': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/nq.zip',  # 2.7M docs\n",
    "    'dbpedia-entity': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/dbpedia-entity.zip',  # 4.6M docs\n",
    "    'fever': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/fever.zip',  # 5.4M docs\n",
    "    'climate-fever': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/climate-fever.zip',  # 5.4M docs\n",
    "}\n",
    "\n",
    "print(f\"Downloading {dataset_name} dataset...\")\n",
    "url = dataset_urls[dataset_name]\n",
    "data_path = util.download_and_unzip(url, \"datasets\")\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "corpus, queries, qrels = GenericDataLoader(data_folder=data_path).load(split=\"test\")\n",
    "\n",
    "doc_ids = list(corpus.keys())\n",
    "doc_texts = [corpus[did]['title'] + ' ' + corpus[did]['text'] for did in doc_ids]\n",
    "query_ids = list(queries.keys())\n",
    "query_texts = [queries[qid] for qid in query_ids]\n",
    "\n",
    "print(f\"\\n✅ Dataset: {dataset_name}\")\n",
    "print(f\"   Documents: {len(corpus):,}\")\n",
    "print(f\"   Queries: {len(queries):,}\")\n",
    "print(f\"   Relevance judgments: {len(qrels):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daef149",
   "metadata": {},
   "source": [
    "## 4. Dense Retrieval: BGE Model\n",
    "\n",
    "Load BGE (bge-base-en-v1.5) and encode documents and queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41371ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BGE model (bge-base-en-v1.5)\n",
    "model_name = 'BAAI/bge-base-en-v1.5'\n",
    "print(f\"Loading BGE model: {model_name}\")\n",
    "model = SentenceTransformer(model_name)\n",
    "dimension = model.get_sentence_embedding_dimension()\n",
    "print(f\"✅ Model loaded (dimension={dimension})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9cdf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode documents\n",
    "batch_size = 32 if len(doc_texts) <= 100_000 else 16\n",
    "print(f\"Encoding {len(doc_texts):,} documents (batch_size={batch_size})...\")\n",
    "\n",
    "doc_embeddings = model.encode(\n",
    "    doc_texts,\n",
    "    batch_size=batch_size,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True\n",
    ")\n",
    "\n",
    "print(f\"✅ Documents encoded: {doc_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce806da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode queries\n",
    "print(f\"Encoding {len(query_texts):,} queries...\")\n",
    "query_embeddings = model.encode(\n",
    "    query_texts,\n",
    "    batch_size=32,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True\n",
    ")\n",
    "\n",
    "print(f\"✅ Queries encoded: {query_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed83c88d",
   "metadata": {},
   "source": [
    "## 5. Build Lucene Indexes\n",
    "\n",
    "Build indexes for all retrieval methods:\n",
    "1. **BM25**: Inverted index\n",
    "2. **SPLADE++ ED**: Impact-based inverted index\n",
    "3. **BGE HNSW**: HNSW vector index (M=16, efC=100, efSearch=1000)\n",
    "4. **BGE Flat**: Flat vector index (brute-force search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd68661f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paper parameters\n",
    "M = 16  # HNSW M parameter\n",
    "ef_construction = 100  # HNSW efC\n",
    "ef_search = 1000  # HNSW efSearch\n",
    "threads = '16'  # 16 threads as per paper\n",
    "k_retrieve = 1000  # Retrieve 1000 hits\n",
    "k_eval = 10  # Evaluate at nDCG@10\n",
    "\n",
    "# Initialize index timing dictionary\n",
    "index_times = {}\n",
    "\n",
    "print(f\"Retrieval: k={k_retrieve}, evaluation@{k_eval}\")\n",
    "print(f\"Parameters: M={M}, efC={ef_construction}, efSearch={ef_search}, threads={threads}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c298789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare directory structure\n",
    "base_dir = f'indexes_{dataset_name}'\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "# 1. BM25 Index\n",
    "bm25_docs_dir = os.path.join(base_dir, 'bm25_docs')\n",
    "bm25_index_dir = os.path.join(base_dir, 'bm25_index')\n",
    "os.makedirs(bm25_docs_dir, exist_ok=True)\n",
    "\n",
    "print(\"Writing BM25 documents...\")\n",
    "bm25_jsonl = os.path.join(bm25_docs_dir, 'docs.jsonl')\n",
    "with open(bm25_jsonl, 'w', encoding='utf-8') as f:\n",
    "    for did, text in zip(doc_ids, doc_texts):\n",
    "        f.write(json.dumps({'id': did, 'contents': text}) + \"\\n\")\n",
    "\n",
    "print(\"Building BM25 index...\")\n",
    "bm25_start = time.time()\n",
    "subprocess.run([\n",
    "    'python', '-m', 'pyserini.index.lucene',\n",
    "    '--collection', 'JsonCollection',\n",
    "    '--input', bm25_docs_dir,\n",
    "    '--index', bm25_index_dir,\n",
    "    '--generator', 'DefaultLuceneDocumentGenerator',\n",
    "    '--threads', threads,\n",
    "    '--storePositions',\n",
    "    '--storeDocvectors',\n",
    "    '--storeRaw'\n",
    "], check=True)\n",
    "\n",
    "bm25_elapsed = time.time() - bm25_start\n",
    "print(f\"✅ BM25 index ready ({bm25_elapsed:.2f}s)\")\n",
    "index_times['BM25'] = bm25_elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2802abe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. SPLADE++ Index Configuration\n",
    "# ========================================\n",
    "# Choose SPLADE model variant (uncomment ONE of the following):\n",
    "# ========================================\n",
    "\n",
    "# Option 1: Ensemble Distil (default in paper, but updated June 2025)\n",
    "SPLADE_MODEL = 'naver/splade-cocondenser-ensembledistil'\n",
    "\n",
    "# Option 2: Self Distil (alternative training method)\n",
    "# SPLADE_MODEL = 'naver/splade-cocondenser-selfdistil'\n",
    "\n",
    "# Option 3: v2 Distil (older version, may match paper better)\n",
    "# SPLADE_MODEL = 'naver/splade_v2_distil'\n",
    "\n",
    "# ========================================\n",
    "print(f\"Selected SPLADE model: {SPLADE_MODEL}\")\n",
    "print(f\"Note: To change model, uncomment a different option above\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "splade_docs_dir = os.path.join(base_dir, 'splade_docs')\n",
    "splade_encoded_dir = os.path.join(base_dir, 'splade_encoded')\n",
    "splade_index_dir = os.path.join(base_dir, 'splade_index')\n",
    "os.makedirs(splade_docs_dir, exist_ok=True)\n",
    "os.makedirs(splade_encoded_dir, exist_ok=True)\n",
    "\n",
    "print(\"Writing SPLADE documents...\")\n",
    "splade_jsonl = os.path.join(splade_docs_dir, 'docs.jsonl')\n",
    "with open(splade_jsonl, 'w', encoding='utf-8') as f:\n",
    "    for did, text in zip(doc_ids, doc_texts):\n",
    "        f.write(json.dumps({'id': did, 'text': text}) + \"\\n\")\n",
    "\n",
    "print(f\"Encoding with {SPLADE_MODEL} (using GPU)...\")\n",
    "subprocess.run([\n",
    "    'python', '-m', 'pyserini.encode',\n",
    "    'input', '--corpus', splade_docs_dir,\n",
    "    '--fields', 'text',\n",
    "    'output', '--embeddings', splade_encoded_dir,\n",
    "    'encoder', '--encoder', SPLADE_MODEL,\n",
    "    '--device', 'cuda',\n",
    "    '--batch', '32'\n",
    "], check=True)\n",
    "\n",
    "print(\"Building SPLADE impact index...\")\n",
    "splade_start = time.time()\n",
    "subprocess.run([\n",
    "    'python', '-m', 'pyserini.index.lucene',\n",
    "    '--collection', 'JsonVectorCollection',\n",
    "    '--input', splade_encoded_dir,\n",
    "    '--index', splade_index_dir,\n",
    "    '--generator', 'DefaultLuceneDocumentGenerator',\n",
    "    '--impact',\n",
    "    '--threads', threads,\n",
    "    '--storeRaw'\n",
    "], check=True)\n",
    "\n",
    "splade_elapsed = time.time() - splade_start\n",
    "print(f\"✅ SPLADE++ index ready ({splade_elapsed:.2f}s)\")\n",
    "index_times['SPLADE++ ED'] = splade_elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e880228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. BGE HNSW Index (using FAISS)\n",
    "import faiss\n",
    "\n",
    "hnsw_index_path = os.path.join(base_dir, 'hnsw_index.faiss')\n",
    "\n",
    "print(f\"Building FAISS HNSW index (M={M}, efC={ef_construction}, efSearch={ef_search})...\")\n",
    "hnsw_start = time.time()\n",
    "\n",
    "# Create HNSW index\n",
    "quantizer = faiss.IndexFlatIP(dimension)  # Inner product for cosine similarity (normalized vectors)\n",
    "hnsw_index = faiss.IndexHNSWFlat(dimension, M, faiss.METRIC_INNER_PRODUCT)\n",
    "hnsw_index.hnsw.efConstruction = ef_construction\n",
    "hnsw_index.hnsw.efSearch = ef_search\n",
    "\n",
    "# Add vectors to index\n",
    "print(f\"Adding {len(doc_embeddings):,} vectors to HNSW index...\")\n",
    "hnsw_index.add(doc_embeddings)\n",
    "\n",
    "# Save index\n",
    "faiss.write_index(hnsw_index, hnsw_index_path)\n",
    "hnsw_elapsed = time.time() - hnsw_start\n",
    "\n",
    "index_times['BGE-HNSW'] = hnsw_elapsed\n",
    "print(f\"✅ HNSW index saved ({hnsw_index.ntotal:,} vectors) ({hnsw_elapsed:.2f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd34c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. BGE Flat Index (using FAISS, brute-force search)\n",
    "flat_index_path = os.path.join(base_dir, 'flat_index.faiss')\n",
    "\n",
    "print(\"Building FAISS Flat index (brute-force)...\")\n",
    "flat_start = time.time()\n",
    "\n",
    "# Create flat index for exact search\n",
    "flat_index = faiss.IndexFlatIP(dimension)  # Inner product for cosine similarity\n",
    "flat_index.add(doc_embeddings)\n",
    "\n",
    "# Save index\n",
    "faiss.write_index(flat_index, flat_index_path)\n",
    "flat_elapsed = time.time() - flat_start\n",
    "\n",
    "index_times['BGE-Flat'] = flat_elapsed\n",
    "print(f\"✅ Flat index saved ({flat_index.ntotal:,} vectors) ({flat_elapsed:.2f}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a7e730",
   "metadata": {},
   "source": [
    "## 5a. Build INT8 Quantized Indexes\n",
    "\n",
    "Build int8 quantized versions for comparison with full precision indexes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ab9070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INT8 Quantization: HNSW with int8 quantization\n",
    "hnsw_int8_index_path = os.path.join(base_dir, 'hnsw_int8_index.faiss')\n",
    "\n",
    "print(f\"Building INT8 HNSW index (M={M}, efC={ef_construction}, efSearch={ef_search})...\")\n",
    "hnsw_int8_start = time.time()\n",
    "\n",
    "# Convert float32 embeddings to int8 using simple quantization\n",
    "# Range: [-128, 127]\n",
    "doc_embeddings_int8 = np.clip(doc_embeddings * 127, -128, 127).astype(np.int8).astype(np.float32) / 127\n",
    "\n",
    "# Create HNSW index with int8 quantization\n",
    "quantizer_int8 = faiss.IndexFlatIP(dimension)\n",
    "hnsw_int8_index = faiss.IndexHNSWFlat(dimension, M, faiss.METRIC_INNER_PRODUCT)\n",
    "hnsw_int8_index.hnsw.efConstruction = ef_construction\n",
    "hnsw_int8_index.hnsw.efSearch = ef_search\n",
    "\n",
    "print(f\"Adding {len(doc_embeddings_int8):,} vectors to INT8 HNSW index...\")\n",
    "hnsw_int8_index.add(doc_embeddings_int8)\n",
    "\n",
    "# Save index\n",
    "faiss.write_index(hnsw_int8_index, hnsw_int8_index_path)\n",
    "hnsw_int8_elapsed = time.time() - hnsw_int8_start\n",
    "\n",
    "index_times['BGE-HNSW-int8'] = hnsw_int8_elapsed\n",
    "print(f\"✅ INT8 HNSW index saved ({hnsw_int8_index.ntotal:,} vectors) ({hnsw_int8_elapsed:.2f}s)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe7d62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INT8 Quantization: Flat index with int8 quantization\n",
    "flat_int8_index_path = os.path.join(base_dir, 'flat_int8_index.faiss')\n",
    "\n",
    "print(\"Building INT8 Flat index (brute-force with quantization)...\")\n",
    "flat_int8_start = time.time()\n",
    "\n",
    "# Create flat index with int8 quantization\n",
    "flat_int8_index = faiss.IndexFlatIP(dimension)\n",
    "flat_int8_index.add(doc_embeddings_int8)\n",
    "\n",
    "# Save index\n",
    "faiss.write_index(flat_int8_index, flat_int8_index_path)\n",
    "flat_int8_elapsed = time.time() - flat_int8_start\n",
    "\n",
    "index_times['BGE-Flat-int8'] = flat_int8_elapsed\n",
    "print(f\"✅ INT8 Flat index saved ({flat_int8_index.ntotal:,} vectors) ({flat_int8_elapsed:.2f}s)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92266d2",
   "metadata": {},
   "source": [
    "## 6. Initialize Searchers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de01564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM25 searcher\n",
    "bm25_searcher = LuceneSearcher(bm25_index_dir)\n",
    "bm25_searcher.set_bm25(k1=0.9, b=0.4)\n",
    "\n",
    "# SPLADE searcher (uses the same model selected above)\n",
    "from pyserini.search.lucene import LuceneImpactSearcher\n",
    "from pyserini.encode import SpladeQueryEncoder\n",
    "\n",
    "print(f\"Initializing SPLADE searcher with model: {SPLADE_MODEL}\")\n",
    "splade_searcher = LuceneImpactSearcher(\n",
    "    splade_index_dir,\n",
    "    query_encoder=SpladeQueryEncoder(SPLADE_MODEL, device='cuda:0'),\n",
    "    min_idf=0\n",
    ")\n",
    "\n",
    "# Load FAISS indexes for dense retrieval\n",
    "import faiss\n",
    "hnsw_index_path = os.path.join(base_dir, 'hnsw_index.faiss')\n",
    "flat_index_path = os.path.join(base_dir, 'flat_index.faiss')\n",
    "hnsw_int8_index_path = os.path.join(base_dir, 'hnsw_int8_index.faiss')\n",
    "flat_int8_index_path = os.path.join(base_dir, 'flat_int8_index.faiss')\n",
    "\n",
    "hnsw_index = faiss.read_index(hnsw_index_path)\n",
    "flat_index = faiss.read_index(flat_index_path)\n",
    "hnsw_int8_index = faiss.read_index(hnsw_int8_index_path)\n",
    "flat_int8_index = faiss.read_index(flat_int8_index_path)\n",
    "\n",
    "print(f\"✅ All searchers initialized\")\n",
    "print(f\"   SPLADE model: {SPLADE_MODEL}\")\n",
    "print(f\"   HNSW index: {hnsw_index.ntotal:,} vectors\")\n",
    "print(f\"   HNSW-int8 index: {hnsw_int8_index.ntotal:,} vectors\")\n",
    "print(f\"   Flat index: {flat_index.ntotal:,} vectors\")\n",
    "print(f\"   Flat-int8 index: {flat_int8_index.ntotal:,} vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebce6ead",
   "metadata": {},
   "source": [
    "## 7. Search Functions\n",
    "\n",
    "Implement search with QPS measurement (16 threads)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b79f732",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id_to_idx = {did: i for i, did in enumerate(doc_ids)}\n",
    "\n",
    "def search_bm25(searcher, query_texts, k=1000):\n",
    "    \"\"\"BM25 search\"\"\"\n",
    "    all_indices = []\n",
    "    all_scores = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for q in tqdm(query_texts, desc=\"BM25 search\"):\n",
    "        hits = searcher.search(q, k)\n",
    "        docids = [h.docid for h in hits]\n",
    "        scores = [h.score for h in hits]\n",
    "        all_indices.append([doc_id_to_idx[d] for d in docids])\n",
    "        all_scores.append(scores)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    qps = len(query_texts) / elapsed\n",
    "    \n",
    "    return {\n",
    "        'name': 'BM25',\n",
    "        'indices': np.array(all_indices, dtype=object),\n",
    "        'scores': np.array(all_scores, dtype=object),\n",
    "        'qps': qps\n",
    "    }\n",
    "\n",
    "def search_splade(searcher, query_texts, k=1000):\n",
    "    \"\"\"SPLADE++ ED search\"\"\"\n",
    "    all_indices = []\n",
    "    all_scores = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for q in tqdm(query_texts, desc=\"SPLADE++ ED search\"):\n",
    "        hits = searcher.search(q, k)\n",
    "        docids = [h.docid for h in hits]\n",
    "        scores = [h.score for h in hits]\n",
    "        all_indices.append([doc_id_to_idx[d] for d in docids])\n",
    "        all_scores.append(scores)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    qps = len(query_texts) / elapsed\n",
    "    \n",
    "    return {\n",
    "        'name': 'SPLADE++ ED',\n",
    "        'indices': np.array(all_indices, dtype=object),\n",
    "        'scores': np.array(all_scores, dtype=object),\n",
    "        'qps': qps\n",
    "    }\n",
    "\n",
    "def search_dense(faiss_index, query_embeddings, name, k=1000):\n",
    "    \"\"\"Dense retrieval with FAISS (HNSW or Flat)\"\"\"\n",
    "    all_indices = []\n",
    "    all_scores = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for emb in tqdm(query_embeddings, desc=f\"{name} search\"):\n",
    "        # FAISS search returns (distances, indices)\n",
    "        scores, indices = faiss_index.search(emb.reshape(1, -1), k)\n",
    "        all_indices.append(indices[0].tolist())\n",
    "        all_scores.append(scores[0].tolist())\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    qps = len(query_embeddings) / elapsed\n",
    "    \n",
    "    return {\n",
    "        'name': name,\n",
    "        'indices': np.array(all_indices, dtype=object),\n",
    "        'scores': np.array(all_scores, dtype=object),\n",
    "        'qps': qps\n",
    "    }\n",
    "\n",
    "print(\"✅ Search functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd984c41",
   "metadata": {},
   "source": [
    "## 8. Run All Searches\n",
    "\n",
    "Retrieve 1000 hits per query using 16 threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e358b426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all searches\n",
    "results_bm25 = search_bm25(bm25_searcher, query_texts, k=k_retrieve)\n",
    "results_splade = search_splade(splade_searcher, query_texts, k=k_retrieve)\n",
    "results_hnsw = search_dense(hnsw_index, query_embeddings, 'BGE-HNSW', k=k_retrieve)\n",
    "results_flat = search_dense(flat_index, query_embeddings, 'BGE-Flat', k=k_retrieve)\n",
    "\n",
    "# INT8 quantized searches\n",
    "query_embeddings_int8 = np.clip(query_embeddings * 127, -128, 127).astype(np.int8).astype(np.float32) / 127\n",
    "results_hnsw_int8 = search_dense(hnsw_int8_index, query_embeddings_int8, 'BGE-HNSW-int8', k=k_retrieve)\n",
    "results_flat_int8 = search_dense(flat_int8_index, query_embeddings_int8, 'BGE-Flat-int8', k=k_retrieve)\n",
    "\n",
    "print(\"\\n✅ All searches complete\")\n",
    "print(f\"   BM25: {results_bm25['qps']:.2f} QPS\")\n",
    "print(f\"   SPLADE++ ED: {results_splade['qps']:.2f} QPS\")\n",
    "print(f\"   BGE-HNSW: {results_hnsw['qps']:.2f} QPS\")\n",
    "print(f\"   BGE-HNSW-int8: {results_hnsw_int8['qps']:.2f} QPS\")\n",
    "print(f\"   BGE-Flat: {results_flat['qps']:.2f} QPS\")\n",
    "print(f\"   BGE-Flat-int8: {results_flat_int8['qps']:.2f} QPS\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50278285",
   "metadata": {},
   "source": [
    "## 9. Evaluation at nDCG@10\n",
    "\n",
    "Evaluate retrieval quality using nDCG@10 as per BEIR guidelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a719140e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall_at_k(retrieved_indices, qrels, query_ids, doc_ids, k=10):\n",
    "    \"\"\"Calculate Recall@k following BEIR guidelines\"\"\"\n",
    "    recalls = []\n",
    "    \n",
    "    for i, qid in enumerate(query_ids):\n",
    "        if qid not in qrels:\n",
    "            continue\n",
    "        \n",
    "        relevant_docs = set(qrels[qid].keys())\n",
    "        retrieved_docs = set([doc_ids[idx] for idx in retrieved_indices[i][:k] if idx >= 0])\n",
    "        \n",
    "        if len(relevant_docs) > 0:\n",
    "            recalls.append(len(relevant_docs & retrieved_docs) / len(relevant_docs))\n",
    "    \n",
    "    return np.mean(recalls) if recalls else 0.0\n",
    "\n",
    "def calculate_ndcg_at_k(retrieved_indices, qrels, query_ids, doc_ids, k=10):\n",
    "    \"\"\"Calculate nDCG@k following BEIR guidelines\"\"\"\n",
    "    ndcgs = []\n",
    "    \n",
    "    for i, qid in enumerate(query_ids):\n",
    "        if qid not in qrels:\n",
    "            continue\n",
    "        \n",
    "        relevant_docs = qrels[qid]\n",
    "        retrieved_docs = [doc_ids[idx] for idx in retrieved_indices[i][:k] if idx >= 0]\n",
    "        \n",
    "        # Calculate DCG\n",
    "        dcg = 0\n",
    "        for rank, doc_id in enumerate(retrieved_docs, 1):\n",
    "            rel = relevant_docs.get(doc_id, 0)\n",
    "            dcg += (2 ** rel - 1) / np.log2(rank + 1)\n",
    "        \n",
    "        # Calculate IDCG\n",
    "        ideal = sorted(relevant_docs.values(), reverse=True)[:k]\n",
    "        idcg = sum((2 ** r - 1) / np.log2(rank + 2) for rank, r in enumerate(ideal))\n",
    "        \n",
    "        ndcgs.append(dcg / idcg if idcg > 0 else 0)\n",
    "    \n",
    "    return np.mean(ndcgs) if ndcgs else 0.0\n",
    "\n",
    "# Evaluate all methods with both metrics\n",
    "for results in [results_bm25, results_splade, results_hnsw, results_flat, results_hnsw_int8, results_flat_int8]:\n",
    "    results['recall@10'] = calculate_recall_at_k(\n",
    "        results['indices'], qrels, query_ids, doc_ids, k=k_eval\n",
    "    )\n",
    "    results['ndcg@10'] = calculate_ndcg_at_k(\n",
    "        results['indices'], qrels, query_ids, doc_ids, k=k_eval\n",
    "    )\n",
    "\n",
    "print(\"✅ Evaluation complete (Recall@10 and nDCG@10 for all methods)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fadaacf",
   "metadata": {},
   "source": [
    "## 10. Results Summary\n",
    "\n",
    "Display results in a table matching the paper format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1015379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results dataframe matching paper table format\n",
    "results_df = pd.DataFrame([\n",
    "    {\n",
    "        'Method': 'BM25',\n",
    "        'Type': 'Sparse (Baseline)',\n",
    "        'Recall@10': results_bm25['recall@10'],\n",
    "        'nDCG@10': results_bm25['ndcg@10'],\n",
    "        'QPS': results_bm25['qps'],\n",
    "    },\n",
    "    {\n",
    "        'Method': 'SPLADE++ ED',\n",
    "        'Type': 'Sparse (Learned)',\n",
    "        'Recall@10': results_splade['recall@10'],\n",
    "        'nDCG@10': results_splade['ndcg@10'],\n",
    "        'QPS': results_splade['qps'],\n",
    "    },\n",
    "    {\n",
    "        'Method': 'BGE-HNSW',\n",
    "        'Type': 'Dense (HNSW)',\n",
    "        'Recall@10': results_hnsw['recall@10'],\n",
    "        'nDCG@10': results_hnsw['ndcg@10'],\n",
    "        'QPS': results_hnsw['qps'],\n",
    "    },\n",
    "    {\n",
    "        'Method': 'BGE-Flat',\n",
    "        'Type': 'Dense (Flat)',\n",
    "        'Recall@10': results_flat['recall@10'],\n",
    "        'nDCG@10': results_flat['ndcg@10'],\n",
    "        'QPS': results_flat['qps'],\n",
    "    },\n",
    "])\n",
    "\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(f\"RESULTS: {dataset_name.upper()}\")\n",
    "print(f\"{'='*90}\")\n",
    "print(results_df.to_string(index=False))\n",
    "print(f\"{'='*90}\")\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  Name: {dataset_name}\")\n",
    "print(f\"  Documents (|C|): {len(corpus):,}\")\n",
    "print(f\"  Queries (|Q|): {len(queries):,}\")\n",
    "print(f\"  Relevance judgments: {len(qrels):,}\")\n",
    "print(f\"\\nIndexing Parameters:\")\n",
    "print(f\"  HNSW: M={M}, efC={ef_construction}, efSearch={ef_search}\")\n",
    "print(f\"  Threads: {threads}\")\n",
    "print(f\"\\nRetrieval & Evaluation:\")\n",
    "print(f\"  Retrieved: k={k_retrieve}\")\n",
    "print(f\"  Evaluated: Recall@{k_eval}, nDCG@{k_eval}\")\n",
    "print(f\"  QPS measured with {threads} threads\")\n",
    "print(f\"{'='*90}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2488e9a9",
   "metadata": {},
   "source": [
    "## 10a. Index Time Summary\n",
    "\n",
    "Display the time taken to build each index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b30af11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index time dataframe\n",
    "index_time_df = pd.DataFrame([\n",
    "    {\n",
    "        'Method': 'BM25',\n",
    "        'Type': 'Sparse (Baseline)',\n",
    "        'Index Time (s)': index_times.get('BM25', 0),\n",
    "    },\n",
    "    {\n",
    "        'Method': 'SPLADE++ ED',\n",
    "        'Type': 'Sparse (Learned)',\n",
    "        'Index Time (s)': index_times.get('SPLADE++ ED', 0),\n",
    "    },\n",
    "    {\n",
    "        'Method': 'BGE-HNSW',\n",
    "        'Type': 'Dense (HNSW)',\n",
    "        'Index Time (s)': index_times.get('BGE-HNSW', 0),\n",
    "    },\n",
    "    {\n",
    "        'Method': 'BGE-Flat',\n",
    "        'Type': 'Dense (Flat)',\n",
    "        'Index Time (s)': index_times.get('BGE-Flat', 0),\n",
    "    },\n",
    "])\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"INDEX TIME: {dataset_name.upper()}\")\n",
    "print(f\"{'='*80}\")\n",
    "print(index_time_df.to_string(index=False))\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total indexing time: {index_time_df['Index Time (s)'].sum():.2f}s\")\n",
    "print(f\"{'='*80}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f68eede",
   "metadata": {},
   "source": [
    "## 10b. Table 3: INT8 Quantization - Indexing Time\n",
    "\n",
    "Compare indexing time between full precision and int8 quantized dense indexes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d12f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 3: INT8 Quantization - Indexing Time\n",
    "table3_df = pd.DataFrame([\n",
    "    {\n",
    "        'Method': 'BGE-HNSW',\n",
    "        'Quantization': 'FP32',\n",
    "        'Index Time (s)': index_times.get('BGE-HNSW', 0),\n",
    "    },\n",
    "    {\n",
    "        'Method': 'BGE-HNSW',\n",
    "        'Quantization': 'int8',\n",
    "        'Index Time (s)': index_times.get('BGE-HNSW-int8', 0),\n",
    "    },\n",
    "    {\n",
    "        'Method': 'BGE-Flat',\n",
    "        'Quantization': 'FP32',\n",
    "        'Index Time (s)': index_times.get('BGE-Flat', 0),\n",
    "    },\n",
    "    {\n",
    "        'Method': 'BGE-Flat',\n",
    "        'Quantization': 'int8',\n",
    "        'Index Time (s)': index_times.get('BGE-Flat-int8', 0),\n",
    "    },\n",
    "])\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"TABLE 3: INT8 QUANTIZATION - INDEXING TIME: {dataset_name.upper()}\")\n",
    "print(f\"{'='*80}\")\n",
    "print(table3_df.to_string(index=False))\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Calculate speedup\n",
    "fp32_hnsw_time = index_times.get('BGE-HNSW', 1)\n",
    "int8_hnsw_time = index_times.get('BGE-HNSW-int8', 1)\n",
    "hnsw_speedup = fp32_hnsw_time / int8_hnsw_time if int8_hnsw_time > 0 else 1.0\n",
    "\n",
    "fp32_flat_time = index_times.get('BGE-Flat', 1)\n",
    "int8_flat_time = index_times.get('BGE-Flat-int8', 1)\n",
    "flat_speedup = fp32_flat_time / int8_flat_time if int8_flat_time > 0 else 1.0\n",
    "\n",
    "print(f\"Indexing Speedup (FP32 vs int8):\")\n",
    "print(f\"  BGE-HNSW: {hnsw_speedup:.2f}x\")\n",
    "print(f\"  BGE-Flat: {flat_speedup:.2f}x\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae06005",
   "metadata": {},
   "source": [
    "## 10c. Table 4: INT8 Quantization - Query Performance and Quality\n",
    "\n",
    "Compare query performance (QPS) and retrieval quality (nDCG@10) between full precision and int8 quantized dense indexes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbad2301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 4: INT8 Quantization - Query Performance and Quality\n",
    "table4_df = pd.DataFrame([\n",
    "    {\n",
    "        'Method': 'BGE-HNSW',\n",
    "        'Quantization': 'FP32',\n",
    "        'QPS': results_hnsw['qps'],\n",
    "        'nDCG@10': results_hnsw['ndcg@10'],\n",
    "        'Recall@10': results_hnsw['recall@10'],\n",
    "    },\n",
    "    {\n",
    "        'Method': 'BGE-HNSW',\n",
    "        'Quantization': 'int8',\n",
    "        'QPS': results_hnsw_int8['qps'],\n",
    "        'nDCG@10': results_hnsw_int8['ndcg@10'],\n",
    "        'Recall@10': results_hnsw_int8['recall@10'],\n",
    "    },\n",
    "    {\n",
    "        'Method': 'BGE-Flat',\n",
    "        'Quantization': 'FP32',\n",
    "        'QPS': results_flat['qps'],\n",
    "        'nDCG@10': results_flat['ndcg@10'],\n",
    "        'Recall@10': results_flat['recall@10'],\n",
    "    },\n",
    "    {\n",
    "        'Method': 'BGE-Flat',\n",
    "        'Quantization': 'int8',\n",
    "        'QPS': results_flat_int8['qps'],\n",
    "        'nDCG@10': results_flat_int8['ndcg@10'],\n",
    "        'Recall@10': results_flat_int8['recall@10'],\n",
    "    },\n",
    "])\n",
    "\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(f\"TABLE 4: INT8 QUANTIZATION - QUERY PERFORMANCE & QUALITY: {dataset_name.upper()}\")\n",
    "print(f\"{'='*100}\")\n",
    "print(table4_df.to_string(index=False))\n",
    "print(f\"{'='*100}\\n\")\n",
    "\n",
    "# Calculate query speedup and quality retention\n",
    "fp32_hnsw_qps = results_hnsw['qps']\n",
    "int8_hnsw_qps = results_hnsw_int8['qps']\n",
    "hnsw_qps_speedup = int8_hnsw_qps / fp32_hnsw_qps if fp32_hnsw_qps > 0 else 1.0\n",
    "hnsw_ndcg_retention = (results_hnsw_int8['ndcg@10'] / results_hnsw['ndcg@10'] * 100) if results_hnsw['ndcg@10'] > 0 else 100\n",
    "\n",
    "fp32_flat_qps = results_flat['qps']\n",
    "int8_flat_qps = results_flat_int8['qps']\n",
    "flat_qps_speedup = int8_flat_qps / fp32_flat_qps if fp32_flat_qps > 0 else 1.0\n",
    "flat_ndcg_retention = (results_flat_int8['ndcg@10'] / results_flat['ndcg@10'] * 100) if results_flat['ndcg@10'] > 0 else 100\n",
    "\n",
    "print(f\"Query Performance Speedup (int8 vs FP32):\")\n",
    "print(f\"  BGE-HNSW: {hnsw_qps_speedup:.2f}x (nDCG@10 retention: {hnsw_ndcg_retention:.1f}%)\")\n",
    "print(f\"  BGE-Flat: {flat_qps_speedup:.2f}x (nDCG@10 retention: {flat_ndcg_retention:.1f}%)\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab357754",
   "metadata": {},
   "source": [
    "## 11. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b65ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations matching paper analysis\n",
    "output_dir = f'results_{dataset_name}'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "colors = {'Sparse (Baseline)': 'orange', 'Sparse (Learned)': 'red', \n",
    "          'Dense (HNSW)': 'steelblue', 'Dense (Flat)': 'lightblue'}\n",
    "\n",
    "# Plot 1: Speed (QPS) vs Quality (nDCG@10)\n",
    "for _, row in results_df.iterrows():\n",
    "    ax1.scatter(row['QPS'], row['nDCG@10'], \n",
    "              s=200, alpha=0.7, color=colors[row['Type']], \n",
    "              edgecolors='black', linewidth=1.5)\n",
    "    ax1.annotate(row['Method'], \n",
    "               (row['QPS'], row['nDCG@10']), \n",
    "               xytext=(8, 8), textcoords='offset points', \n",
    "               fontsize=10, fontweight='bold')\n",
    "\n",
    "ax1.set_xlabel('QPS (queries per second, 16 threads)', fontsize=11)\n",
    "ax1.set_ylabel('nDCG@10', fontsize=11)\n",
    "ax1.set_title(f'Speed vs Quality (nDCG@10) — {dataset_name}', fontsize=12, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Speed (QPS) vs Quality (Recall@10)\n",
    "for _, row in results_df.iterrows():\n",
    "    ax2.scatter(row['QPS'], row['Recall@10'], \n",
    "              s=200, alpha=0.7, color=colors[row['Type']], \n",
    "              edgecolors='black', linewidth=1.5)\n",
    "    ax2.annotate(row['Method'], \n",
    "               (row['QPS'], row['Recall@10']), \n",
    "               xytext=(8, 8), textcoords='offset points', \n",
    "               fontsize=10, fontweight='bold')\n",
    "\n",
    "ax2.set_xlabel('QPS (queries per second, 16 threads)', fontsize=11)\n",
    "ax2.set_ylabel('Recall@10', fontsize=11)\n",
    "ax2.set_title(f'Speed vs Quality (Recall@10) — {dataset_name}', fontsize=12, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/speed_vs_quality.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Bar chart comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Quality metrics\n",
    "x = np.arange(len(results_df))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax1.bar(x - width/2, results_df['Recall@10'], width, label='Recall@10', alpha=0.8)\n",
    "bars2 = ax1.bar(x + width/2, results_df['nDCG@10'], width, label='nDCG@10', alpha=0.8)\n",
    "\n",
    "ax1.set_xlabel('Method', fontsize=11)\n",
    "ax1.set_ylabel('Score', fontsize=11)\n",
    "ax1.set_title(f'Quality Metrics Comparison — {dataset_name}', fontsize=12, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(results_df['Method'], rotation=15, ha='right')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# QPS comparison\n",
    "bars = ax2.bar(results_df['Method'], results_df['QPS'], alpha=0.8, \n",
    "               color=[colors[t] for t in results_df['Type']], edgecolor='black')\n",
    "ax2.set_xlabel('Method', fontsize=11)\n",
    "ax2.set_ylabel('QPS (16 threads)', fontsize=11)\n",
    "ax2.set_title(f'Query Performance — {dataset_name}', fontsize=12, fontweight='bold')\n",
    "ax2.set_xticklabels(results_df['Method'], rotation=15, ha='right')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/metrics_comparison.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✅ Visualizations complete and saved to {output_dir}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d8ff54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INT8 Quantization Comparison Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Indexing Time Comparison\n",
    "ax = axes[0, 0]\n",
    "methods = ['HNSW', 'Flat']\n",
    "fp32_times = [index_times.get('BGE-HNSW', 0), index_times.get('BGE-Flat', 0)]\n",
    "int8_times = [index_times.get('BGE-HNSW-int8', 0), index_times.get('BGE-Flat-int8', 0)]\n",
    "\n",
    "x = np.arange(len(methods))\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, fp32_times, width, label='FP32', alpha=0.8, color='steelblue')\n",
    "ax.bar(x + width/2, int8_times, width, label='int8', alpha=0.8, color='coral')\n",
    "ax.set_xlabel('Index Type', fontsize=11)\n",
    "ax.set_ylabel('Indexing Time (s)', fontsize=11)\n",
    "ax.set_title(f'Indexing Time: FP32 vs int8 — {dataset_name}', fontsize=12, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(methods)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 2: Query Performance (QPS) Comparison\n",
    "ax = axes[0, 1]\n",
    "fp32_qps = [results_hnsw['qps'], results_flat['qps']]\n",
    "int8_qps = [results_hnsw_int8['qps'], results_flat_int8['qps']]\n",
    "\n",
    "ax.bar(x - width/2, fp32_qps, width, label='FP32', alpha=0.8, color='steelblue')\n",
    "ax.bar(x + width/2, int8_qps, width, label='int8', alpha=0.8, color='coral')\n",
    "ax.set_xlabel('Index Type', fontsize=11)\n",
    "ax.set_ylabel('QPS (16 threads)', fontsize=11)\n",
    "ax.set_title(f'Query Performance: FP32 vs int8 — {dataset_name}', fontsize=12, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(methods)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 3: nDCG@10 Comparison\n",
    "ax = axes[1, 0]\n",
    "fp32_ndcg = [results_hnsw['ndcg@10'], results_flat['ndcg@10']]\n",
    "int8_ndcg = [results_hnsw_int8['ndcg@10'], results_flat_int8['ndcg@10']]\n",
    "\n",
    "ax.bar(x - width/2, fp32_ndcg, width, label='FP32', alpha=0.8, color='steelblue')\n",
    "ax.bar(x + width/2, int8_ndcg, width, label='int8', alpha=0.8, color='coral')\n",
    "ax.set_xlabel('Index Type', fontsize=11)\n",
    "ax.set_ylabel('nDCG@10', fontsize=11)\n",
    "ax.set_title(f'Retrieval Quality (nDCG@10): FP32 vs int8 — {dataset_name}', fontsize=12, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(methods)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 4: Speed-Quality Tradeoff\n",
    "ax = axes[1, 1]\n",
    "colors_int8 = {'FP32': 'steelblue', 'int8': 'coral'}\n",
    "for method, qps, ndcg, quantization in [\n",
    "    ('HNSW-FP32', results_hnsw['qps'], results_hnsw['ndcg@10'], 'FP32'),\n",
    "    ('HNSW-int8', results_hnsw_int8['qps'], results_hnsw_int8['ndcg@10'], 'int8'),\n",
    "    ('Flat-FP32', results_flat['qps'], results_flat['ndcg@10'], 'FP32'),\n",
    "    ('Flat-int8', results_flat_int8['qps'], results_flat_int8['ndcg@10'], 'int8'),\n",
    "]:\n",
    "    ax.scatter(qps, ndcg, s=200, alpha=0.7, color=colors_int8[quantization], \n",
    "              edgecolors='black', linewidth=1.5)\n",
    "    ax.annotate(method, (qps, ndcg), xytext=(8, 8), textcoords='offset points', \n",
    "               fontsize=10, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('QPS (16 threads)', fontsize=11)\n",
    "ax.set_ylabel('nDCG@10', fontsize=11)\n",
    "ax.set_title(f'Speed-Quality Tradeoff: FP32 vs int8 — {dataset_name}', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/int8_quantization_comparison.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✅ INT8 quantization comparison visualizations saved to {output_dir}/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f8d6ff",
   "metadata": {},
   "source": [
    "## 12. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e2ee12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "output_dir = f'results_{dataset_name}'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "results_path = os.path.join(output_dir, f'{dataset_name}_results.csv')\n",
    "results_df.to_csv(results_path, index=False)\n",
    "\n",
    "# Save index times to CSV\n",
    "index_time_path = os.path.join(output_dir, f'{dataset_name}_index_times.csv')\n",
    "index_time_df.to_csv(index_time_path, index=False)\n",
    "\n",
    "# Save Table 3 (INT8 Indexing Time)\n",
    "table3_path = os.path.join(output_dir, f'{dataset_name}_table3_int8_indexing.csv')\n",
    "table3_df.to_csv(table3_path, index=False)\n",
    "\n",
    "# Save Table 4 (INT8 Query Performance & Quality)\n",
    "table4_path = os.path.join(output_dir, f'{dataset_name}_table4_int8_performance.csv')\n",
    "table4_df.to_csv(table4_path, index=False)\n",
    "\n",
    "# Save detailed results with metadata\n",
    "metadata = {\n",
    "    'dataset': dataset_name,\n",
    "    'num_documents': len(corpus),\n",
    "    'num_queries': len(queries),\n",
    "    'num_qrels': len(qrels),\n",
    "    'hnsw_M': M,\n",
    "    'hnsw_efC': ef_construction,\n",
    "    'hnsw_efSearch': ef_search,\n",
    "    'threads': threads,\n",
    "    'k_retrieve': k_retrieve,\n",
    "    'k_eval': k_eval,\n",
    "    'index_times': index_times,\n",
    "    'total_indexing_time': float(index_time_df['Index Time (s)'].sum()),\n",
    "    'int8_quantization': {\n",
    "        'method': 'simple_linear_quantization',\n",
    "        'scale': 127,\n",
    "        'range': '[-128, 127]',\n",
    "    },\n",
    "    'speedup': {\n",
    "        'hnsw_indexing': float(fp32_hnsw_time / int8_hnsw_time) if int8_hnsw_time > 0 else None,\n",
    "        'flat_indexing': float(fp32_flat_time / int8_flat_time) if int8_flat_time > 0 else None,\n",
    "        'hnsw_query': float(hnsw_qps_speedup),\n",
    "        'flat_query': float(flat_qps_speedup),\n",
    "    },\n",
    "    'quality_retention': {\n",
    "        'hnsw_ndcg': float(hnsw_ndcg_retention),\n",
    "        'flat_ndcg': float(flat_ndcg_retention),\n",
    "    },\n",
    "}\n",
    "\n",
    "metadata_path = os.path.join(output_dir, f'{dataset_name}_metadata.json')\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"✅ Results saved:\")\n",
    "print(f\"   - {results_path}\")\n",
    "print(f\"   - {index_time_path}\")\n",
    "print(f\"   - {table3_path} (Table 3: INT8 Indexing Time)\")\n",
    "print(f\"   - {table4_path} (Table 4: INT8 Query Performance)\")\n",
    "print(f\"   - {metadata_path}\")\n",
    "print(f\"   - {output_dir}/speed_vs_quality.pdf\")\n",
    "print(f\"   - {output_dir}/metrics_comparison.pdf\")\n",
    "print(f\"   - {output_dir}/int8_quantization_comparison.pdf\")\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"SUMMARY - PAPER TABLES REPLICATED:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"✅ Table 1: Dense vs Sparse Retrieval Baseline Results\")\n",
    "print(f\"✅ Table 3: INT8 Quantization - Indexing Time Comparison\")\n",
    "print(f\"✅ Table 4: INT8 Quantization - Query Performance & Quality\")\n",
    "print(f\"{'='*80}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a012d132",
   "metadata": {},
   "source": [
    "## 13. Download Results to Local Machine\n",
    "\n",
    "Detect environment (Colab/Kaggle/Local) and download all results and visualizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357aa9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect environment and download/copy results\n",
    "import shutil\n",
    "import platform\n",
    "\n",
    "def detect_environment():\n",
    "    \"\"\"Detect if running on Colab, Kaggle, or Local\"\"\"\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        return 'colab'\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    if os.path.exists('/kaggle/'):\n",
    "        return 'kaggle'\n",
    "    \n",
    "    return 'local'\n",
    "\n",
    "environment = detect_environment()\n",
    "print(f\"Environment detected: {environment.upper()}\")\n",
    "\n",
    "if environment == 'colab':\n",
    "    # Google Colab: Mount Google Drive and copy results\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/gdrive', force_remount=True)\n",
    "        \n",
    "        colab_save_dir = f'/content/gdrive/My Drive/BEIR_Results/{dataset_name}'\n",
    "        os.makedirs(colab_save_dir, exist_ok=True)\n",
    "        \n",
    "        # Copy entire results directory to Google Drive\n",
    "        shutil.copytree(output_dir, os.path.join(colab_save_dir, 'results'), dirs_exist_ok=True)\n",
    "        shutil.copytree(base_dir, os.path.join(colab_save_dir, 'indexes'), dirs_exist_ok=True)\n",
    "        \n",
    "        print(f\"\\n✅ Results saved to Google Drive: {colab_save_dir}\")\n",
    "        print(f\"\\nFiles saved:\")\n",
    "        for root, dirs, files in os.walk(colab_save_dir):\n",
    "            level = root.replace(colab_save_dir, '').count(os.sep)\n",
    "            indent = ' ' * 2 * level\n",
    "            print(f\"{indent}{os.path.basename(root)}/\")\n",
    "            subindent = ' ' * 2 * (level + 1)\n",
    "            for file in files:\n",
    "                print(f\"{subindent}{file}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not mount Google Drive: {e}\")\n",
    "        print(f\"Files remain in: {output_dir}\")\n",
    "\n",
    "elif environment == 'kaggle':\n",
    "    # Kaggle: Save to /kaggle/working/ (synced to outputs)\n",
    "    kaggle_save_dir = f'/kaggle/working/BEIR_Results_{dataset_name}'\n",
    "    os.makedirs(kaggle_save_dir, exist_ok=True)\n",
    "    \n",
    "    # Copy entire results directory\n",
    "    shutil.copytree(output_dir, os.path.join(kaggle_save_dir, 'results'), dirs_exist_ok=True)\n",
    "    shutil.copytree(base_dir, os.path.join(kaggle_save_dir, 'indexes'), dirs_exist_ok=True)\n",
    "    \n",
    "    print(f\"\\n✅ Results saved to Kaggle working directory: {kaggle_save_dir}\")\n",
    "    print(f\"Files will be available in 'Output' section when notebook completes\")\n",
    "    print(f\"\\nFiles saved:\")\n",
    "    for root, dirs, files in os.walk(kaggle_save_dir):\n",
    "        level = root.replace(kaggle_save_dir, '').count(os.sep)\n",
    "        indent = ' ' * 2 * level\n",
    "        print(f\"{indent}{os.path.basename(root)}/\")\n",
    "        subindent = ' ' * 2 * (level + 1)\n",
    "        for file in files:\n",
    "            print(f\"{subindent}{file}\")\n",
    "\n",
    "else:\n",
    "    # Local execution: Files are already saved\n",
    "    print(f\"\\n✅ Results already saved locally to: {output_dir}\")\n",
    "    print(f\"\\nFiles saved:\")\n",
    "    for root, dirs, files in os.walk(output_dir):\n",
    "        level = root.replace(output_dir, '').count(os.sep)\n",
    "        indent = ' ' * 2 * level\n",
    "        print(f\"{indent}{os.path.basename(root)}/\")\n",
    "        subindent = ' ' * 2 * (level + 1)\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            file_size = os.path.getsize(file_path) / 1024  # Size in KB\n",
    "            print(f\"{subindent}{file} ({file_size:.1f} KB)\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"RESULTS SUMMARY:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"Index directory: {base_dir}\")\n",
    "print(f\"\\nGenerated files:\")\n",
    "print(f\"  📊 {dataset_name}_results.csv - Main results (Dense/Sparse comparison)\")\n",
    "print(f\"  ⏱️  {dataset_name}_index_times.csv - Index construction times\")\n",
    "print(f\"  🔢 {dataset_name}_table3_int8_indexing.csv - Table 3 (INT8 indexing)\")\n",
    "print(f\"  📈 {dataset_name}_table4_int8_performance.csv - Table 4 (INT8 performance)\")\n",
    "print(f\"  📝 {dataset_name}_metadata.json - Complete metadata & speedup metrics\")\n",
    "print(f\"  📉 speed_vs_quality.pdf - Quality vs Speed scatter plots\")\n",
    "print(f\"  📊 metrics_comparison.pdf - Bar chart comparisons\")\n",
    "print(f\"  🔄 int8_quantization_comparison.pdf - INT8 quantization analysis\")\n",
    "print(f\"\\nIndexes saved:\")\n",
    "print(f\"  • BM25 Lucene index\")\n",
    "print(f\"  • SPLADE++ ED impact index\")\n",
    "print(f\"  • BGE-HNSW (FP32 and int8)\")\n",
    "print(f\"  • BGE-Flat (FP32 and int8)\")\n",
    "print(f\"{'='*80}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fcf9f1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 🗄️ ARCHIVE: SPLADE Model Diagnostics (Sections 14-19)\n",
    "\n",
    "**Status**: Archive - Investigation Complete  \n",
    "**Purpose**: Documents the investigation into SPLADE++ underperformance  \n",
    "**Conclusion**: Use `naver/splade_v2_distil` model (98.8% match to paper)  \n",
    "**Date**: December 22, 2025\n",
    "\n",
    "These sections document the diagnostic process that identified the SPLADE model version issue. They are preserved for reference but are **not required for production runs**.\n",
    "\n",
    "**Summary**:\n",
    "- Sections 14-18: Investigated 20% performance gap with `ensembledistil`\n",
    "- Section 19: Tested `v2_distil` model → **98.8% match to paper** ✅\n",
    "- **Result**: Main implementation (Sections 1-13) now uses `v2_distil`\n",
    "\n",
    "**For production runs**: Only execute Sections 1-13. These diagnostic sections can be skipped.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d31498",
   "metadata": {},
   "source": [
    "## 14. SPLADE++ Diagnostics - Model Version Issue\n",
    "\n",
    "**KEY FINDING**: The model `naver/splade-cocondenser-ensembledistil` was last updated on **June 30, 2025** - 9 months after the paper was published (September 2024). This could explain the ~20% performance gap.\n",
    "\n",
    "Let's investigate:\n",
    "1. Check encoded output quality\n",
    "2. Compare weight distributions\n",
    "3. Test alternative SPLADE models from the paper timeframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ba96b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 4: Manual SPLADE Encoding Test (Verify Pyserini Implementation)\n",
    "# import torch\n",
    "# from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "# def encode_splade_manual(text, model, tokenizer, device='cpu'):\n",
    "#     \"\"\"\n",
    "#     Manual SPLADE encoding following the official paper implementation\n",
    "#     This helps verify if Pyserini's encoding differs from expected behavior\n",
    "#     \"\"\"\n",
    "#     model.eval()\n",
    "#     model.to(device)\n",
    "    \n",
    "#     # Tokenize\n",
    "#     tokens = tokenizer(\n",
    "#         text, \n",
    "#         return_tensors='pt', \n",
    "#         padding=True, \n",
    "#         truncation=True, \n",
    "#         max_length=512\n",
    "#     ).to(device)\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         # Forward pass\n",
    "#         output = model(**tokens)\n",
    "#         logits = output.logits  # [batch_size, seq_len, vocab_size]\n",
    "        \n",
    "#         # SPLADE aggregation formula: max(log(1 + relu(logits)))\n",
    "#         # This is the key step - must use MAX pooling, not average!\n",
    "#         relu_log = torch.log(1 + torch.relu(logits))\n",
    "        \n",
    "#         # Apply attention mask and max pool over sequence dimension\n",
    "#         masked_values = relu_log * tokens['attention_mask'].unsqueeze(-1)\n",
    "#         max_values, _ = torch.max(masked_values, dim=1)\n",
    "    \n",
    "#     # Convert to sparse representation (only non-zero values)\n",
    "#     sparse_vec = {}\n",
    "#     vocab = tokenizer.get_vocab()\n",
    "#     inv_vocab = {v: k for k, v in vocab.items()}\n",
    "    \n",
    "#     for idx in range(max_values.shape[1]):\n",
    "#         val = max_values[0, idx].item()\n",
    "#         if val > 0:  # Only store positive values\n",
    "#             token = inv_vocab.get(idx, f\"<unk_{idx}>\")\n",
    "#             sparse_vec[token] = val\n",
    "    \n",
    "#     return sparse_vec\n",
    "\n",
    "# # Test manual encoding on sample document\n",
    "# print(\"=\"*80)\n",
    "# print(\"MANUAL SPLADE ENCODING TEST\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# print(\"\\nLoading SPLADE model for manual test...\")\n",
    "# model_name = 'naver/splade-cocondenser-ensembledistil'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# # Encode first document\n",
    "# sample_text = doc_texts[0][:500]  # First 500 chars\n",
    "# print(f\"\\nSample text (first 100 chars): {sample_text[:100]}...\")\n",
    "\n",
    "# manual_encoding = encode_splade_manual(sample_text, model, tokenizer)\n",
    "\n",
    "# print(f\"\\n{'='*80}\")\n",
    "# print(\"MANUAL ENCODING RESULTS\")\n",
    "# print(f\"{'='*80}\")\n",
    "# print(f\"Non-zero terms: {len(manual_encoding)}\")\n",
    "# print(f\"Weight range: [{min(manual_encoding.values()):.4f}, {max(manual_encoding.values()):.4f}]\")\n",
    "# print(f\"Mean weight: {np.mean(list(manual_encoding.values())):.4f}\")\n",
    "\n",
    "# # Top terms\n",
    "# top_manual = sorted(manual_encoding.items(), key=lambda x: -x[1])[:15]\n",
    "# print(f\"\\nTop 15 terms from manual encoding:\")\n",
    "# for i, (term, weight) in enumerate(top_manual, 1):\n",
    "#     print(f\"  {i:2d}. {term:20s} {weight:8.4f}\")\n",
    "\n",
    "# # Compare with Pyserini encoding if available\n",
    "# # Load Pyserini encoding if not already loaded\n",
    "# try:\n",
    "#     sample_encoding\n",
    "# except NameError:\n",
    "#     print(\"\\nLoading Pyserini encoding for comparison...\")\n",
    "#     encoded_file = os.path.join(splade_encoded_dir, 'embeddings.jsonl')\n",
    "#     if os.path.exists(encoded_file):\n",
    "#         with open(encoded_file, 'r') as f:\n",
    "#             sample_encoding = json.loads(f.readline())\n",
    "#         print(f\"✅ Loaded Pyserini encoding for doc: {sample_encoding['id']}\")\n",
    "#     else:\n",
    "#         sample_encoding = None\n",
    "#         print(f\"❌ Pyserini encoding not found: {encoded_file}\")\n",
    "\n",
    "# if sample_encoding:\n",
    "#     print(f\"\\n{'='*80}\")\n",
    "#     print(\"COMPARISON: Manual vs Pyserini\")\n",
    "#     print(f\"{'='*80}\")\n",
    "    \n",
    "#     pyserini_weights = list(sample_encoding['vector'].values())\n",
    "#     manual_weights = list(manual_encoding.values())\n",
    "    \n",
    "#     print(f\"{'Metric':<30} {'Pyserini':<15} {'Manual':<15} {'Difference'}\")\n",
    "#     print(f\"{'-'*80}\")\n",
    "#     print(f\"{'Non-zero terms':<30} {len(pyserini_weights):<15} {len(manual_weights):<15} {len(manual_weights) - len(pyserini_weights)}\")\n",
    "#     print(f\"{'Mean weight':<30} {np.mean(pyserini_weights):<15.4f} {np.mean(manual_weights):<15.4f} {np.mean(manual_weights) - np.mean(pyserini_weights):.4f}\")\n",
    "#     print(f\"{'Max weight':<30} {max(pyserini_weights):<15.4f} {max(manual_weights):<15.4f} {max(manual_weights) - max(pyserini_weights):.4f}\")\n",
    "    \n",
    "#     # Check if significant difference\n",
    "#     term_diff = abs(len(manual_weights) - len(pyserini_weights))\n",
    "#     weight_diff = abs(np.mean(manual_weights) - np.mean(pyserini_weights))\n",
    "    \n",
    "#     print(f\"\\n{'='*80}\")\n",
    "#     if term_diff > 20 or weight_diff > 0.5:\n",
    "#         print(\"❌ SIGNIFICANT DIFFERENCE DETECTED!\")\n",
    "#         print(\"   Pyserini encoding may differ from expected SPLADE behavior\")\n",
    "#         print(\"   Recommendation: Use manual encoding for full corpus\")\n",
    "#     else:\n",
    "#         print(\"✅ Encodings are similar\")\n",
    "#         print(\"   Pyserini implementation appears correct\")\n",
    "# else:\n",
    "#     print(\"\\n⚠️ Cannot compare - Pyserini encoding not available\")\n",
    "#     print(\"   Run the indexing cells first to generate SPLADE encodings\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9872c6f2",
   "metadata": {},
   "source": [
    "## 16. ROOT CAUSE IDENTIFIED + SOLUTION\n",
    "\n",
    "**🔴 CRITICAL FINDING**: Pyserini applies a **quantization multiplier (~100×)** to SPLADE weights that is NOT in the original paper!\n",
    "\n",
    "**The Problem:**\n",
    "- Expected SPLADE weights: 0.0-3.0 range (float, log-scale)\n",
    "- Pyserini weights: 0-126 range (integer quantization)\n",
    "- Impact: Scoring function completely distorted, causing ~20% performance loss\n",
    "\n",
    "**The Solution:**\n",
    "Re-encode the corpus using the correct SPLADE formula without Pyserini's quantization multiplier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4193e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 2: Inspect Encoded SPLADE Output Quality\n",
    "# def diagnose_splade_encoding():\n",
    "#     \"\"\"Analyze SPLADE encoding to check for issues\"\"\"\n",
    "#     import json\n",
    "    \n",
    "#     print(\"=\"*80)\n",
    "#     print(\"SPLADE ENCODING DIAGNOSTIC\")\n",
    "#     print(\"=\"*80)\n",
    "    \n",
    "#     # Read encoded documents\n",
    "#     encoded_file = os.path.join(splade_encoded_dir, 'embeddings.jsonl')\n",
    "    \n",
    "#     if not os.path.exists(encoded_file):\n",
    "#         print(f\"❌ Encoded file not found: {encoded_file}\")\n",
    "#         return None\n",
    "    \n",
    "#     # Sample 10 documents\n",
    "#     encodings = []\n",
    "#     with open(encoded_file, 'r') as f:\n",
    "#         for i, line in enumerate(f):\n",
    "#             if i >= 10:\n",
    "#                 break\n",
    "#             encodings.append(json.loads(line))\n",
    "    \n",
    "#     print(f\"\\n✅ Loaded {len(encodings)} sample encodings\\n\")\n",
    "    \n",
    "#     # Analyze first document in detail\n",
    "#     first_doc = encodings[0]\n",
    "#     print(f\"Document ID: {first_doc['id']}\")\n",
    "#     print(f\"Number of non-zero terms: {len(first_doc['vector'])}\")\n",
    "    \n",
    "#     # Weight statistics\n",
    "#     weights = list(first_doc['vector'].values())\n",
    "#     print(f\"\\nWeight Statistics:\")\n",
    "#     print(f\"  Min:    {min(weights):.4f}\")\n",
    "#     print(f\"  Max:    {max(weights):.4f}\")\n",
    "#     print(f\"  Mean:   {np.mean(weights):.4f}\")\n",
    "#     print(f\"  Median: {np.median(weights):.4f}\")\n",
    "#     print(f\"  Std:    {np.std(weights):.4f}\")\n",
    "    \n",
    "#     # Top 20 terms\n",
    "#     top_terms = sorted(first_doc['vector'].items(), key=lambda x: -x[1])[:20]\n",
    "#     print(f\"\\nTop 20 terms with highest weights:\")\n",
    "#     for i, (term, weight) in enumerate(top_terms, 1):\n",
    "#         print(f\"  {i:2d}. {term:20s} {weight:8.4f}\")\n",
    "    \n",
    "#     # Aggregate statistics across all samples\n",
    "#     all_weights = []\n",
    "#     term_counts = []\n",
    "#     for enc in encodings:\n",
    "#         weights_list = list(enc['vector'].values())\n",
    "#         all_weights.extend(weights_list)\n",
    "#         term_counts.append(len(enc['vector']))\n",
    "    \n",
    "#     print(f\"\\n{'='*80}\")\n",
    "#     print(f\"AGGREGATE STATISTICS (10 documents)\")\n",
    "#     print(f\"{'='*80}\")\n",
    "#     print(f\"Average non-zero terms per doc: {np.mean(term_counts):.1f}\")\n",
    "#     print(f\"  Expected range: 100-300 terms\")\n",
    "#     print(f\"  Status: {'✅ Good' if 100 <= np.mean(term_counts) <= 300 else '❌ Outside expected range'}\")\n",
    "    \n",
    "#     print(f\"\\nWeight distribution across all samples:\")\n",
    "#     print(f\"  Min:    {min(all_weights):.4f}\")\n",
    "#     print(f\"  Max:    {max(all_weights):.4f}\")\n",
    "#     print(f\"  Mean:   {np.mean(all_weights):.4f}\")\n",
    "#     print(f\"  Median: {np.median(all_weights):.4f}\")\n",
    "    \n",
    "#     # Check for quantization artifacts\n",
    "#     print(f\"\\n{'='*80}\")\n",
    "#     print(f\"QUANTIZATION CHECK\")\n",
    "#     print(f\"{'='*80}\")\n",
    "    \n",
    "#     # Check if weights are already quantized (integers)\n",
    "#     int_like = sum(1 for w in all_weights[:100] if abs(w - round(w)) < 0.001)\n",
    "#     print(f\"Weights that look like integers: {int_like}/100\")\n",
    "#     if int_like > 50:\n",
    "#         print(\"⚠️ WARNING: Weights appear to be pre-quantized!\")\n",
    "#     else:\n",
    "#         print(\"✅ Weights are continuous (not pre-quantized)\")\n",
    "    \n",
    "#     return first_doc, encodings\n",
    "\n",
    "# # Run diagnostic\n",
    "# sample_encoding, all_encodings = diagnose_splade_encoding()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674b4e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the corrected SPLADE index\n",
    "# print(\"=\"*80)\n",
    "# print(\"TESTING CORRECTED SPLADE INDEX\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# # Check if corrected index exists\n",
    "# try:\n",
    "#     splade_correct_index_dir\n",
    "# except NameError:\n",
    "#     print(\"❌ ERROR: Corrected SPLADE index not found!\")\n",
    "#     print(\"   Please run the previous cell to re-encode the corpus first.\")\n",
    "#     print(\"   (The cell titled: 'Re-encode corpus with correct SPLADE implementation')\")\n",
    "#     raise\n",
    "\n",
    "# if not os.path.exists(splade_correct_index_dir):\n",
    "#     print(f\"❌ ERROR: Index directory not found: {splade_correct_index_dir}\")\n",
    "#     print(\"   Please run the previous cell to create the corrected index.\")\n",
    "#     raise FileNotFoundError(f\"Index not found: {splade_correct_index_dir}\")\n",
    "\n",
    "# # Initialize corrected searcher\n",
    "# from pyserini.search.lucene import LuceneImpactSearcher\n",
    "\n",
    "# print(\"\\nInitializing corrected SPLADE searcher...\")\n",
    "# splade_correct_searcher = LuceneImpactSearcher(\n",
    "#     splade_correct_index_dir,\n",
    "#     'naver/splade-cocondenser-ensembledistil',\n",
    "#     encoder_type='pytorch'\n",
    "# )\n",
    "# print(\"✅ Searcher initialized\")\n",
    "\n",
    "# # Run search\n",
    "# print(\"\\nRunning search with corrected SPLADE...\")\n",
    "# results_splade_correct = search_splade(splade_correct_searcher, query_texts, k=k_retrieve)\n",
    "\n",
    "# # Evaluate\n",
    "# results_splade_correct['recall@10'] = calculate_recall_at_k(\n",
    "#     results_splade_correct['indices'], qrels, query_ids, doc_ids, k=k_eval\n",
    "# )\n",
    "# results_splade_correct['ndcg@10'] = calculate_ndcg_at_k(\n",
    "#     results_splade_correct['indices'], qrels, query_ids, doc_ids, k=k_eval\n",
    "# )\n",
    "\n",
    "# # Compare with original\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"COMPARISON: Original Pyserini vs Corrected SPLADE\")\n",
    "# print(\"=\"*80)\n",
    "# print(f\"{'Metric':<30} {'Original':<15} {'Corrected':<15} {'Improvement'}\")\n",
    "# print(\"-\"*80)\n",
    "# print(f\"{'nDCG@10':<30} {results_splade['ndcg@10']:<15.4f} {results_splade_correct['ndcg@10']:<15.4f} {results_splade_correct['ndcg@10'] - results_splade['ndcg@10']:+.4f}\")\n",
    "# print(f\"{'Recall@10':<30} {results_splade['recall@10']:<15.4f} {results_splade_correct['recall@10']:<15.4f} {results_splade_correct['recall@10'] - results_splade['recall@10']:+.4f}\")\n",
    "# print(f\"{'QPS':<30} {results_splade['qps']:<15.2f} {results_splade_correct['qps']:<15.2f} {results_splade_correct['qps'] - results_splade['qps']:+.2f}\")\n",
    "\n",
    "# improvement_pct = ((results_splade_correct['ndcg@10'] - results_splade['ndcg@10']) / results_splade['ndcg@10'] * 100)\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(f\"📊 nDCG@10 Improvement: {improvement_pct:+.1f}%\")\n",
    "# print(f\"Expected paper value: ~0.70 (for SciFact)\")\n",
    "# print(f\"Your corrected value: {results_splade_correct['ndcg@10']:.3f}\")\n",
    "\n",
    "# if results_splade_correct['ndcg@10'] >= 0.68:\n",
    "#     print(\"✅ EXCELLENT! Now matches paper expectations!\")\n",
    "# elif results_splade_correct['ndcg@10'] >= 0.60:\n",
    "#     print(\"✅ GOOD! Significant improvement, close to paper\")\n",
    "# else:\n",
    "#     print(\"⚠️ Still below expected - may need further investigation\")\n",
    "# print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e69b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save corrected SPLADE results\n",
    "# print(\"=\"*80)\n",
    "# print(\"SAVING CORRECTED RESULTS\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# # Update results dataframe with corrected SPLADE\n",
    "# results_df_corrected = pd.DataFrame([\n",
    "#     {\n",
    "#         'Method': 'BM25',\n",
    "#         'Type': 'Sparse (Baseline)',\n",
    "#         'Recall@10': results_bm25['recall@10'],\n",
    "#         'nDCG@10': results_bm25['ndcg@10'],\n",
    "#         'QPS': results_bm25['qps'],\n",
    "#     },\n",
    "#     {\n",
    "#         'Method': 'SPLADE++ ED (Original)',\n",
    "#         'Type': 'Sparse (Learned)',\n",
    "#         'Recall@10': results_splade['recall@10'],\n",
    "#         'nDCG@10': results_splade['ndcg@10'],\n",
    "#         'QPS': results_splade['qps'],\n",
    "#     },\n",
    "#     {\n",
    "#         'Method': 'SPLADE++ ED (Corrected)',\n",
    "#         'Type': 'Sparse (Learned - Fixed)',\n",
    "#         'Recall@10': results_splade_correct['recall@10'],\n",
    "#         'nDCG@10': results_splade_correct['ndcg@10'],\n",
    "#         'QPS': results_splade_correct['qps'],\n",
    "#     },\n",
    "#     {\n",
    "#         'Method': 'BGE-HNSW',\n",
    "#         'Type': 'Dense (HNSW)',\n",
    "#         'Recall@10': results_hnsw['recall@10'],\n",
    "#         'nDCG@10': results_hnsw['ndcg@10'],\n",
    "#         'QPS': results_hnsw['qps'],\n",
    "#     },\n",
    "#     {\n",
    "#         'Method': 'BGE-Flat',\n",
    "#         'Type': 'Dense (Flat)',\n",
    "#         'Recall@10': results_flat['recall@10'],\n",
    "#         'nDCG@10': results_flat['ndcg@10'],\n",
    "#         'QPS': results_flat['qps'],\n",
    "#     },\n",
    "# ])\n",
    "\n",
    "# # Save to CSV\n",
    "# corrected_results_path = os.path.join(output_dir, f'{dataset_name}_results_corrected.csv')\n",
    "# results_df_corrected.to_csv(corrected_results_path, index=False)\n",
    "\n",
    "# print(f\"\\n✅ Corrected results saved to: {corrected_results_path}\")\n",
    "\n",
    "# # Display comparison table\n",
    "# print(\"\\n\" + \"=\"*90)\n",
    "# print(f\"FINAL RESULTS: {dataset_name.upper()} (WITH CORRECTED SPLADE)\")\n",
    "# print(\"=\"*90)\n",
    "# print(results_df_corrected.to_string(index=False))\n",
    "# print(\"=\"*90)\n",
    "\n",
    "# # Calculate improvement summary\n",
    "# improvement = results_splade_correct['ndcg@10'] - results_splade['ndcg@10']\n",
    "# improvement_pct = (improvement / results_splade['ndcg@10'] * 100)\n",
    "\n",
    "# print(f\"\\n📊 IMPROVEMENT SUMMARY:\")\n",
    "# print(f\"   Original SPLADE nDCG@10:  {results_splade['ndcg@10']:.4f}\")\n",
    "# print(f\"   Corrected SPLADE nDCG@10: {results_splade_correct['ndcg@10']:.4f}\")\n",
    "# print(f\"   Absolute improvement:     {improvement:+.4f}\")\n",
    "# print(f\"   Relative improvement:     {improvement_pct:+.1f}%\")\n",
    "# print(f\"\\n   Paper expected value:     ~0.70\")\n",
    "# print(f\"   Match quality:            {(results_splade_correct['ndcg@10']/0.70*100):.1f}%\")\n",
    "# print(\"=\"*90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9ddb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Quick test: Try alternative SPLADE model\n",
    "# print(\"=\"*80)\n",
    "# print(\"TESTING ALTERNATIVE SPLADE MODEL\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# # Try the self-distil variant (different training approach)\n",
    "# alt_model_name = 'naver/splade-cocondenser-selfdistil'\n",
    "\n",
    "# print(f\"\\nTesting model: {alt_model_name}\")\n",
    "# print(\"This model uses self-distillation instead of ensemble distillation\")\n",
    "# print(\"\\nTo test this model:\")\n",
    "# print(f\"1. Go back to the SPLADE encoding cell (Section 5, cell with pyserini.encode)\")\n",
    "# print(f\"2. Change the --encoder parameter to: {alt_model_name}\")\n",
    "# print(f\"3. Re-run the encoding and indexing\")\n",
    "# print(f\"4. Re-run search and evaluation\")\n",
    "\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"ALTERNATIVE: Contact Paper Authors\")\n",
    "# print(\"=\"*80)\n",
    "# print(\"Email: jimmylin@uwaterloo.ca\")\n",
    "# print(\"Ask for:\")\n",
    "# print(\"  - Exact SPLADE model checkpoint used (with date/commit)\")\n",
    "# print(\"  - Pyserini version used\")\n",
    "# print(\"  - Any special configuration parameters\")\n",
    "# print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1e0bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test Alternative SPLADE Model: selfdistil\n",
    "# import subprocess\n",
    "# import time\n",
    "\n",
    "# print(\"=\"*80)\n",
    "# print(\"TESTING ALTERNATIVE SPLADE MODEL: selfdistil\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# # Clear GPU memory first\n",
    "# print(\"\\nClearing GPU memory...\")\n",
    "# import torch\n",
    "# import gc\n",
    "\n",
    "# # Delete any existing models from memory\n",
    "# try:\n",
    "#     del model, tokenizer\n",
    "# except:\n",
    "#     pass\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect()\n",
    "\n",
    "# # Check GPU memory\n",
    "# if torch.cuda.is_available():\n",
    "#     allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "#     reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "#     print(f\"GPU Memory - Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB\")\n",
    "\n",
    "# # Use alternative model\n",
    "# alt_model_name = 'naver/splade-cocondenser-selfdistil'\n",
    "# splade_alt_docs_dir = os.path.join(base_dir, 'splade_docs')  # Reuse existing docs\n",
    "# splade_alt_encoded_dir = os.path.join(base_dir, 'splade_encoded_selfdistil')\n",
    "# splade_alt_index_dir = os.path.join(base_dir, 'splade_index_selfdistil')\n",
    "# os.makedirs(splade_alt_encoded_dir, exist_ok=True)\n",
    "\n",
    "# print(f\"\\nModel: {alt_model_name}\")\n",
    "# print(f\"This uses self-distillation training (different from ensemble)\")\n",
    "\n",
    "# # Encode with alternative model (reduced batch size to fit in GPU)\n",
    "# print(\"\\nVerifying model availability...\")\n",
    "# print(\"Using batch size 16 to avoid OOM...\")\n",
    "# result = subprocess.run([\n",
    "#     'python', '-m', 'pyserini.encode',\n",
    "#     'input', '--corpus', splade_alt_docs_dir,\n",
    "#     '--fields', 'text',\n",
    "#     'output', '--embeddings', splade_alt_encoded_dir,\n",
    "#     'encoder', '--encoder', alt_model_name,\n",
    "#     '--device', 'cuda',\n",
    "#     '--batch', '16'  # Reduced from 32 to avoid OOM\n",
    "# ], capture_output=True, text=True)\n",
    "\n",
    "# if result.returncode != 0:\n",
    "#     print(\"❌ ENCODING FAILED!\")\n",
    "#     print(\"\\nSTDOUT:\")\n",
    "#     print(result.stdout)\n",
    "#     print(\"\\nSTDERR:\")\n",
    "#     print(result.stderr[-2000:])  # Last 2000 chars of error\n",
    "#     raise subprocess.CalledProcessError(result.returncode, result.args, result.stdout, result.stderr)\n",
    "\n",
    "# print(\"✅ Encoding complete\")\n",
    "\n",
    "# # Build index\n",
    "# print(\"\\nBuilding SPLADE index with selfdistil...\")\n",
    "# splade_alt_start = time.time()\n",
    "# subprocess.run([\n",
    "#     'python', '-m', 'pyserini.index.lucene',\n",
    "#     '--collection', 'JsonVectorCollection',\n",
    "#     '--input', splade_alt_encoded_dir,\n",
    "#     '--index', splade_alt_index_dir,\n",
    "#     '--generator', 'DefaultLuceneDocumentGenerator',\n",
    "#     '--impact',\n",
    "#     '--threads', threads,\n",
    "#     '--storeRaw'\n",
    "# ], check=True)\n",
    "\n",
    "# splade_alt_elapsed = time.time() - splade_alt_start\n",
    "# print(f\"✅ Index built ({splade_alt_elapsed:.2f}s)\")\n",
    "\n",
    "# # Initialize searcher\n",
    "# print(\"\\nInitializing selfdistil searcher...\")\n",
    "# from pyserini.search.lucene import LuceneImpactSearcher\n",
    "\n",
    "# splade_alt_searcher = LuceneImpactSearcher(\n",
    "#     splade_alt_index_dir,\n",
    "#     alt_model_name,\n",
    "#     encoder_type='pytorch'\n",
    "# )\n",
    "\n",
    "# # Run search\n",
    "# print(\"\\nRunning search with selfdistil model...\")\n",
    "# results_splade_alt = search_splade(splade_alt_searcher, query_texts, k=k_retrieve)\n",
    "\n",
    "# # Evaluate\n",
    "# results_splade_alt['recall@10'] = calculate_recall_at_k(\n",
    "#     results_splade_alt['indices'], qrels, query_ids, doc_ids, k=k_eval\n",
    "# )\n",
    "# results_splade_alt['ndcg@10'] = calculate_ndcg_at_k(\n",
    "#     results_splade_alt['indices'], qrels, query_ids, doc_ids, k=k_eval\n",
    "# )\n",
    "\n",
    "# # Display comparison\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"COMPARISON: Original vs Alternative SPLADE Models\")\n",
    "# print(\"=\"*80)\n",
    "# print(f\"{'Model':<40} {'nDCG@10':<15} {'Recall@10':<15} {'QPS'}\")\n",
    "# print(\"-\"*80)\n",
    "# print(f\"{'ensembledistil (original)':<40} {results_splade['ndcg@10']:<15.4f} {results_splade['recall@10']:<15.4f} {results_splade['qps']:.2f}\")\n",
    "# print(f\"{'selfdistil (alternative)':<40} {results_splade_alt['ndcg@10']:<15.4f} {results_splade_alt['recall@10']:<15.4f} {results_splade_alt['qps']:.2f}\")\n",
    "\n",
    "# improvement = results_splade_alt['ndcg@10'] - results_splade['ndcg@10']\n",
    "# improvement_pct = (improvement / results_splade['ndcg@10'] * 100) if results_splade['ndcg@10'] > 0 else 0\n",
    "\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(f\"📊 SELFDISTIL RESULTS:\")\n",
    "# print(f\"   nDCG@10: {results_splade_alt['ndcg@10']:.4f}\")\n",
    "# print(f\"   Change from ensembledistil: {improvement:+.4f} ({improvement_pct:+.1f}%)\")\n",
    "# print(f\"   Paper expected: ~0.70\")\n",
    "# print(f\"   Match quality: {(results_splade_alt['ndcg@10']/0.70*100):.1f}%\")\n",
    "\n",
    "# if results_splade_alt['ndcg@10'] >= 0.68:\n",
    "#     print(\"\\n✅ SUCCESS! This model matches paper expectations!\")\n",
    "# elif results_splade_alt['ndcg@10'] >= 0.60:\n",
    "#     print(\"\\n✅ GOOD! Significant improvement, close to paper\")\n",
    "# elif improvement > 0:\n",
    "#     print(\"\\n⚠️ Better, but still below paper. May need to try splade_v2_distil\")\n",
    "# else:\n",
    "#     print(\"\\n⚠️ No improvement. Model version likely not the issue.\")\n",
    "\n",
    "# print(\"=\"*80)\n",
    "# improvement = results_splade_alt['ndcg@10'] - results_splade['ndcg@10']\n",
    "# improvement_pct = (improvement / results_splade['ndcg@10'] * 100) if results_splade['ndcg@10'] > 0 else 0\n",
    "\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(f\"📊 SELFDISTIL RESULTS:\")\n",
    "# print(f\"   nDCG@10: {results_splade_alt['ndcg@10']:.4f}\")\n",
    "# print(f\"   Change from ensembledistil: {improvement:+.4f} ({improvement_pct:+.1f}%)\")\n",
    "# print(f\"   Paper expected: ~0.70\")\n",
    "# print(f\"   Match quality: {(results_splade_alt['ndcg@10']/0.70*100):.1f}%\")\n",
    "\n",
    "# if results_splade_alt['ndcg@10'] >= 0.68:\n",
    "#     print(\"\\n✅ SUCCESS! This model matches paper expectations!\")\n",
    "# elif results_splade_alt['ndcg@10'] >= 0.60:\n",
    "#     print(\"\\n✅ GOOD! Significant improvement, close to paper\")\n",
    "# elif improvement > 0:\n",
    "#     print(\"\\n⚠️ Better, but still below paper. May need to try splade_v2_distil\")\n",
    "# else:\n",
    "#     print(\"\\n⚠️ No improvement. Model version likely not the issue.\")\n",
    "# print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d8d79d",
   "metadata": {},
   "source": [
    "## 18. Root Cause Analysis - Revision\n",
    "\n",
    "**❌ FINDING**: Removing quantization made things WORSE (-20% performance)\n",
    "\n",
    "This means Pyserini's quantization is **correct** for Lucene's impact scoring! The real issues are likely:\n",
    "\n",
    "1. **Model Version Mismatch** (MOST LIKELY)\n",
    "   - Current model: June 30, 2025 update\n",
    "   - Paper: September 2024\n",
    "   - Model weights may have changed\n",
    "\n",
    "2. **Query Encoding Difference**\n",
    "   - Documents use one method\n",
    "   - Queries might use another\n",
    "   \n",
    "3. **Scoring Function Parameters**\n",
    "   - Lucene impact scoring has parameters\n",
    "   - May differ from paper's implementation\n",
    "\n",
    "Let's test alternative SPLADE models from the correct timeframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8b22a7",
   "metadata": {},
   "source": [
    "## 17. Next Steps and Alternative Approaches\n",
    "\n",
    "If the corrected encoding doesn't fully resolve the gap, consider:\n",
    "\n",
    "### Alternative SPLADE Models\n",
    "- `naver/splade-cocondenser-selfdistil` - Different training method\n",
    "- `naver/splade_v2_distil` - Older version (may match paper timeframe better)\n",
    "- Contact paper authors for the exact model checkpoint used\n",
    "\n",
    "### Further Diagnostics\n",
    "- Compare term overlap between your encodings and expected results\n",
    "- Check if query encoding also needs correction\n",
    "- Verify Lucene scoring function parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d05e97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Re-encode corpus with correct SPLADE implementation (no quantization)\n",
    "# import torch\n",
    "# from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "# from tqdm.auto import tqdm\n",
    "# import json\n",
    "# import numpy as np\n",
    "\n",
    "# print(\"=\"*80)\n",
    "# print(\"RE-ENCODING CORPUS WITH CORRECT SPLADE (NO QUANTIZATION)\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# # OPTION: Set to True to test with subset first (faster)\n",
    "# TEST_SUBSET = False  # Change to True to encode only 1000 docs for testing\n",
    "# subset_size = 1000 if TEST_SUBSET else len(doc_texts)\n",
    "\n",
    "# if TEST_SUBSET:\n",
    "#     print(f\"⚠️  TEST MODE: Encoding only {subset_size} documents for quick validation\")\n",
    "#     print(\"   Set TEST_SUBSET=False to encode full corpus\")\n",
    "# else:\n",
    "#     print(f\"📊 FULL MODE: Encoding all {len(doc_texts):,} documents\")\n",
    "\n",
    "# # Create new output directory\n",
    "# splade_correct_encoded_dir = os.path.join(base_dir, 'splade_encoded_correct')\n",
    "# splade_correct_index_dir = os.path.join(base_dir, 'splade_index_correct')\n",
    "# os.makedirs(splade_correct_encoded_dir, exist_ok=True)\n",
    "\n",
    "# print(\"\\nLoading SPLADE model...\")\n",
    "# model_name = 'naver/splade-cocondenser-ensembledistil'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "# model.eval()\n",
    "\n",
    "# # Use GPU if available with optimizations\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# # Clear GPU cache before starting\n",
    "# if device == 'cuda':\n",
    "#     torch.cuda.empty_cache()\n",
    "#     import gc\n",
    "#     gc.collect()\n",
    "\n",
    "# model.to(device)\n",
    "\n",
    "# # Enable half precision for 2x speedup on GPU (but use smaller batches to avoid OOM)\n",
    "# if device == 'cuda':\n",
    "#     model = model.half()  # Use FP16 for faster inference\n",
    "#     print(f\"✅ Model loaded on GPU with FP16 acceleration\")\n",
    "# else:\n",
    "#     print(f\"✅ Model loaded on: {device}\")\n",
    "\n",
    "# def encode_splade_batch(texts, model, tokenizer, device='cpu'):\n",
    "#     \"\"\"Optimized batch SPLADE encoding\"\"\"\n",
    "#     # Tokenize batch\n",
    "#     tokens = tokenizer(\n",
    "#         texts,\n",
    "#         return_tensors='pt',\n",
    "#         padding=True,\n",
    "#         truncation=True,\n",
    "#         max_length=512\n",
    "#     ).to(device)\n",
    "    \n",
    "#     with torch.no_grad(), torch.amp.autocast('cuda', enabled=(device=='cuda')):\n",
    "#         # Forward pass\n",
    "#         output = model(**tokens)\n",
    "#         logits = output.logits  # [batch_size, seq_len, vocab_size]\n",
    "        \n",
    "#         # SPLADE formula: max(log(1 + relu(logits)))\n",
    "#         relu_log = torch.log(1 + torch.relu(logits))\n",
    "        \n",
    "#         # Apply attention mask and max pool\n",
    "#         masked_values = relu_log * tokens['attention_mask'].unsqueeze(-1)\n",
    "#         max_values, _ = torch.max(masked_values, dim=1)  # [batch_size, vocab_size]\n",
    "    \n",
    "#     # Convert to sparse vectors (vectorized, much faster)\n",
    "#     max_values_np = max_values.cpu().float().numpy()\n",
    "#     inv_vocab = {v: k for k, v in tokenizer.get_vocab().items()}\n",
    "#     batch_vectors = []\n",
    "    \n",
    "#     for i in range(max_values_np.shape[0]):\n",
    "#         # Only process non-zero values (sparse)\n",
    "#         nonzero_indices = np.where(max_values_np[i] > 0)[0]\n",
    "#         sparse_vec = {\n",
    "#             inv_vocab.get(int(idx), f\"<unk_{idx}>\"): float(max_values_np[i, idx])\n",
    "#             for idx in nonzero_indices\n",
    "#         }\n",
    "#         batch_vectors.append(sparse_vec)\n",
    "    \n",
    "#     return batch_vectors\n",
    "\n",
    "# # Encode all documents in batches with GPU-friendly batch size\n",
    "# print(f\"\\nEncoding {len(doc_texts):,} documents...\")\n",
    "# batch_size = 32 if device == 'cuda' else 8  # Reduced from 128 to avoid OOM\n",
    "# print(f\"Batch size: {batch_size} (estimated time: ~{len(doc_texts)//batch_size//20} minutes)\")\n",
    "\n",
    "# output_file = os.path.join(splade_correct_encoded_dir, 'embeddings.jsonl')\n",
    "# with open(output_file, 'w', encoding='utf-8') as f:\n",
    "#     for i in tqdm(range(0, min(subset_size, len(doc_texts)), batch_size), desc=\"Encoding batches\"):\n",
    "#         batch_texts = doc_texts[i:i+batch_size]\n",
    "#         batch_ids = doc_ids[i:i+batch_size]\n",
    "        \n",
    "#         batch_vectors = encode_splade_batch(batch_texts, model, tokenizer, device)\n",
    "        \n",
    "#         # Write to file\n",
    "#         for doc_id, vector in zip(batch_ids, batch_vectors):\n",
    "#             f.write(json.dumps({\n",
    "#                 'id': doc_id,\n",
    "#                 'vector': vector,\n",
    "#                 'contents': ''  # Not needed for indexing\n",
    "#             }) + '\\n')\n",
    "\n",
    "# print(f\"✅ Encoding complete: {output_file}\")\n",
    "# print(f\"   Encoded {min(subset_size, len(doc_texts)):,} documents\")\n",
    "\n",
    "# # Build Lucene index with correct encodings\n",
    "# print(\"\\nBuilding SPLADE index with correct encodings...\")\n",
    "# splade_correct_start = time.time()\n",
    "\n",
    "# subprocess.run([\n",
    "#     'python', '-m', 'pyserini.index.lucene',\n",
    "#     '--collection', 'JsonVectorCollection',\n",
    "#     '--input', splade_correct_encoded_dir,\n",
    "#     '--index', splade_correct_index_dir,\n",
    "#     '--generator', 'DefaultLuceneDocumentGenerator',\n",
    "#     '--impact',\n",
    "#     '--threads', threads,\n",
    "#     '--storeRaw'\n",
    "# ], check=True)\n",
    "\n",
    "# splade_correct_elapsed = time.time() - splade_correct_start\n",
    "# index_times['SPLADE++ ED (Correct)'] = splade_correct_elapsed\n",
    "\n",
    "# print(f\"✅ Correct SPLADE index built ({splade_correct_elapsed:.2f}s)\")\n",
    "# print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661802ba",
   "metadata": {},
   "source": [
    "## 16. ROOT CAUSE IDENTIFIED + SOLUTION\n",
    "\n",
    "**🔴 CRITICAL FINDING**: Pyserini applies a **quantization multiplier (~100×)** to SPLADE weights that is NOT in the original paper!\n",
    "\n",
    "**The Problem:**\n",
    "- Expected SPLADE weights: 0.0-3.0 range (float, log-scale)\n",
    "- Pyserini weights: 0-126 range (integer quantization)\n",
    "- Impact: Scoring function completely distorted, causing ~20% performance loss\n",
    "\n",
    "**The Solution:**\n",
    "We need to use **correct SPLADE encoding** without Pyserini's quantization. Two approaches:\n",
    "\n",
    "### Option 1: Manual Encoding (Recommended)\n",
    "Re-encode the corpus using our manual function that follows the official SPLADE paper\n",
    "\n",
    "### Option 2: Fix Pyserini Quantization\n",
    "Try to disable Pyserini's impact quantization (if possible)\n",
    "\n",
    "Let's implement Option 1 below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41fd743",
   "metadata": {},
   "source": [
    "## Section 19: Test SPLADE v2 Distil Model\n",
    "\n",
    "**Final attempt**: Testing `naver/splade_v2_distil` - an older distillation version that may be more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb68aa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# import torch\n",
    "# import subprocess\n",
    "# from pyserini.encode import SpladeQueryEncoder\n",
    "# from pyserini.search.lucene import LuceneImpactSearcher\n",
    "# import time\n",
    "# import numpy as np\n",
    "# from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "# import os\n",
    "# import json\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Clear GPU memory\n",
    "# print(\"🧹 Clearing GPU memory...\")\n",
    "# if 'model' in dir():\n",
    "#     del model\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect()\n",
    "\n",
    "# # Show GPU memory\n",
    "# if torch.cuda.is_available():\n",
    "#     print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB total\")\n",
    "#     print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "#     print(f\"GPU Memory Cached: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"TESTING SPLADE v2 DISTIL MODEL\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# # Model configuration\n",
    "# alt_model_name = \"naver/splade_v2_distil\"\n",
    "# alt_docs_dir = os.path.join(base_dir, \"splade_v2_docs\")\n",
    "# alt_encoded_dir = os.path.join(base_dir, \"splade_v2_encoded\")\n",
    "# alt_index_path = os.path.join(base_dir, \"splade_v2_index\")\n",
    "# os.makedirs(alt_docs_dir, exist_ok=True)\n",
    "# os.makedirs(alt_encoded_dir, exist_ok=True)\n",
    "\n",
    "# print(f\"\\n📦 Model: {alt_model_name}\")\n",
    "# print(f\"📁 Documents: {alt_docs_dir}\")\n",
    "# print(f\"📁 Encoded: {alt_encoded_dir}\")\n",
    "# print(f\"📁 Index: {alt_index_path}\")\n",
    "\n",
    "# # Step 1: Write documents for v2 model encoding\n",
    "# print(f\"\\n⚙️ Writing documents for v2 distil encoding...\")\n",
    "# alt_jsonl = os.path.join(alt_docs_dir, 'docs.jsonl')\n",
    "# with open(alt_jsonl, 'w', encoding='utf-8') as f:\n",
    "#     for did, text in zip(doc_ids, doc_texts):\n",
    "#         f.write(json.dumps({'id': did, 'text': text}) + \"\\n\")\n",
    "\n",
    "# # Step 2: Encode corpus with v2 distil model\n",
    "# if not os.path.exists(alt_encoded_dir) or len(os.listdir(alt_encoded_dir)) == 0:\n",
    "#     print(f\"\\n⚙️ Encoding corpus with {alt_model_name}...\")\n",
    "#     print(f\"   Batch size: 16 (reduced for GPU memory)\")\n",
    "    \n",
    "#     start_time = time.time()\n",
    "    \n",
    "#     cmd = [\n",
    "#         \"python\", \"-m\", \"pyserini.encode\",\n",
    "#         \"input\", \"--corpus\", alt_docs_dir,\n",
    "#         \"--fields\", \"text\",\n",
    "#         \"output\", \"--embeddings\", alt_encoded_dir,\n",
    "#         \"encoder\", \"--encoder\", alt_model_name,\n",
    "#         \"--device\", \"cuda:0\",\n",
    "#         \"--batch\", \"16\"  # Reduced batch size\n",
    "#     ]\n",
    "    \n",
    "#     result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    \n",
    "#     encoding_time = time.time() - start_time\n",
    "    \n",
    "#     if result.returncode != 0:\n",
    "#         print(f\"❌ Encoding failed!\")\n",
    "#         print(f\"Error: {result.stderr}\")\n",
    "#     else:\n",
    "#         print(f\"✅ Encoding complete in {encoding_time:.1f}s\")\n",
    "        \n",
    "#         # Note: For SPLADE, we don't need to copy corpus as encoded dir has everything\n",
    "#         os.makedirs(alt_docs_dir, exist_ok=True)\n",
    "# else:\n",
    "#     print(f\"\\n✓ Using existing encoded corpus at {alt_encoded_dir}\")\n",
    "\n",
    "# # Step 3: Create Pyserini index\n",
    "# if not os.path.exists(alt_index_path):\n",
    "#     print(f\"\\n⚙️ Creating Pyserini index...\")\n",
    "    \n",
    "#     start_time = time.time()\n",
    "    \n",
    "#     cmd = [\n",
    "#         \"python\", \"-m\", \"pyserini.index.lucene\",\n",
    "#         \"--collection\", \"JsonVectorCollection\",\n",
    "#         \"--input\", alt_encoded_dir,\n",
    "#         \"--index\", alt_index_path,\n",
    "#         \"--generator\", \"DefaultLuceneDocumentGenerator\",\n",
    "#         \"--threads\", \"1\",\n",
    "#         \"--impact\",\n",
    "#         \"--pretokenized\"\n",
    "#     ]\n",
    "    \n",
    "#     result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    \n",
    "#     indexing_time = time.time() - start_time\n",
    "    \n",
    "#     if result.returncode != 0:\n",
    "#         print(f\"❌ Indexing failed!\")\n",
    "#         print(f\"Error: {result.stderr}\")\n",
    "#     else:\n",
    "#         print(f\"✅ Indexing complete in {indexing_time:.1f}s\")\n",
    "# else:\n",
    "#     print(f\"\\n✓ Using existing index at {alt_index_path}\")\n",
    "\n",
    "# # Step 4: Initialize searcher and run evaluation\n",
    "# print(f\"\\n⚙️ Initializing v2 distil searcher...\")\n",
    "\n",
    "# searcher = LuceneImpactSearcher(\n",
    "#     alt_index_path,\n",
    "#     query_encoder=SpladeQueryEncoder(alt_model_name, device='cuda:0'),\n",
    "#     min_idf=0\n",
    "# )\n",
    "\n",
    "# print(f\"\\n⚙️ Running search with v2 distil model...\")\n",
    "\n",
    "# start_time = time.time()\n",
    "# splade_v2_results = {}\n",
    "\n",
    "# for query_id in tqdm(queries.keys(), desc=\"SPLADE++ v2 search\"):\n",
    "#     query_text = queries[query_id]\n",
    "#     hits = searcher.search(query_text, k=1000)\n",
    "    \n",
    "#     splade_v2_results[query_id] = {\n",
    "#         hit.docid: float(hit.score) for hit in hits\n",
    "#     }\n",
    "\n",
    "# search_time = time.time() - start_time\n",
    "# qps_v2 = len(queries) / search_time\n",
    "\n",
    "# # Evaluate\n",
    "# ndcg_v2, _map_v2, recall_v2, precision_v2 = EvaluateRetrieval.evaluate(\n",
    "#     qrels, \n",
    "#     splade_v2_results, \n",
    "#     [1, 3, 5, 10, 100, 1000]\n",
    "# )\n",
    "\n",
    "# ndcg10_v2 = ndcg_v2['NDCG@10']\n",
    "# recall10_v2 = recall_v2['Recall@10']\n",
    "\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"SPLADE v2 DISTIL MODEL RESULTS\")\n",
    "# print(\"=\"*80)\n",
    "# print(f\"{'Metric':<20} {'Value'}\")\n",
    "# print(\"-\"*80)\n",
    "# print(f\"{'nDCG@10':<20} {ndcg10_v2:.4f}\")\n",
    "# print(f\"{'Recall@10':<20} {recall10_v2:.4f}\")\n",
    "# print(f\"{'QPS':<20} {qps_v2:.2f}\")\n",
    "\n",
    "# # Compare to paper target\n",
    "# paper_target = 0.70\n",
    "# match_quality = (ndcg10_v2 / paper_target) * 100\n",
    "# gap = paper_target - ndcg10_v2\n",
    "\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(f\"📊 COMPARISON TO PAPER:\")\n",
    "# print(f\"   v2_distil nDCG@10: {ndcg10_v2:.4f}\")\n",
    "# print(f\"   Paper expected: ~{paper_target}\")\n",
    "# print(f\"   Match quality: {match_quality:.1f}%\")\n",
    "# print(f\"   Gap: {gap:.4f} ({gap/paper_target*100:.1f}% below)\")\n",
    "# print()\n",
    "\n",
    "# if ndcg10_v2 >= 0.68:\n",
    "#     print(\"✅ SUCCESS! Close to paper results!\")\n",
    "# elif ndcg10_v2 >= 0.65:\n",
    "#     print(\"⚠️ Close, but still slightly below paper\")\n",
    "# else:\n",
    "#     print(\"⚠️ Still significantly below paper (~21% gap)\")\n",
    "#     print(\"   Likely cause: Model updates after paper publication\")\n",
    "#     print(\"   BGE and BM25 match perfectly, validating your implementation\")\n",
    "#     print()\n",
    "#     print(\"📝 RECOMMENDATION: Accept current results and document this limitation.\")\n",
    "#     print(\"   All 3 tested SPLADE models (ensembledistil, selfdistil, v2_distil)\")\n",
    "#     print(\"   show similar underperformance, suggesting systematic model changes\")\n",
    "#     print(\"   since paper publication (Sept 2024).\")\n",
    "# print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚öñÔ∏è Notebook 3: Retrieval Benchmark (Dense vs Sparse)\n",
    "**Author:** Gabriele Righi\n",
    "\n",
    "**Project:** Dense vs Sparse Retrieval Reproducibility\n",
    "\n",
    "## üéØ Objective\n",
    "This notebook performs the **core comparative analysis** of the project. It evaluates the retrieval effectiveness (Quality) and efficiency (Speed/RAM) of different indexing strategies on the **Natural Questions (NQ)** dataset.\n",
    "\n",
    "## ‚öôÔ∏è Key Operations\n",
    "1.  **Index Loading:** Loads the pre-built Faiss indexes (HNSW FP32, HNSW INT8, Flat INT8) created in Notebook 2.\n",
    "2.  **Dense Retrieval:** Executes search queries to measure **nDCG@10**, **Recall@10**, and **QPS** (Queries Per Second).\n",
    "3.  **Sparse Baseline (BM25):** Calculates the BM25 baseline using the optimized `bm25s` library to replicate Table 3 of the original paper.\n",
    "4.  **Results Aggregation:** Compiles all metrics into a final CSV report (`final_results_nq.csv`) representing the replication of **Table 3** (Effectiveness) and **Table 4** (Efficiency).\n",
    "\n",
    "## üìÇ Inputs & Outputs\n",
    "* **Input:** `doc_embeddings.npy` (NB1), `.faiss` indexes (NB2), NQ Dataset.\n",
    "* **Output:** `final_results_nq.csv` containing the final benchmark metrics.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è CRITICAL SETUP: ENVIRONMENT & DEPENDENCY FIX\n",
    "\n",
    "**Did you get a `ValueError: numpy.dtype size changed` below?**\n",
    "Don't panic! This is expected because we are updating core libraries (Numpy/Pandas) to ensure compatibility between `beir`, `faiss`, and `bm25s`.\n",
    "\n",
    "### üõë FOLLOW THESE STEPS CAREFULLY:\n",
    "1.  **RUN CELL 1:** Execute the installation cell below. It might crash or show the error.\n",
    "2.  **RESTART THE SESSION:** Go to the top menu: **Run** -> **Restart and clear cell output**.\n",
    "3.  **SKIP CELL 1:** Once restarted, do **NOT** run the installation cell again.\n",
    "4.  **PROCEED:** Go directly to **Cell 2** and run the rest of the notebook.\n",
    "\n",
    "The code will work perfectly after the restart! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T18:27:56.731083Z",
     "iopub.status.busy": "2026-02-10T18:27:56.730900Z",
     "iopub.status.idle": "2026-02-10T18:29:19.312289Z",
     "shell.execute_reply": "2026-02-10T18:29:19.310832Z",
     "shell.execute_reply.started": "2026-02-10T18:27:56.731060Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# CELL 1: ROBUST ENVIRONMENT SETUP\n",
    "# ==============================================================================\n",
    "# 1. UNINSTALL conflicting libraries first (The \"Nuclear\" Step)\n",
    "!pip uninstall -y numpy pandas scipy scikit-learn\n",
    "\n",
    "# 2. INSTALL a coherent ecosystem compatible with Numpy 1.x\n",
    "# We use --no-cache-dir to ensure we download fresh compatible wheels\n",
    "!pip install \"numpy<2.0\" \"pandas==2.2.2\" \"scipy==1.13.1\" \"scikit-learn==1.5.0\" --no-cache-dir --force-reinstall\n",
    "\n",
    "# 3. INSTALL Project Libraries\n",
    "!pip install faiss-cpu sentence-transformers beir pyserini --no-deps\n",
    "\n",
    "# 4. INSTALL Core ML Libraries\n",
    "!pip install tqdm transformers torch torchvision\n",
    "\n",
    "# 5. VERIFY Installation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "print(f\"‚úÖ Setup Success. Numpy: {np.__version__} | Pandas: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CELL 2: COMPLETE BENCHMARK SCRIPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T15:47:22.391877Z",
     "iopub.status.busy": "2026-01-29T15:47:22.391462Z",
     "iopub.status.idle": "2026-01-29T16:32:49.944819Z",
     "shell.execute_reply": "2026-01-29T16:32:49.943834Z",
     "shell.execute_reply.started": "2026-01-29T15:47:22.391832Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"üöÄ STARTING FINAL EVALUATION BENCHMARK...\")\n",
    "\n",
    "# ==========================================\n",
    "# 1. FILE DETECTION\n",
    "# ==========================================\n",
    "print(\"\\n[1/5] Detecting Index Files...\")\n",
    "input_root = '/kaggle/input'\n",
    "paths = {\n",
    "    'doc_emb': None,       \n",
    "    'hnsw_fp32': None, \n",
    "    'hnsw_int8': None, \n",
    "    'flat_int8': None\n",
    "}\n",
    "\n",
    "for root, _, files in os.walk(input_root):\n",
    "    if 'doc_embeddings.npy' in files: \n",
    "        paths['doc_emb'] = os.path.join(root, 'doc_embeddings.npy')\n",
    "    if 'hnsw_index.faiss' in files: \n",
    "        paths['hnsw_fp32'] = os.path.join(root, 'hnsw_index.faiss')\n",
    "    if 'hnsw_int8_index.faiss' in files: \n",
    "        paths['hnsw_int8'] = os.path.join(root, 'hnsw_int8_index.faiss')\n",
    "    if 'flat_int8_index.faiss' in files: \n",
    "        paths['flat_int8'] = os.path.join(root, 'flat_int8_index.faiss')\n",
    "\n",
    "print(f\"üìÇ Files found: {[k for k, v in paths.items() if v]}\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. LOAD DATASET (Natural Questions)\n",
    "# ==========================================\n",
    "print(\"\\n[2/5] Loading NQ Dataset...\")\n",
    "qrels_ds = load_dataset(\"Cohere/beir-embed-english-v3\", \"nq-qrels\", split=\"test\")\n",
    "queries_ds = load_dataset(\"Cohere/beir-embed-english-v3\", \"nq-queries\", split=\"test\")\n",
    "corpus_ds = load_dataset(\"Cohere/beir-embed-english-v3\", \"nq-corpus\", split=\"train\")\n",
    "\n",
    "doc_ids_list = corpus_ds['_id']\n",
    "query_ids_ordered = queries_ds['_id']\n",
    "\n",
    "qrels = {}\n",
    "q_key = 'query_id' if 'query_id' in qrels_ds.column_names else 'query-id'\n",
    "c_key = 'corpus_id' if 'corpus_id' in qrels_ds.column_names else 'corpus-id'\n",
    "\n",
    "for row in qrels_ds:\n",
    "    qid = str(row[q_key])\n",
    "    did = str(row[c_key])\n",
    "    if qid not in qrels: qrels[qid] = {}\n",
    "    qrels[qid][did] = int(row['score'])\n",
    "\n",
    "# ==========================================\n",
    "# 3. QUERY ENCODING (WITH INSTRUCTION)\n",
    "# ==========================================\n",
    "print(\"\\n[3/5] Encoding Queries (With BGE Instruction)...\")\n",
    "instruction = \"Represent this sentence for searching relevant passages: \"\n",
    "query_texts = queries_ds['text']\n",
    "queries_with_instruction = [instruction + q for q in query_texts]\n",
    "\n",
    "model = SentenceTransformer('BAAI/bge-base-en-v1.5')\n",
    "query_embeddings = model.encode(queries_with_instruction, batch_size=32, convert_to_numpy=True, normalize_embeddings=True)\n",
    "print(f\"‚úÖ Queries Encoded. Shape: {query_embeddings.shape}\")\n",
    "\n",
    "del model\n",
    "gc.collect()\n",
    "\n",
    "# ==========================================\n",
    "# 4. EVALUATION FUNCTION\n",
    "# ==========================================\n",
    "def evaluate(index, index_name, q_embs, k=10):\n",
    "    print(f\"\\nüîé Testing: {index_name} ...\")\n",
    "    start_time = time.time()\n",
    "    scores, indices = index.search(q_embs, k)\n",
    "    search_time = time.time() - start_time\n",
    "    qps = len(q_embs) / search_time\n",
    "    \n",
    "    ndcg_list = []\n",
    "    recall_list = []\n",
    "    \n",
    "    for i, (res_indices, res_scores) in enumerate(zip(indices, scores)):\n",
    "        if i >= len(query_ids_ordered): break\n",
    "        qid = str(query_ids_ordered[i])\n",
    "        \n",
    "        if qid not in qrels: continue\n",
    "        relevant_docs = qrels[qid]\n",
    "        \n",
    "        retrieved_ids = []\n",
    "        for idx in res_indices:\n",
    "            if idx < len(doc_ids_list):\n",
    "                retrieved_ids.append(str(doc_ids_list[idx]))\n",
    "            else:\n",
    "                retrieved_ids.append(\"-1\") \n",
    "\n",
    "        dcg = 0.0\n",
    "        for rank, doc_id in enumerate(retrieved_ids, 1):\n",
    "            if doc_id in relevant_docs:\n",
    "                dcg += relevant_docs[doc_id] / np.log2(rank + 1)\n",
    "        \n",
    "        ideal_rels = sorted(relevant_docs.values(), reverse=True)[:k]\n",
    "        idcg = sum(r / np.log2(rank + 2) for rank, r in enumerate(ideal_rels))\n",
    "        ndcg_list.append(dcg / idcg if idcg > 0 else 0)\n",
    "\n",
    "        rel_set = set(relevant_docs.keys())\n",
    "        ret_set = set(retrieved_ids)\n",
    "        if len(rel_set) > 0:\n",
    "            recall_list.append(len(rel_set & ret_set) / len(rel_set))\n",
    "            \n",
    "    mean_ndcg = np.mean(ndcg_list)\n",
    "    mean_recall = np.mean(recall_list)\n",
    "    \n",
    "    print(f\"   üëâ Result: nDCG@10={mean_ndcg:.4f} | Recall@10={mean_recall:.4f} | QPS={qps:.1f}\")\n",
    "    \n",
    "    return {\n",
    "        'Method': index_name, \n",
    "        'nDCG@10': round(mean_ndcg, 4), \n",
    "        'Recall@10': round(mean_recall, 4),\n",
    "        'QPS': round(qps, 1)\n",
    "    }\n",
    "\n",
    "# ==========================================\n",
    "# 5. RUN EXPERIMENTS\n",
    "# ==========================================\n",
    "results = []\n",
    "\n",
    "if paths['hnsw_fp32']:\n",
    "    idx = faiss.read_index(paths['hnsw_fp32'])\n",
    "    results.append(evaluate(idx, \"HNSW FP32\", query_embeddings))\n",
    "    del idx; gc.collect()\n",
    "\n",
    "if paths['hnsw_int8']:\n",
    "    idx = faiss.read_index(paths['hnsw_int8'])\n",
    "    results.append(evaluate(idx, \"HNSW INT8\", query_embeddings))\n",
    "    del idx; gc.collect()\n",
    "\n",
    "if paths['flat_int8']:\n",
    "    idx = faiss.read_index(paths['flat_int8'])\n",
    "    results.append(evaluate(idx, \"Flat INT8\", query_embeddings))\n",
    "    del idx; gc.collect()\n",
    "\n",
    "if paths['doc_emb']:\n",
    "    print(\"\\n------------------------------------------------\")\n",
    "    print(\"4/4. Building Flat FP32 (Exact Search) in Memory...\")\n",
    "    try:\n",
    "        doc_embeddings = np.load(paths['doc_emb'], mmap_mode='r')\n",
    "        d = doc_embeddings.shape[1]\n",
    "        idx = faiss.IndexFlatIP(d)\n",
    "        idx.add(np.array(doc_embeddings)) \n",
    "        results.append(evaluate(idx, \"Flat FP32 (Exact)\", query_embeddings))\n",
    "        del idx; del doc_embeddings; gc.collect()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Flat FP32 skipped due to memory limit: {e}\")\n",
    "\n",
    "# ==========================================\n",
    "# 6. SAVE RESULTS\n",
    "# ==========================================\n",
    "print(\"\\nüèÜ FINAL BENCHMARK RESULTS:\")\n",
    "df = pd.DataFrame(results)\n",
    "print(df)\n",
    "\n",
    "df.to_csv(\"final_results_nq.csv\", index=False)\n",
    "print(\"\\n‚úÖ Results saved to 'final_results_nq.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä BM25 Baseline Calculation (Sparse Retrieval)\n",
    "\n",
    "In this section, we compute the **BM25** scores to serve as a standardized baseline for our dense retrieval experiments (Table 1 & 3 replication).\n",
    "\n",
    "We use the `bm25s` library, a high-performance Python implementation that allows indexing the entire Natural Questions (NQ) corpus (2.6M documents) efficiently in RAM without external dependencies like Java/Lucene.\n",
    "\n",
    "**Metrics computed:** nDCG@10, Recall@10, and QPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T17:37:58.790170Z",
     "iopub.status.busy": "2026-01-29T17:37:58.787246Z",
     "iopub.status.idle": "2026-01-29T18:03:30.178701Z",
     "shell.execute_reply": "2026-01-29T18:03:30.176224Z",
     "shell.execute_reply.started": "2026-01-29T17:37:58.790115Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# BM25 CALCULATION (SPARSE BASELINE)\n",
    "# ==========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "print(\"üöÄ STARTING BM25 CALCULATION (Using fast 'bm25s' library)...\")\n",
    "\n",
    "# 1. Install optimized libraries\n",
    "#    We install 'PyStemmer' (wrapper for C Stemming lib) to make tokenization fast.\n",
    "#    Note: Ignore any \"JAX/CUDA\" warnings in the output; they are irrelevant for CPU execution.\n",
    "!pip install PyStemmer bm25s --no-deps\n",
    "\n",
    "import bm25s\n",
    "import Stemmer \n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# A. DATA PREPARATION\n",
    "# ---------------------------------------------------------\n",
    "# BM25 operates on raw text. We extract it from the datasets loaded in memory.\n",
    "print(\"   üì• Extracting raw text from Corpus and Queries...\")\n",
    "corpus_texts = corpus_ds['text']\n",
    "query_texts = queries_ds['text']\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# B. TOKENIZATION & STEMMING\n",
    "# ---------------------------------------------------------\n",
    "print(\"   ‚úÇÔ∏è  Tokenizing Corpus (this may take 2-4 minutes)...\")\n",
    "# We use the English stemmer to reduce words to their root (e.g., \"running\" -> \"run\")\n",
    "stemmer = Stemmer.Stemmer(\"english\")\n",
    "corpus_tokens = bm25s.tokenize(corpus_texts, stopwords=\"en\", stemmer=stemmer)\n",
    "query_tokens = bm25s.tokenize(query_texts, stopwords=\"en\", stemmer=stemmer)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# C. INDEX CONSTRUCTION\n",
    "# ---------------------------------------------------------\n",
    "print(\"   üèóÔ∏è  Building BM25 Index...\")\n",
    "retriever = bm25s.BM25()\n",
    "retriever.index(corpus_tokens)\n",
    "print(\"   ‚úÖ Index built successfully!\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# D. RETRIEVAL (SEARCH)\n",
    "# ---------------------------------------------------------\n",
    "print(\"   üîé Executing Search (Top-10 for all queries)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform search\n",
    "results, scores = retriever.retrieve(query_tokens, k=10)\n",
    "\n",
    "search_time = time.time() - start_time\n",
    "qps_bm25 = len(query_texts) / search_time\n",
    "print(f\"   ‚úÖ Search completed in {search_time:.2f}s (QPS: {qps_bm25:.1f})\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# E. EVALUATION (nDCG & Recall)\n",
    "# ---------------------------------------------------------\n",
    "print(\"   üìä Calculating Metrics...\")\n",
    "ndcg_list = []\n",
    "recall_list = []\n",
    "\n",
    "for i, res_indices in enumerate(results):\n",
    "    # Safety check for index alignment\n",
    "    if i >= len(query_ids_ordered): break\n",
    "    \n",
    "    qid = str(query_ids_ordered[i])\n",
    "    \n",
    "    # Skip if we don't have ground truth for this query\n",
    "    if qid not in qrels: continue\n",
    "    \n",
    "    relevant_docs = qrels[qid]\n",
    "    \n",
    "    # Map internal BM25 integer IDs back to dataset String IDs\n",
    "    retrieved_ids = []\n",
    "    for idx in res_indices:\n",
    "        if idx < len(doc_ids_list):\n",
    "            retrieved_ids.append(str(doc_ids_list[idx]))\n",
    "        else:\n",
    "            retrieved_ids.append(\"-1\")\n",
    "\n",
    "    # --- nDCG Calculation ---\n",
    "    dcg = 0.0\n",
    "    for rank, doc_id in enumerate(retrieved_ids, 1):\n",
    "        if doc_id in relevant_docs:\n",
    "            dcg += relevant_docs[doc_id] / np.log2(rank + 1)\n",
    "    \n",
    "    ideal_rels = sorted(relevant_docs.values(), reverse=True)[:10]\n",
    "    idcg = sum(r / np.log2(rank + 2) for rank, r in enumerate(ideal_rels))\n",
    "    ndcg_list.append(dcg / idcg if idcg > 0 else 0)\n",
    "\n",
    "    # --- Recall Calculation ---\n",
    "    rel_set = set(relevant_docs.keys())\n",
    "    ret_set = set(retrieved_ids)\n",
    "    if len(rel_set) > 0:\n",
    "        recall_list.append(len(rel_set & ret_set) / len(rel_set))\n",
    "\n",
    "mean_ndcg_bm25 = np.mean(ndcg_list)\n",
    "mean_recall_bm25 = np.mean(recall_list)\n",
    "\n",
    "print(f\"\\nüèÜ BM25 RESULTS:\")\n",
    "print(f\"   üëâ nDCG@10:   {mean_ndcg_bm25:.4f}\")\n",
    "print(f\"   üëâ Recall@10: {mean_recall_bm25:.4f}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# F. UPDATE RESULTS TABLE\n",
    "# ---------------------------------------------------------\n",
    "bm25_result = {\n",
    "    'Method': 'BM25 (Sparse)', \n",
    "    'nDCG@10': mean_ndcg_bm25, \n",
    "    'Recall@10': mean_recall_bm25, \n",
    "    'QPS': qps_bm25\n",
    "}\n",
    "\n",
    "# Load previous results, append BM25, and save\n",
    "try:\n",
    "    csv_path = \"final_results_nq.csv\"\n",
    "    if os.path.exists(csv_path):\n",
    "        df_final = pd.read_csv(csv_path)\n",
    "        # Remove old BM25 entry if it exists to avoid duplicates\n",
    "        df_final = df_final[df_final['Method'] != 'BM25 (Sparse)']\n",
    "        # Add new result\n",
    "        df_final = pd.concat([df_final, pd.DataFrame([bm25_result])], ignore_index=True)\n",
    "    else:\n",
    "        df_final = pd.DataFrame([bm25_result])\n",
    "    \n",
    "    print(\"\\nüìÑ UPDATED FINAL TABLE:\")\n",
    "    print(df_final)\n",
    "    df_final.to_csv(\"final_results_with_bm25.csv\", index=False)\n",
    "    print(\"‚úÖ Saved to 'final_results_with_bm25.csv'\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error updating CSV: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T18:05:01.330821Z",
     "iopub.status.busy": "2026-01-29T18:05:01.330259Z",
     "iopub.status.idle": "2026-01-29T18:05:01.447721Z",
     "shell.execute_reply": "2026-01-29T18:05:01.446604Z",
     "shell.execute_reply.started": "2026-01-29T18:05:01.330773Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# FINAL PAPER TABLE GENERATION\n",
    "# ==========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load the final results\n",
    "df = pd.read_csv(\"final_results_with_bm25.csv\")\n",
    "\n",
    "# Constants for NQ Dataset (needed for memory calculation)\n",
    "N_DOCS = 2681468   # Number of documents in NQ\n",
    "DIM = 768          # Dimension of BGE-Base vectors\n",
    "\n",
    "# ==========================================\n",
    "# CALCULATE MISSING METRICS (Table 4)\n",
    "# ==========================================\n",
    "\n",
    "def calculate_theoretical_memory(method_name):\n",
    "    \"\"\"Calculates RAM usage in GB based on vector size.\"\"\"\n",
    "    if \"BM25\" in method_name:\n",
    "        # BM25 index size varies, but typically smaller than dense FP32.\n",
    "        # Estimate: ~0.5 GB for vocabulary + inverted index of this size\n",
    "        return 0.50 \n",
    "    \n",
    "    # Dense Retrieval Logic\n",
    "    if \"INT8\" in method_name:\n",
    "        bytes_per_vec = DIM * 1  # 1 byte per dimension (Quantized)\n",
    "    else: \n",
    "        bytes_per_vec = DIM * 4  # 4 bytes per dimension (FP32)\n",
    "    \n",
    "    raw_size_gb = (N_DOCS * bytes_per_vec) / (1024**3)\n",
    "    \n",
    "    # HNSW adds graph overhead (links between nodes). \n",
    "    # Typically +20-30% overhead compared to raw vectors.\n",
    "    if \"HNSW\" in method_name:\n",
    "        return raw_size_gb * 1.25\n",
    "    else:\n",
    "        return raw_size_gb\n",
    "\n",
    "# 1. Compute Memory (GB)\n",
    "df['Memory (GB)'] = df['Method'].apply(calculate_theoretical_memory).round(2)\n",
    "\n",
    "# 2. Compute Latency (ms per query) -> Latency = 1000 / QPS\n",
    "df['Latency (ms)'] = (1000 / df['QPS']).round(2)\n",
    "\n",
    "# ==========================================\n",
    "# FORMATTING TABLES\n",
    "# ==========================================\n",
    "\n",
    "# --- TABLE 3 REPLICATION: Effectiveness ---\n",
    "# Focus on nDCG and Recall\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"üìÑ TABLE 3: RETRIEVAL EFFECTIVENESS\")\n",
    "print(\"=\"*40)\n",
    "table_3 = df[['Method', 'nDCG@10', 'Recall@10']].sort_values(by='nDCG@10', ascending=False)\n",
    "print(table_3.to_markdown(index=False))\n",
    "\n",
    "\n",
    "# --- TABLE 4 REPLICATION: Efficiency ---\n",
    "# Focus on Memory, QPS, and Latency\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"‚öôÔ∏è TABLE 4: RETRIEVAL EFFICIENCY\")\n",
    "print(\"=\"*40)\n",
    "# We sort by QPS (Speed) to show the fastest first\n",
    "table_4 = df[['Method', 'Memory (GB)', 'QPS', 'Latency (ms)']].sort_values(by='QPS', ascending=False)\n",
    "print(table_4.to_markdown(index=False))\n",
    "\n",
    "# --- SAVE FORMATTED DATA ---\n",
    "df.to_csv(\"paper_final_tables.csv\", index=False)\n",
    "print(\"\\n‚úÖ Formatted tables saved to 'paper_final_tables.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 9368954,
     "sourceId": 14665319,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 294386801,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

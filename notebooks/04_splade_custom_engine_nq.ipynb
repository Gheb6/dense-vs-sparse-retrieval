{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cbbd7d3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# ‚ö° Notebook 4: SPLADE Custom Matrix Engine (Experimental)\n",
    "**Author:** Gabriele Righi\n",
    "\n",
    "**Project:** Dense vs Sparse Retrieval Reproducibility\n",
    "\n",
    "## üéØ Objective\n",
    "This notebook implements a **custom \"Pure Python\" search engine** for the SPLADE model.\n",
    "It serves as the engineering solution to the **\"Sparse Indexing Challenge\"**: standard tools (like Pyserini) introduced quantization artifacts that degraded SPLADE's performance. Here, we implement exact sparse retrieval to measure the true quality of the model.\n",
    "\n",
    "## üõ†Ô∏è Methodology (The \"Matrix Engine\")\n",
    "Instead of using a traditional Inverted Index (Lucene), this notebook treats retrieval as a **Linear Algebra** problem:\n",
    "1.  **Model:** Uses `naver/splade-cocondenser-ensembledistil` to generate sparse weights.\n",
    "2.  **Structure:** Constructs a **SciPy CSR Matrix** ($D$) to store document vectors efficiently.\n",
    "3.  **Search:** Performs retrieval via **Sparse Matrix Multiplication** ($S = Q \\times D^T$).\n",
    "    * *Advantage:* Preserves full `float32` precision (avoiding the quality loss of integer quantization).\n",
    "\n",
    "## üìâ Scope & Constraints\n",
    "Due to the computational cost of encoding **2.6 Million documents** with a BERT-based model (approx. 5+ hours on a single T4 GPU), this notebook performs a **Fast Estimation** on a subset:\n",
    "* **Subset Size:** First **100,000 documents** of Natural Questions (NQ).\n",
    "* **Goal:** To obtain a representative **nDCG@10** score for the comparison table without the massive time overhead.\n",
    "\n",
    "## üìÇ Inputs & Outputs\n",
    "* **Input:** Raw text from NQ Dataset (`Cohere/beir-embed-english-v3`).\n",
    "* **Output:** Evaluation metrics (nDCG, Recall) for SPLADE on the specified subset.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6c2ed5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import collections\n",
    "# ==========================================\n",
    "# 0. CONFIGURATION (UPDATED)\n",
    "# ==========================================\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 32   \n",
    "MAX_DOCS = 100000 \n",
    "MODEL_ID = \"naver/splade-cocondenser-ensembledistil\"\n",
    "\n",
    "def encode_splade(texts, batch_size=32):\n",
    "    \"\"\"\n",
    "    Generates sparse vectors with Memory Optimization (AMP + Smaller Batches).\n",
    "    \"\"\"\n",
    "    all_indices = []\n",
    "    all_values = []\n",
    "    all_indptr = [0]\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding Batch\"):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        \n",
    "        # Tokenization\n",
    "        inputs = tokenizer(\n",
    "            batch_texts, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=256\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with torch.amp.autocast('cuda'): \n",
    "                logits = model(**inputs).logits\n",
    "                \n",
    "                # SPLADE LOGIC inside autocast\n",
    "                values = torch.log(1 + torch.relu(logits))\n",
    "                batch_scores, _ = torch.max(values, dim=1)\n",
    "        \n",
    "        # Move to CPU immediately to free GPU VRAM\n",
    "        batch_scores = batch_scores.float().cpu().numpy() # Cast back to float32 for numpy\n",
    "        \n",
    "        # Clear GPU cache explicitely if needed (optional but safer)\n",
    "        del inputs, logits, values\n",
    "        \n",
    "        for score_vec in batch_scores:\n",
    "            non_zero_indices = np.nonzero(score_vec)[0]\n",
    "            non_zero_values = score_vec[non_zero_indices]\n",
    "            \n",
    "            all_indices.extend(non_zero_indices)\n",
    "            all_values.extend(non_zero_values)\n",
    "            all_indptr.append(len(all_indices))\n",
    "            \n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    return csr_matrix(\n",
    "        (all_values, all_indices, all_indptr), \n",
    "        shape=(len(texts), vocab_size),\n",
    "        dtype=np.float32\n",
    "    )\n",
    "\n",
    "# ==========================================\n",
    "# 2. DATASET LOADING & PREPROCESSING\n",
    "# ==========================================\n",
    "print(f\"\\n[2/4] Loading NQ Dataset (First {MAX_DOCS} docs)...\")\n",
    "corpus_ds = load_dataset(\"Cohere/beir-embed-english-v3\", \"nq-corpus\", split=\"train\")\n",
    "queries_ds = load_dataset(\"Cohere/beir-embed-english-v3\", \"nq-queries\", split=\"test\")\n",
    "qrels_ds = load_dataset(\"Cohere/beir-embed-english-v3\", \"nq-qrels\", split=\"test\")\n",
    "\n",
    "# Optimization: Slice the corpus to speed up the process\n",
    "corpus_ds = corpus_ds.select(range(MAX_DOCS))\n",
    "\n",
    "# Combining Title + Text for better recall (Standard SPLADE practice)\n",
    "doc_texts = [f\"{t} {txt}\" for t, txt in zip(corpus_ds['title'], corpus_ds['text'])]\n",
    "doc_ids = corpus_ds['_id']\n",
    "\n",
    "# --- ROBUST QREL LOADING (Fix for KeyError) ---\n",
    "print(\"Mapping QRELs (Ground Truth)...\")\n",
    "qrels = collections.defaultdict(dict)\n",
    "\n",
    "# Automatically detect column names (handles 'query-id' vs 'query_id')\n",
    "cols = qrels_ds.column_names\n",
    "q_key = 'query_id' if 'query_id' in cols else 'query-id'\n",
    "c_key = 'corpus_id' if 'corpus_id' in cols else 'corpus-id'\n",
    "s_key = 'score'\n",
    "\n",
    "print(f\"Detected columns -> Query: '{q_key}', Corpus: '{c_key}'\")\n",
    "\n",
    "for row in tqdm(qrels_ds, desc=\"Processing QRELs\"):\n",
    "    qid = str(row[q_key])\n",
    "    did = str(row[c_key])\n",
    "    score = int(row[s_key])\n",
    "    qrels[qid][did] = score\n",
    "\n",
    "# ==========================================\n",
    "# 3. ENCODING (The Heavy Lifting)\n",
    "# ==========================================\n",
    "print(\"\\n[3/4] Encoding Corpus (Matrix D)...\")\n",
    "doc_matrix = encode_splade(doc_texts, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(\"Encoding Queries (Matrix Q)...\")\n",
    "query_matrix = encode_splade(queries_ds['text'], batch_size=BATCH_SIZE)\n",
    "\n",
    "# Convert Document Matrix to CSC (Compressed Sparse Column)\n",
    "# This makes column slicing and transposition much faster for the dot product\n",
    "doc_matrix_t = doc_matrix.T.tocsc()\n",
    "\n",
    "# Clean up raw text to save RAM\n",
    "del doc_texts, model\n",
    "gc.collect()\n",
    "\n",
    "# ==========================================\n",
    "# 4. SEARCH & EVALUATION\n",
    "# ==========================================\n",
    "print(\"\\n[4/4] Running Search (Matrix Multiplication S = Q * D^T)...\")\n",
    "\n",
    "k = 10\n",
    "ndcg_list = []\n",
    "doc_ids_lookup = np.array(doc_ids) # Faster numpy access for retrieval\n",
    "\n",
    "for i in tqdm(range(query_matrix.shape[0]), desc=\"Evaluating\"):\n",
    "    # Get Query ID (using the robust key detection logic if needed, but usually '_id' is standard here)\n",
    "    qid = str(queries_ds[i]['_id'])\n",
    "    \n",
    "    # Skip if query has no relevant documents in the ground truth\n",
    "    if qid not in qrels: continue\n",
    "    \n",
    "    # Get current query vector (Sparse)\n",
    "    q_vec = query_matrix[i]\n",
    "    \n",
    "    # Dot Product: (1, Vocab) @ (Vocab, Docs) -> (1, Docs)\n",
    "    # This calculates the score for this query against ALL 100k documents\n",
    "    scores = q_vec.dot(doc_matrix_t).toarray().flatten()\n",
    "    \n",
    "    # Fast Top-K Retrieval\n",
    "    if len(scores) > k:\n",
    "        # argpartition is O(n), much faster than O(n log n) full sort\n",
    "        top_k_idx = np.argpartition(scores, -k)[-k:]\n",
    "        # Sort only the top k results\n",
    "        top_k_idx = top_k_idx[np.argsort(scores[top_k_idx])][::-1]\n",
    "    else:\n",
    "        top_k_idx = np.argsort(scores)[::-1]\n",
    "        \n",
    "    retrieved_ids = doc_ids_lookup[top_k_idx]\n",
    "    \n",
    "    # --- Metric Calculation (nDCG@10) ---\n",
    "    relevant_docs = qrels[qid]\n",
    "    \n",
    "    dcg = 0.0\n",
    "    idcg = 0.0\n",
    "    \n",
    "    # Calculate DCG (Discounted Cumulative Gain)\n",
    "    for rank, doc_id in enumerate(retrieved_ids):\n",
    "        if doc_id in relevant_docs:\n",
    "            rel_score = relevant_docs[doc_id]\n",
    "            dcg += rel_score / np.log2(rank + 2)\n",
    "            \n",
    "    # Calculate IDCG (Ideal DCG)\n",
    "    ideal_rels = sorted(relevant_docs.values(), reverse=True)[:k]\n",
    "    for rank, rel_score in enumerate(ideal_rels):\n",
    "        idcg += rel_score / np.log2(rank + 2)\n",
    "        \n",
    "    ndcg_list.append(dcg / idcg if idcg > 0 else 0)\n",
    "\n",
    "# ==========================================\n",
    "# FINAL RESULTS\n",
    "# ==========================================\n",
    "mean_ndcg = np.mean(ndcg_list)\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\"üèÜ SPLADE RESULT (NQ Subset {MAX_DOCS})\")\n",
    "print(f\"nDCG@10:    {mean_ndcg:.4f}\")\n",
    "print(\"=\"*40)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

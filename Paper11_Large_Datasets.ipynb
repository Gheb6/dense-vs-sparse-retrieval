{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400e7cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q sentence-transformers\n",
    "!pip install -q faiss-cpu\n",
    "!pip install -q beir\n",
    "!pip install -q rank-bm25\n",
    "!pip install -q pandas matplotlib seaborn\n",
    "\n",
    "print(\"âœ… Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df981ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "from beir import util\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "\n",
    "# BM25\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"âœ… Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bf6fa7",
   "metadata": {},
   "source": [
    "## Dataset Selection\n",
    "Choose a dataset. Defaults to FiQA for medium scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6c81ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select dataset: 'scifact', 'fiqa', or 'nq'\n",
    "dataset_name = 'fiqa'  # change to 'nq' only if your machine can handle it\n",
    "\n",
    "dataset_urls = {\n",
    "    'scifact': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/scifact.zip',\n",
    "    'fiqa': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/fiqa.zip',\n",
    "    # Note: NQ is very large; ensure sufficient resources\n",
    "    'nq': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/nq.zip',\n",
    "}\n",
    "\n",
    "url = dataset_urls[dataset_name]\n",
    "print(f\"Downloading {dataset_name} dataset...\")\n",
    "data_path = util.download_and_unzip(url, \"datasets\")\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "corpus, queries, qrels = GenericDataLoader(data_folder=data_path).load(split=\"test\")\n",
    "\n",
    "print(f\"\\nâœ… Dataset loaded!\\n   Documents: {len(corpus):,}\\n   Queries: {len(queries):,}\\n   Relevance judgments: {len(qrels):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13a5730",
   "metadata": {},
   "source": [
    "## Prepare Model and Texts\n",
    "Memory-safe batching for larger corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108593e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding model\n",
    "model_name = 'BAAI/bge-base-en-v1.5'\n",
    "print(f\"Loading model: {model_name}\")\n",
    "model = SentenceTransformer(model_name)\n",
    "dimension = model.get_sentence_embedding_dimension()\n",
    "\n",
    "# Prepare texts\n",
    "doc_ids = list(corpus.keys())\n",
    "doc_texts = [corpus[did]['title'] + ' ' + corpus[did]['text'] for did in doc_ids]\n",
    "query_ids = list(queries.keys())\n",
    "query_texts = [queries[qid] for qid in query_ids]\n",
    "\n",
    "print(f\"âœ… Model loaded (dim={dimension})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d7ac86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode documents with memory-safe batching\n",
    "batch_size_docs = 32 if len(doc_texts) <= 100_000 else 16\n",
    "print(f\"Encoding {len(doc_texts):,} documents (batch_size={batch_size_docs})...\")\n",
    "\n",
    "doc_embeddings = model.encode(\n",
    "    doc_texts,\n",
    "    batch_size=batch_size_docs,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True\n",
    ")\n",
    "\n",
    "print(f\"âœ… Documents encoded! Shape: {doc_embeddings.shape}, Memory: {doc_embeddings.nbytes / (1024**2):.2f} MB\")\n",
    "\n",
    "# Encode queries\n",
    "batch_size_queries = 32\n",
    "print(f\"Encoding {len(query_texts):,} queries (batch_size={batch_size_queries})...\")\n",
    "query_embeddings = model.encode(\n",
    "    query_texts,\n",
    "    batch_size=batch_size_queries,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True\n",
    ")\n",
    "print(f\"âœ… Queries encoded! Shape: {query_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f488e6",
   "metadata": {},
   "source": [
    "## Build Indexes\n",
    "HNSW for medium/large datasets, INT8 quantization for memory reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c281a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flat Index (baseline)\n",
    "print(\"Building Flat Index (exact search)...\")\n",
    "start_time = time.time()\n",
    "flat_index = faiss.IndexFlatIP(dimension)\n",
    "flat_index.add(doc_embeddings.astype('float32'))\n",
    "build_time_flat = time.time() - start_time\n",
    "print(f\"âœ… Flat Index built in {build_time_flat:.3f}s (vectors={flat_index.ntotal:,})\")\n",
    "\n",
    "# HNSW Index\n",
    "M = 16 if len(doc_ids) < 200_000 else 32\n",
    "ef_construction = 100\n",
    "ef_search = 50 if len(doc_ids) < 200_000 else 100\n",
    "\n",
    "print(\"Building HNSW Index...\")\n",
    "start_time = time.time()\n",
    "hnsw_index = faiss.IndexHNSWFlat(dimension, M)\n",
    "hnsw_index.hnsw.efConstruction = ef_construction\n",
    "hnsw_index.add(doc_embeddings.astype('float32'))\n",
    "hnsw_index.hnsw.efSearch = ef_search\n",
    "build_time_hnsw = time.time() - start_time\n",
    "print(f\"âœ… HNSW built in {build_time_hnsw:.3f}s (M={M}, efSearch={ef_search})\")\n",
    "\n",
    "# HNSW + INT8 Quantization\n",
    "print(\"Building HNSW INT8 Index...\")\n",
    "start_time = time.time()\n",
    "hnsw_int8_index = faiss.IndexHNSWSQ(dimension, faiss.ScalarQuantizer.QT_8bit, M)\n",
    "hnsw_int8_index.hnsw.efConstruction = ef_construction\n",
    "hnsw_int8_index.train(doc_embeddings.astype('float32'))\n",
    "hnsw_int8_index.add(doc_embeddings.astype('float32'))\n",
    "hnsw_int8_index.hnsw.efSearch = ef_search\n",
    "build_time_hnsw_int8 = time.time() - start_time\n",
    "print(f\"âœ… HNSW INT8 built in {build_time_hnsw_int8:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c953796",
   "metadata": {},
   "source": [
    "## BM25 Baseline\n",
    "Tokenize and build BM25 index over documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e11bad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenize(text):\n",
    "    text = text.lower()\n",
    "    return re.findall(r'\\b\\w+\\b', text)\n",
    "\n",
    "print(\"Tokenizing documents for BM25...\")\n",
    "doc_texts_tokenized = [simple_tokenize(corpus[did]['title'] + ' ' + corpus[did]['text']) for did in doc_ids]\n",
    "\n",
    "print(\"Building BM25 index...\")\n",
    "start_time = time.time()\n",
    "bm25_index = BM25Okapi(doc_texts_tokenized)\n",
    "build_time_bm25 = time.time() - start_time\n",
    "print(f\"âœ… BM25 built in {build_time_bm25:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8919ed98",
   "metadata": {},
   "source": [
    "## Search Functions\n",
    "Shared utilities to run and measure searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee2b676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_and_measure(index, query_embeddings, k=10, name=\"Index\"):\n",
    "    print(f\"\\nSearching with {name}...\")\n",
    "    latencies = []; all_indices = []; all_scores = []\n",
    "    for query_emb in tqdm(query_embeddings, desc=f\"{name} search\"):\n",
    "        start = time.time()\n",
    "        scores, indices = index.search(query_emb.reshape(1, -1).astype('float32'), k)\n",
    "        latencies.append((time.time() - start) * 1000)\n",
    "        all_indices.append(indices[0])\n",
    "        all_scores.append(scores[0])\n",
    "    latencies = np.array(latencies)\n",
    "    return {\n",
    "        'name': name,\n",
    "        'indices': np.array(all_indices),\n",
    "        'scores': np.array(all_scores),\n",
    "        'latencies': latencies,\n",
    "        'median_latency': np.median(latencies),\n",
    "        'p95_latency': np.percentile(latencies, 95),\n",
    "        'p99_latency': np.percentile(latencies, 99),\n",
    "    }\n",
    "\n",
    "def search_bm25(bm25_index, query_texts, k=10):\n",
    "    print(\"\\nSearching with BM25...\")\n",
    "    latencies = []; all_indices = []; all_scores = []\n",
    "    for q in tqdm(query_texts, desc=\"BM25 search\"):\n",
    "        tokens = simple_tokenize(q)\n",
    "        start = time.time()\n",
    "        scores = bm25_index.get_scores(tokens)\n",
    "        latencies.append((time.time() - start) * 1000)\n",
    "        top_idx = np.argsort(-scores)[:k]\n",
    "        all_indices.append(top_idx)\n",
    "        all_scores.append(scores[top_idx])\n",
    "    latencies = np.array(latencies)\n",
    "    return {\n",
    "        'name': 'BM25',\n",
    "        'indices': np.array(all_indices),\n",
    "        'scores': np.array(all_scores),\n",
    "        'latencies': latencies,\n",
    "        'median_latency': np.median(latencies),\n",
    "        'p95_latency': np.percentile(latencies, 95),\n",
    "        'p99_latency': np.percentile(latencies, 99),\n",
    "    }\n",
    "\n",
    "def merge_rankings(dense_indices, dense_scores, sparse_indices, sparse_scores, k=10, alpha=0.5):\n",
    "    merged = {}\n",
    "    for rank, (idx, s) in enumerate(zip(dense_indices, dense_scores), 1):\n",
    "        doc_id = doc_ids[idx]\n",
    "        merged[doc_id] = merged.get(doc_id, 0) + alpha / (60 + rank)\n",
    "    for rank, (idx, s) in enumerate(zip(sparse_indices, sparse_scores), 1):\n",
    "        doc_id = doc_ids[idx]\n",
    "        merged[doc_id] = merged.get(doc_id, 0) + (1 - alpha) / (60 + rank)\n",
    "    ranked = sorted(merged.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "    return np.array([doc_ids.index(doc_id) for doc_id, _ in ranked])\n",
    "\n",
    "def hybrid_search(dense_index, query_embeddings, bm25_index, query_texts, alpha=0.5, k=10):\n",
    "    print(f\"\\nSearching with Hybrid (Î±={alpha})...\")\n",
    "    latencies = []; all_indices = []\n",
    "    for emb, q in tqdm(zip(query_embeddings, query_texts), total=len(query_embeddings), desc=\"Hybrid search\"):\n",
    "        start = time.time()\n",
    "        dscores, dindices = dense_index.search(emb.reshape(1, -1).astype('float32'), k)\n",
    "        dindices = dindices[0]; dscores = dscores[0]\n",
    "        tokens = simple_tokenize(q)\n",
    "        sscores = bm25_index.get_scores(tokens)\n",
    "        sindices = np.argsort(-sscores)[:k]\n",
    "        sscores = sscores[sindices]\n",
    "        merged = merge_rankings(dindices, dscores, sindices, sscores, k=k, alpha=alpha)\n",
    "        latencies.append((time.time() - start) * 1000)\n",
    "        all_indices.append(merged)\n",
    "    latencies = np.array(latencies)\n",
    "    return {\n",
    "        'name': f\"Hybrid (Î±={alpha})\",\n",
    "        'indices': np.array(all_indices),\n",
    "        'latencies': latencies,\n",
    "        'median_latency': np.median(latencies),\n",
    "        'p95_latency': np.percentile(latencies, 95),\n",
    "        'p99_latency': np.percentile(latencies, 99),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9b77d3",
   "metadata": {},
   "source": [
    "## Run Searches\n",
    "Collect top-10 results and latency stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8bb9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "results_flat = search_and_measure(flat_index, query_embeddings, k=k, name=\"Flat\")\n",
    "results_hnsw = search_and_measure(hnsw_index, query_embeddings, k=k, name=\"HNSW\")\n",
    "results_hnsw_int8 = search_and_measure(hnsw_int8_index, query_embeddings, k=k, name=\"HNSW-INT8\")\n",
    "results_bm25 = search_bm25(bm25_index, query_texts, k=k)\n",
    "\n",
    "# Hybrid runs with different Î±\n",
    "alpha_values = [0.3, 0.5, 0.7]\n",
    "hybrid_results = []\n",
    "for alpha in alpha_values:\n",
    "    res = hybrid_search(hnsw_index, query_embeddings, bm25_index, query_texts, alpha=alpha, k=k)\n",
    "    hybrid_results.append(res)\n",
    "print(\"âœ… Searches complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db203735",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Compute Recall@10 and nDCG@10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899dbb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall(retrieved_indices, qrels, query_ids, doc_ids, k=10):\n",
    "    recalls = []\n",
    "    for i, qid in enumerate(query_ids):\n",
    "        if qid not in qrels:\n",
    "            continue\n",
    "        relevant_docs = set(qrels[qid].keys())\n",
    "        retrieved_docs = set([doc_ids[idx] for idx in retrieved_indices[i][:k] if idx >= 0])\n",
    "        if len(relevant_docs) > 0:\n",
    "            recalls.append(len(relevant_docs & retrieved_docs) / len(relevant_docs))\n",
    "    return np.mean(recalls) if recalls else 0.0\n",
    "\n",
    "def calculate_ndcg(retrieved_indices, qrels, query_ids, doc_ids, k=10):\n",
    "    ndcgs = []\n",
    "    for i, qid in enumerate(query_ids):\n",
    "        if qid not in qrels:\n",
    "            continue\n",
    "        relevant_docs = qrels[qid]\n",
    "        retrieved_docs = [doc_ids[idx] for idx in retrieved_indices[i][:k] if idx >= 0]\n",
    "        dcg = 0\n",
    "        for rank, doc_id in enumerate(retrieved_docs, 1):\n",
    "            rel = relevant_docs.get(doc_id, 0)\n",
    "            dcg += (2 ** rel - 1) / np.log2(rank + 1)\n",
    "        ideal = sorted(relevant_docs.values(), reverse=True)[:k]\n",
    "        idcg = sum((2 ** r - 1) / np.log2(rank + 2) for rank, r in enumerate(ideal))\n",
    "        ndcgs.append(dcg / idcg if idcg > 0 else 0)\n",
    "    return np.mean(ndcgs) if ndcgs else 0.0\n",
    "\n",
    "# Evaluate all\n",
    "for results in [results_flat, results_hnsw, results_hnsw_int8, results_bm25] + hybrid_results:\n",
    "    results['recall@10'] = calculate_recall(results['indices'], qrels, query_ids, doc_ids, k=10)\n",
    "    results['ndcg@10'] = calculate_ndcg(results['indices'], qrels, query_ids, doc_ids, k=10)\n",
    "\n",
    "print(\"âœ… Evaluation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3509ddc",
   "metadata": {},
   "source": [
    "## Comparison Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4440b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = pd.DataFrame([\n",
    "    {\n",
    "        'Method': 'Flat',\n",
    "        'Type': 'Dense',\n",
    "        'Recall@10': results_flat['recall@10'],\n",
    "        'nDCG@10': results_flat['ndcg@10'],\n",
    "        'Median Latency (ms)': results_flat['median_latency'],\n",
    "        'P95 Latency (ms)': results_flat['p95_latency'],\n",
    "    },\n",
    "    {\n",
    "        'Method': 'HNSW',\n",
    "        'Type': 'Dense',\n",
    "        'Recall@10': results_hnsw['recall@10'],\n",
    "        'nDCG@10': results_hnsw['ndcg@10'],\n",
    "        'Median Latency (ms)': results_hnsw['median_latency'],\n",
    "        'P95 Latency (ms)': results_hnsw['p95_latency'],\n",
    "    },\n",
    "    {\n",
    "        'Method': 'HNSW-INT8',\n",
    "        'Type': 'Dense',\n",
    "        'Recall@10': results_hnsw_int8['recall@10'],\n",
    "        'nDCG@10': results_hnsw_int8['ndcg@10'],\n",
    "        'Median Latency (ms)': results_hnsw_int8['median_latency'],\n",
    "        'P95 Latency (ms)': results_hnsw_int8['p95_latency'],\n",
    "    },\n",
    "    {\n",
    "        'Method': 'BM25',\n",
    "        'Type': 'Sparse',\n",
    "        'Recall@10': results_bm25['recall@10'],\n",
    "        'nDCG@10': results_bm25['ndcg@10'],\n",
    "        'Median Latency (ms)': results_bm25['median_latency'],\n",
    "        'P95 Latency (ms)': results_bm25['p95_latency'],\n",
    "    },\n",
    "])\n",
    "\n",
    "# Hybrid rows\n",
    "hybrid_rows = []\n",
    "for res in hybrid_results:\n",
    "    hybrid_rows.append({\n",
    "        'Method': res['name'],\n",
    "        'Type': 'Hybrid',\n",
    "        'Recall@10': res['recall@10'],\n",
    "        'nDCG@10': res['ndcg@10'],\n",
    "        'Median Latency (ms)': res['median_latency'],\n",
    "        'P95 Latency (ms)': res['p95_latency'],\n",
    "    })\n",
    "comparison_df = pd.concat([comparison_df, pd.DataFrame(hybrid_rows)], ignore_index=True)\n",
    "\n",
    "print(\"\\nðŸ“Š COMPARISON TABLE\")\n",
    "print(\"=\"*100)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1591db30",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8205bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speed vs Quality\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "for _, row in comparison_df.iterrows():\n",
    "    color = 'orange' if row['Type'] == 'Sparse' else ('green' if row['Type'] == 'Hybrid' else 'steelblue')\n",
    "    ax.scatter(row['Median Latency (ms)'], row['nDCG@10'], s=200, alpha=0.75, color=color, edgecolors='black')\n",
    "    ax.annotate(row['Method'], (row['Median Latency (ms)'], row['nDCG@10']), xytext=(8, 8), textcoords='offset points', fontsize=9, fontweight='bold')\n",
    "ax.set_xlabel('Median Latency (ms)')\n",
    "ax.set_ylabel('nDCG@10')\n",
    "ax.set_title(f'Speed vs Quality â€” {dataset_name}')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'speed_vs_quality_{dataset_name}.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"âœ… Plot saved as speed_vs_quality_{dataset_name}.png\")\n",
    "\n",
    "# Bar chart quality\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.bar(comparison_df['Method'], comparison_df['nDCG@10'], color='skyblue', edgecolor='black')\n",
    "ax.set_ylabel('nDCG@10')\n",
    "ax.set_title(f'Quality Comparison â€” {dataset_name}')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.xticks(rotation=30, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'quality_comparison_{dataset_name}.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"âœ… Plot saved as quality_comparison_{dataset_name}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc41bc0",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111f9ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tables with dataset suffix\n",
    "comparison_df.to_csv(f'experiment_results_{dataset_name}.csv', index=False)\n",
    "latency_df = pd.DataFrame({\n",
    "    'Flat': results_flat['latencies'],\n",
    "    'HNSW': results_hnsw['latencies'],\n",
    "    'HNSW-INT8': results_hnsw_int8['latencies'],\n",
    "    'BM25': results_bm25['latencies'],\n",
    "})\n",
    "latency_df.to_csv(f'latency_data_{dataset_name}.csv', index=False)\n",
    "\n",
    "# Hybrid results table\n",
    "hybrid_comparison_df = pd.DataFrame([{\n",
    "    'Method': res['name'],\n",
    "    'Recall@10': res['recall@10'],\n",
    "    'nDCG@10': res['ndcg@10'],\n",
    "    'Median Latency (ms)': res['median_latency'],\n",
    "    'P95 Latency (ms)': res['p95_latency'],\n",
    "} for res in hybrid_results])\n",
    "hybrid_comparison_df.to_csv(f'hybrid_results_{dataset_name}.csv', index=False)\n",
    "\n",
    "print(\"âœ… Saved: experiment_results, latency_data, hybrid_results (dataset-specific)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

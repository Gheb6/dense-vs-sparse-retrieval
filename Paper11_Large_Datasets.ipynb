{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3119d4f",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è IMPORTANT: Runtime Restart Required\n",
    "\n",
    "**After running Cell 1 (install dependencies), you MUST restart the runtime/kernel before running Cell 2.**\n",
    "\n",
    "Why? PyJnius starts a JVM when pyserini is first imported, and it cannot be changed once started. The restart ensures Java 21 is used from the beginning.\n",
    "\n",
    "**Steps:**\n",
    "1. Run Cell 1 (Install dependencies) \n",
    "2. **Runtime ‚Üí Restart runtime** (Colab) or **Kernel ‚Üí Restart** (Jupyter)\n",
    "3. Run Cell 2 and continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400e7cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q sentence-transformers pyserini pandas matplotlib seaborn beir\n",
    "# Install newer Java (class file version 65 requires Java 21 JDK)\n",
    "!apt-get -y install -qq openjdk-21-jdk-headless || true\n",
    "print(\"‚úÖ Dependencies installed (Pyserini + Java 21 JDK for Lucene + BEIR)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df981ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import re\n",
    "import subprocess\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Ensure Java 21 is used (needed for Pyserini/Lucene class version 65)\n",
    "java_home = \"/usr/lib/jvm/java-21-openjdk-amd64\"\n",
    "os.environ[\"JAVA_HOME\"] = os.environ.get(\"JAVA_HOME\", java_home)\n",
    "os.environ[\"JAVAHOME\"] = os.environ.get(\"JAVAHOME\", java_home)\n",
    "os.environ[\"JDK_HOME\"] = os.environ.get(\"JDK_HOME\", java_home)\n",
    "os.environ[\"PATH\"] = f\"{os.environ['JAVA_HOME']}/bin:\" + os.environ.get(\"PATH\", \"\")\n",
    "# Force alternatives to Java 21\n",
    "try:\n",
    "    subprocess.run([\"update-alternatives\", \"--set\", \"java\", f\"{java_home}/bin/java\"], check=True)\n",
    "    subprocess.run([\"update-alternatives\", \"--set\", \"javac\", f\"{java_home}/bin/javac\"], check=True)\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è update-alternatives failed: {e}\")\n",
    "# Verify Java version\n",
    "try:\n",
    "    subprocess.run([\"java\", \"-version\"], check=True)\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Java version check failed: {e}\")\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from beir import util\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"‚úÖ Libraries imported (using Lucene/Pyserini; Java forced to 21)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a541492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic: Check actual Java version being used\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"üîç Java Diagnostic:\")\n",
    "try:\n",
    "    result = subprocess.run([\"java\", \"-version\"], capture_output=True, text=True)\n",
    "    print(\"Java version:\", result.stderr.split('\\n')[0])\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Java not found: {e}\")\n",
    "\n",
    "print(\"\\nüîç Python JVM Status:\")\n",
    "try:\n",
    "    import jnius_config\n",
    "    if jnius_config.vm_running:\n",
    "        print(\"‚ö†Ô∏è JVM already running - restart required to change Java version\")\n",
    "    else:\n",
    "        print(\"‚úÖ JVM not started yet\")\n",
    "except:\n",
    "    print(\"‚úÖ jnius not loaded yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bf6fa7",
   "metadata": {},
   "source": [
    "## Dataset Selection\n",
    "Choose a dataset. Defaults to FiQA for medium scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6c81ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select dataset: 'scifact', 'fiqa', 'trec-covid', 'webis-touche2020', 'quora', 'robust04', 'trec-news', or 'nq'\n",
    "dataset_name = 'fiqa'  # pick from the list above; 'nq' is very large\n",
    "\n",
    "dataset_urls = {\n",
    "    'scifact': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/scifact.zip',          # ~5k docs\n",
    "    'fiqa': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/fiqa.zip',            # ~57k docs\n",
    "    'trec-covid': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/trec-covid.zip',  # ~171k docs\n",
    "    'webis-touche2020': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/webis-touche2020.zip', # ~382k docs\n",
    "    'quora': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/quora.zip',          # ~523k docs\n",
    "    'robust04': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/robust04.zip',    # ~528k docs\n",
    "    'trec-news': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/trec-news.zip',  # ~595k docs\n",
    "    # Note: NQ is very large; ensure sufficient resources\n",
    "    'nq': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/nq.zip',                # ~2.6M docs\n",
    "}\n",
    "\n",
    "url = dataset_urls[dataset_name]\n",
    "print(f\"Downloading {dataset_name} dataset...\")\n",
    "data_path = util.download_and_unzip(url, \"datasets\")\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "corpus, queries, qrels = GenericDataLoader(data_folder=data_path).load(split=\"test\")\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset loaded!\\n   Documents: {len(corpus):,}\\n   Queries: {len(queries):,}\\n   Relevance judgments: {len(qrels):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13a5730",
   "metadata": {},
   "source": [
    "## Prepare Model and Texts\n",
    "Memory-safe batching for larger corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108593e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding model\n",
    "model_name = 'BAAI/bge-base-en-v1.5'\n",
    "print(f\"Loading model: {model_name}\")\n",
    "model = SentenceTransformer(model_name)\n",
    "dimension = model.get_sentence_embedding_dimension()\n",
    "\n",
    "# Prepare texts\n",
    "doc_ids = list(corpus.keys())\n",
    "doc_texts = [corpus[did]['title'] + ' ' + corpus[did]['text'] for did in doc_ids]\n",
    "query_ids = list(queries.keys())\n",
    "query_texts = [queries[qid] for qid in query_ids]\n",
    "\n",
    "print(f\"‚úÖ Model loaded (dim={dimension})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d7ac86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode documents with memory-safe batching\n",
    "batch_size_docs = 32 if len(doc_texts) <= 100_000 else 16\n",
    "print(f\"Encoding {len(doc_texts):,} documents (batch_size={batch_size_docs})...\")\n",
    "\n",
    "doc_embeddings = model.encode(\n",
    "    doc_texts,\n",
    "    batch_size=batch_size_docs,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Documents encoded! Shape: {doc_embeddings.shape}, Memory: {doc_embeddings.nbytes / (1024**2):.2f} MB\")\n",
    "\n",
    "# Encode queries\n",
    "batch_size_queries = 32\n",
    "print(f\"Encoding {len(query_texts):,} queries (batch_size={batch_size_queries})...\")\n",
    "query_embeddings = model.encode(\n",
    "    query_texts,\n",
    "    batch_size=batch_size_queries,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True\n",
    ")\n",
    "print(f\"‚úÖ Queries encoded! Shape: {query_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f488e6",
   "metadata": {},
   "source": [
    "## Build Indexes\n",
    "HNSW for medium/large datasets, INT8 quantization for memory reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c281a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Lucene indexes (BM25 + SPLADE ED + HNSW vectors + flat vectors)\n",
    "print(\"Preparing Lucene inputs...\")\n",
    "\n",
    "bm25_root = f'lucene_bm25_{dataset_name}'\n",
    "bm25_docs_dir = os.path.join(bm25_root, 'docs')\n",
    "bm25_index_dir = os.path.join(bm25_root, 'index')\n",
    "os.makedirs(bm25_docs_dir, exist_ok=True)\n",
    "\n",
    "bm25_jsonl = os.path.join(bm25_docs_dir, 'docs.jsonl')\n",
    "with open(bm25_jsonl, 'w', encoding='utf-8') as f:\n",
    "    for did, text in zip(doc_ids, doc_texts):\n",
    "        f.write(json.dumps({'id': did, 'contents': text}) + \"\\n\")\n",
    "\n",
    "# SPLADE docs (same jsonl as BM25)\n",
    "splade_root = f'lucene_splade_{dataset_name}'\n",
    "splade_docs_dir = os.path.join(splade_root, 'docs')\n",
    "splade_index_dir = os.path.join(splade_root, 'index')\n",
    "splade_encoded_dir = os.path.join(splade_root, 'encoded')\n",
    "os.makedirs(splade_docs_dir, exist_ok=True)\n",
    "os.makedirs(splade_encoded_dir, exist_ok=True)\n",
    "\n",
    "splade_jsonl = os.path.join(splade_docs_dir, 'docs.jsonl')\n",
    "if not os.path.exists(splade_jsonl):\n",
    "    with open(splade_jsonl, 'w', encoding='utf-8') as f:\n",
    "        for did, text in zip(doc_ids, doc_texts):\n",
    "            f.write(json.dumps({'id': did, 'contents': text}) + \"\\n\")\n",
    "\n",
    "dense_root = f'lucene_dense_{dataset_name}'\n",
    "dense_vec_dir = os.path.join(dense_root, 'vectors')\n",
    "dense_index_dir = os.path.join(dense_root, 'index_hnsw')\n",
    "dense_flat_index_dir = os.path.join(dense_root, 'index_flat')\n",
    "os.makedirs(dense_vec_dir, exist_ok=True)\n",
    "\n",
    "dense_jsonl = os.path.join(dense_vec_dir, 'vectors.jsonl')\n",
    "with open(dense_jsonl, 'w', encoding='utf-8') as f:\n",
    "    for did, text, vec in zip(doc_ids, doc_texts, doc_embeddings):\n",
    "        f.write(json.dumps({'id': did, 'contents': text, 'vector': vec.tolist()}) + \"\\n\")\n",
    "\n",
    "# Match paper defaults\n",
    "M = 16\n",
    "ef_construction = 100\n",
    "ef_search = 1000\n",
    "\n",
    "threads = '16'\n",
    "\n",
    "print(\"Indexing BM25 (Lucene)...\")\n",
    "subprocess.run([\n",
    "    'python', '-m', 'pyserini.index.lucene',\n",
    "    '--collection', 'JsonCollection',\n",
    "    '--input', bm25_root,\n",
    "    '--index', bm25_index_dir,\n",
    "    '--generator', 'DefaultLuceneDocumentGenerator',\n",
    "    '--threads', threads,\n",
    "    '--storePositions',\n",
    "    '--storeDocvectors',\n",
    "    '--storeRaw'\n",
    "], check=True)\n",
    "print(\"‚úÖ BM25 index ready\")\n",
    "\n",
    "print(\"Encoding SPLADE ED (naver/splade_cocondenser_ensembledistil)...\")\n",
    "subprocess.run([\n",
    "    'python', '-m', 'pyserini.encode',\n",
    "    '--encoder', 'naver/splade_cocondenser_ensembledistil',\n",
    "    '--fields', 'contents',\n",
    "    '--input', splade_docs_dir,\n",
    "    '--output', splade_encoded_dir,\n",
    "    '--batch', '32',\n",
    "    '--format', 'jsonl',\n",
    "    '--device', 'cpu'\n",
    "], check=True)\n",
    "print(\"‚úÖ SPLADE encoding ready\")\n",
    "\n",
    "print(\"Indexing SPLADE impact (Lucene)...\")\n",
    "subprocess.run([\n",
    "    'python', '-m', 'pyserini.index.lucene',\n",
    "    '--collection', 'JsonCollection',\n",
    "    '--input', splade_encoded_dir,\n",
    "    '--index', splade_index_dir,\n",
    "    '--generator', 'ImpactLuceneDocumentGenerator',\n",
    "    '--impact',\n",
    "    '--threads', threads,\n",
    "    '--storePositions',\n",
    "    '--storeDocvectors',\n",
    "    '--storeRaw'\n",
    "], check=True)\n",
    "print(\"‚úÖ SPLADE impact index ready\")\n",
    "\n",
    "print(\"Indexing Dense HNSW (Lucene vectors)...\")\n",
    "subprocess.run([\n",
    "    'python', '-m', 'pyserini.index.lucene',\n",
    "    '--collection', 'JsonVectorCollection',\n",
    "    '--input', dense_root,\n",
    "    '--index', dense_index_dir,\n",
    "    '--generator', 'DefaultLuceneDocumentGenerator',\n",
    "    '--threads', threads,\n",
    "    '--dim', str(dimension),\n",
    "    '--hnswM', str(M),\n",
    "    '--hnswefC', str(ef_construction),\n",
    "    '--hnswefS', str(ef_search),\n",
    "    '--storeRaw'\n",
    "], check=True)\n",
    "print(\"‚úÖ Dense HNSW index ready (Lucene)\")\n",
    "\n",
    "print(\"Indexing Dense FLAT (Lucene vectors)...\")\n",
    "subprocess.run([\n",
    "    'python', '-m', 'pyserini.index.lucene',\n",
    "    '--collection', 'JsonVectorCollection',\n",
    "    '--input', dense_root,\n",
    "    '--index', dense_flat_index_dir,\n",
    "    '--generator', 'DefaultLuceneDocumentGenerator',\n",
    "    '--threads', threads,\n",
    "    '--dim', str(dimension),\n",
    "    '--vector-indexing-approach', 'flat',\n",
    "    '--storeRaw'\n",
    "], check=True)\n",
    "print(\"‚úÖ Dense FLAT index ready (Lucene)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c953796",
   "metadata": {},
   "source": [
    "## BM25 Baseline\n",
    "Tokenize and build BM25 index over documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e11bad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Lucene searchers\n",
    "bm25_searcher = LuceneSearcher(bm25_index_dir)\n",
    "bm25_searcher.set_bm25(k1=0.9, b=0.4)\n",
    "\n",
    "dense_hnsw_searcher = LuceneSearcher(dense_index_dir)\n",
    "dense_flat_searcher = LuceneSearcher(dense_flat_index_dir)\n",
    "\n",
    "from pyserini.search.lucene import LuceneImpactSearcher, SpladeQueryEncoder\n",
    "splade_query_encoder = SpladeQueryEncoder('naver/splade_cocondenser_ensembledistil')\n",
    "splade_searcher = LuceneImpactSearcher(splade_index_dir, splade_query_encoder)\n",
    "\n",
    "print(\"‚úÖ Lucene searchers ready (BM25 + SPLADE ED + HNSW + FLAT)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8919ed98",
   "metadata": {},
   "source": [
    "## Search Functions\n",
    "Shared utilities to run and measure searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee2b676",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id_to_idx = {did: i for i, did in enumerate(doc_ids)}\n",
    "\n",
    "def search_dense_lucene(searcher, query_embeddings, k=1000, name=\"Lucene-HNSW\"):\n",
    "    latencies = []; all_indices = []; all_scores = []\n",
    "    for emb in tqdm(query_embeddings, desc=f\"{name} search\"):\n",
    "        start = time.time()\n",
    "        hits = searcher.search(query_vector=emb.tolist(), k=k)\n",
    "        latencies.append((time.time() - start) * 1000)\n",
    "        docids = [h.docid for h in hits]\n",
    "        scores = [h.score for h in hits]\n",
    "        all_indices.append([doc_id_to_idx[d] for d in docids])\n",
    "        all_scores.append(scores)\n",
    "    latencies = np.array(latencies)\n",
    "    return {\n",
    "        'name': name,\n",
    "        'indices': np.array(all_indices),\n",
    "        'scores': np.array(all_scores),\n",
    "        'latencies': latencies,\n",
    "        'median_latency': np.median(latencies),\n",
    "        'p95_latency': np.percentile(latencies, 95),\n",
    "        'p99_latency': np.percentile(latencies, 99),\n",
    "    }\n",
    "\n",
    "def search_sparse_impact(searcher, query_texts, k=1000, name=\"SPLADE-ED\"):\n",
    "    latencies = []; all_indices = []; all_scores = []\n",
    "    for q in tqdm(query_texts, desc=f\"{name} search\"):\n",
    "        start = time.time()\n",
    "        hits = searcher.search(q, k)\n",
    "        latencies.append((time.time() - start) * 1000)\n",
    "        docids = [h.docid for h in hits]\n",
    "        scores = [h.score for h in hits]\n",
    "        all_indices.append([doc_id_to_idx[d] for d in docids])\n",
    "        all_scores.append(scores)\n",
    "    latencies = np.array(latencies)\n",
    "    return {\n",
    "        'name': name,\n",
    "        'indices': np.array(all_indices),\n",
    "        'scores': np.array(all_scores),\n",
    "        'latencies': latencies,\n",
    "        'median_latency': np.median(latencies),\n",
    "        'p95_latency': np.percentile(latencies, 95),\n",
    "        'p99_latency': np.percentile(latencies, 99),\n",
    "    }\n",
    "\n",
    "def search_bm25_lucene(searcher, query_texts, k=1000):\n",
    "    latencies = []; all_indices = []; all_scores = []\n",
    "    for q in tqdm(query_texts, desc=\"BM25 search\"):\n",
    "        start = time.time()\n",
    "        hits = searcher.search(q, k)\n",
    "        latencies.append((time.time() - start) * 1000)\n",
    "        docids = [h.docid for h in hits]\n",
    "        scores = [h.score for h in hits]\n",
    "        all_indices.append([doc_id_to_idx[d] for d in docids])\n",
    "        all_scores.append(scores)\n",
    "    latencies = np.array(latencies)\n",
    "    return {\n",
    "        'name': 'BM25',\n",
    "        'indices': np.array(all_indices),\n",
    "        'scores': np.array(all_scores),\n",
    "        'latencies': latencies,\n",
    "        'median_latency': np.median(latencies),\n",
    "        'p95_latency': np.percentile(latencies, 95),\n",
    "        'p99_latency': np.percentile(latencies, 99),\n",
    "    }\n",
    "\n",
    "def merge_rankings(dense_indices, dense_scores, sparse_indices, sparse_scores, k=10, alpha=0.5):\n",
    "    merged = {}\n",
    "    for rank, (idx, s) in enumerate(zip(dense_indices, dense_scores), 1):\n",
    "        doc_id = doc_ids[idx]\n",
    "        merged[doc_id] = merged.get(doc_id, 0) + alpha / (60 + rank)\n",
    "    for rank, (idx, s) in enumerate(zip(sparse_indices, sparse_scores), 1):\n",
    "        doc_id = doc_ids[idx]\n",
    "        merged[doc_id] = merged.get(doc_id, 0) + (1 - alpha) / (60 + rank)\n",
    "    ranked = sorted(merged.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "    return np.array([doc_id_to_idx[doc_id] for doc_id, _ in ranked])\n",
    "\n",
    "def hybrid_search(dense_res, sparse_res, alpha=0.5, k=10):\n",
    "    latencies = []; all_indices = []\n",
    "    for d_indices, d_scores, s_indices, s_scores in tqdm(zip(dense_res['indices'], dense_res['scores'], sparse_res['indices'], sparse_res['scores']), total=len(dense_res['indices']), desc=\"Hybrid search\"):\n",
    "        start = time.time()\n",
    "        merged = merge_rankings(d_indices, d_scores, s_indices, s_scores, k=k, alpha=alpha)\n",
    "        latencies.append((time.time() - start) * 1000)\n",
    "        all_indices.append(merged)\n",
    "    latencies = np.array(latencies)\n",
    "    return {\n",
    "        'name': f\"Hybrid (Œ±={alpha})\",\n",
    "        'indices': np.array(all_indices),\n",
    "        'latencies': latencies,\n",
    "        'median_latency': np.median(latencies),\n",
    "        'p95_latency': np.percentile(latencies, 95),\n",
    "        'p99_latency': np.percentile(latencies, 99),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9b77d3",
   "metadata": {},
   "source": [
    "## Run Searches\n",
    "Collect top-10 results and latency stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8bb9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_eval = 10\n",
    "k_retrieve = 1000\n",
    "\n",
    "results_dense_flat = search_dense_lucene(dense_flat_searcher, query_embeddings, k=k_retrieve, name=\"Lucene-FLAT\")\n",
    "results_dense_hnsw = search_dense_lucene(dense_hnsw_searcher, query_embeddings, k=k_retrieve, name=\"Lucene-HNSW\")\n",
    "results_bm25 = search_bm25_lucene(bm25_searcher, query_texts, k=k_retrieve)\n",
    "results_splade = search_sparse_impact(splade_searcher, query_texts, k=k_retrieve, name=\"SPLADE-ED\")\n",
    "\n",
    "# Hybrid runs with different Œ± using HNSW dense + SPLADE\n",
    "alpha_values = [0.3, 0.5, 0.7]\n",
    "hybrid_results = []\n",
    "for alpha in alpha_values:\n",
    "    res = hybrid_search(results_dense_hnsw, results_splade, alpha=alpha, k=k_eval)\n",
    "    hybrid_results.append(res)\n",
    "print(\"‚úÖ Searches complete (Flat + HNSW + BM25 + SPLADE)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db203735",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Compute Recall@10 and nDCG@10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899dbb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall(retrieved_indices, qrels, query_ids, doc_ids, k=10):\n",
    "    recalls = []\n",
    "    for i, qid in enumerate(query_ids):\n",
    "        if qid not in qrels:\n",
    "            continue\n",
    "        relevant_docs = set(qrels[qid].keys())\n",
    "        retrieved_docs = set([doc_ids[idx] for idx in retrieved_indices[i][:k] if idx >= 0])\n",
    "        if len(relevant_docs) > 0:\n",
    "            recalls.append(len(relevant_docs & retrieved_docs) / len(relevant_docs))\n",
    "    return np.mean(recalls) if recalls else 0.0\n",
    "\n",
    "def calculate_ndcg(retrieved_indices, qrels, query_ids, doc_ids, k=10):\n",
    "    ndcgs = []\n",
    "    for i, qid in enumerate(query_ids):\n",
    "        if qid not in qrels:\n",
    "            continue\n",
    "        relevant_docs = qrels[qid]\n",
    "        retrieved_docs = [doc_ids[idx] for idx in retrieved_indices[i][:k] if idx >= 0]\n",
    "        dcg = 0\n",
    "        for rank, doc_id in enumerate(retrieved_docs, 1):\n",
    "            rel = relevant_docs.get(doc_id, 0)\n",
    "            dcg += (2 ** rel - 1) / np.log2(rank + 1)\n",
    "        ideal = sorted(relevant_docs.values(), reverse=True)[:k]\n",
    "        idcg = sum((2 ** r - 1) / np.log2(rank + 2) for rank, r in enumerate(ideal))\n",
    "        ndcgs.append(dcg / idcg if idcg > 0 else 0)\n",
    "    return np.mean(ndcgs) if ndcgs else 0.0\n",
    "\n",
    "# Evaluate all\n",
    "for results in [results_dense_flat, results_dense_hnsw, results_bm25, results_splade] + hybrid_results:\n",
    "    results['recall@10'] = calculate_recall(results['indices'], qrels, query_ids, doc_ids, k=k_eval)\n",
    "    results['ndcg@10'] = calculate_ndcg(results['indices'], qrels, query_ids, doc_ids, k=k_eval)\n",
    "\n",
    "print(\"‚úÖ Evaluation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3509ddc",
   "metadata": {},
   "source": [
    "## Comparison Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4440b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = pd.DataFrame([\n",
    "    {\n",
    "        'Method': 'Lucene-FLAT',\n",
    "        'Type': 'Dense',\n",
    "        'Recall@10': results_dense_flat['recall@10'],\n",
    "        'nDCG@10': results_dense_flat['ndcg@10'],\n",
    "        'Median Latency (ms)': results_dense_flat['median_latency'],\n",
    "        'P95 Latency (ms)': results_dense_flat['p95_latency'],\n",
    "    },\n",
    "    {\n",
    "        'Method': 'Lucene-HNSW',\n",
    "        'Type': 'Dense',\n",
    "        'Recall@10': results_dense_hnsw['recall@10'],\n",
    "        'nDCG@10': results_dense_hnsw['ndcg@10'],\n",
    "        'Median Latency (ms)': results_dense_hnsw['median_latency'],\n",
    "        'P95 Latency (ms)': results_dense_hnsw['p95_latency'],\n",
    "    },\n",
    "    {\n",
    "        'Method': 'BM25',\n",
    "        'Type': 'Sparse',\n",
    "        'Recall@10': results_bm25['recall@10'],\n",
    "        'nDCG@10': results_bm25['ndcg@10'],\n",
    "        'Median Latency (ms)': results_bm25['median_latency'],\n",
    "        'P95 Latency (ms)': results_bm25['p95_latency'],\n",
    "    },\n",
    "    {\n",
    "        'Method': 'SPLADE-ED',\n",
    "        'Type': 'Sparse',\n",
    "        'Recall@10': results_splade['recall@10'],\n",
    "        'nDCG@10': results_splade['ndcg@10'],\n",
    "        'Median Latency (ms)': results_splade['median_latency'],\n",
    "        'P95 Latency (ms)': results_splade['p95_latency'],\n",
    "    },\n",
    "])\n",
    "\n",
    "# Hybrid rows\n",
    "hybrid_rows = []\n",
    "for res in hybrid_results:\n",
    "    hybrid_rows.append({\n",
    "        'Method': res['name'],\n",
    "        'Type': 'Hybrid',\n",
    "        'Recall@10': res['recall@10'],\n",
    "        'nDCG@10': res['ndcg@10'],\n",
    "        'Median Latency (ms)': res['median_latency'],\n",
    "        'P95 Latency (ms)': res['p95_latency'],\n",
    "    })\n",
    "comparison_df = pd.concat([comparison_df, pd.DataFrame(hybrid_rows)], ignore_index=True)\n",
    "\n",
    "print(\"\\nüìä COMPARISON TABLE\")\n",
    "print(\"=\"*100)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1591db30",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8205bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temp directory for plots\n",
    "import os\n",
    "temp_plots_dir = 'temp_plots'\n",
    "os.makedirs(temp_plots_dir, exist_ok=True)\n",
    "\n",
    "# Speed vs Quality\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "for _, row in comparison_df.iterrows():\n",
    "    color = 'orange' if row['Type'] == 'Sparse' else ('green' if row['Type'] == 'Hybrid' else 'steelblue')\n",
    "    ax.scatter(row['Median Latency (ms)'], row['nDCG@10'], s=200, alpha=0.75, color=color, edgecolors='black')\n",
    "    ax.annotate(row['Method'], (row['Median Latency (ms)'], row['nDCG@10']), xytext=(8, 8), textcoords='offset points', fontsize=9, fontweight='bold')\n",
    "ax.set_xlabel('Median Latency (ms)')\n",
    "ax.set_ylabel('nDCG@10')\n",
    "ax.set_title(f'Speed vs Quality ‚Äî {dataset_name}')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plot1_path = os.path.join(temp_plots_dir, f'speed_vs_quality_{dataset_name}.pdf')\n",
    "plt.savefig(plot1_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"‚úÖ Plot saved as {plot1_path}\")\n",
    "\n",
    "# Bar chart quality\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.bar(comparison_df['Method'], comparison_df['nDCG@10'], color='skyblue', edgecolor='black')\n",
    "ax.set_ylabel('nDCG@10')\n",
    "ax.set_title(f'Quality Comparison ‚Äî {dataset_name}')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.xticks(rotation=30, ha='right')\n",
    "plt.tight_layout()\n",
    "plot2_path = os.path.join(temp_plots_dir, f'quality_comparison_{dataset_name}.pdf')\n",
    "plt.savefig(plot2_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"‚úÖ Plot saved as {plot2_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc41bc0",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111f9ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect environment and set output directory\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def detect_environment():\n",
    "    \"\"\"Detect if running on Colab, Kaggle, Modal, or local\"\"\"\n",
    "    if 'COLAB_GPU' in os.environ or 'COLAB_TPU_ADDR' in os.environ:\n",
    "        return 'colab'\n",
    "    elif 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
    "        return 'kaggle'\n",
    "    elif 'MODAL_PROJECT_NAME' in os.environ:\n",
    "        return 'modal'\n",
    "    else:\n",
    "        return 'local'\n",
    "\n",
    "environment = detect_environment()\n",
    "print(f\"üîç Detected environment: {environment.upper()}\")\n",
    "\n",
    "# Set output directory based on environment\n",
    "if environment == 'colab':\n",
    "    # For Colab, save to /content/results/\n",
    "    output_dir = '/content/results'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"üíæ Saving to: {output_dir}\")\n",
    "    \n",
    "elif environment == 'kaggle':\n",
    "    # For Kaggle, save to /kaggle/working/\n",
    "    output_dir = '/kaggle/working'\n",
    "    print(f\"üíæ Saving to: {output_dir}\")\n",
    "\n",
    "elif environment == 'modal':\n",
    "    # For Modal, save to a results directory (often mapped to a Volume)\n",
    "    output_dir = f'/root/results/{dataset_name}'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"üíæ Saving to Modal Volume path: {output_dir}\")\n",
    "    \n",
    "else:\n",
    "    # For local, save to current directory or create results folder\n",
    "    output_dir = f'{dataset_name}_results'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"üíæ Saving to: {output_dir}/\")\n",
    "\n",
    "# Save comparison table\n",
    "comparison_path = os.path.join(output_dir, f'experiment_results_{dataset_name}.csv')\n",
    "comparison_df.to_csv(comparison_path, index=False)\n",
    "print(f\"‚úÖ Saved: {comparison_path}\")\n",
    "\n",
    "# Save latency data\n",
    "latency_df = pd.DataFrame({\n",
    "    'Lucene-FLAT': results_dense_flat['latencies'],\n",
    "    'Lucene-HNSW': results_dense_hnsw['latencies'],\n",
    "    'BM25': results_bm25['latencies'],\n",
    "    'SPLADE-ED': results_splade['latencies'],\n",
    "})\n",
    "latency_path = os.path.join(output_dir, f'latency_data_{dataset_name}.csv')\n",
    "latency_df.to_csv(latency_path, index=False)\n",
    "print(f\"‚úÖ Saved: {latency_path}\")\n",
    "\n",
    "# Save hybrid results table\n",
    "hybrid_comparison_df = pd.DataFrame([{\n",
    "    'Method': res['name'],\n",
    "    'Recall@10': res['recall@10'],\n",
    "    'nDCG@10': res['ndcg@10'],\n",
    "    'Median Latency (ms)': res['median_latency'],\n",
    "    'P95 Latency (ms)': res['p95_latency'],\n",
    "} for res in hybrid_results])\n",
    "hybrid_path = os.path.join(output_dir, f'hybrid_results_{dataset_name}.csv')\n",
    "hybrid_comparison_df.to_csv(hybrid_path, index=False)\n",
    "print(f\"‚úÖ Saved: {hybrid_path}\")\n",
    "\n",
    "# Copy plot files to output directory\n",
    "temp_plots_dir = 'temp_plots'\n",
    "if os.path.exists(temp_plots_dir):\n",
    "    plot_files = [\n",
    "        f'speed_vs_quality_{dataset_name}.pdf',\n",
    "        f'quality_comparison_{dataset_name}.pdf'\n",
    "    ]\n",
    "    for plot_file in plot_files:\n",
    "        src = os.path.join(temp_plots_dir, plot_file)\n",
    "        if os.path.exists(src):\n",
    "            dst = os.path.join(output_dir, plot_file)\n",
    "            shutil.copy2(src, dst)\n",
    "            print(f\"‚úÖ Copied plot: {dst}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"üìÅ All results saved to: {output_dir}/\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Environment-specific download instructions and automatic downloads\n",
    "if environment == 'colab':\n",
    "    print(\"\\nüì• AUTO-DOWNLOADING FILES TO YOUR PC...\")\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        # Download all result files\n",
    "        for filename in os.listdir(output_dir):\n",
    "            filepath = os.path.join(output_dir, filename)\n",
    "            if os.path.isfile(filepath):\n",
    "                print(f\"   üì¶ Downloading: {filename}\")\n",
    "                files.download(filepath)\n",
    "        print(\"‚úÖ All files downloaded to your PC!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Auto-download failed: {e}\")\n",
    "        print(\"\\nüì• MANUAL DOWNLOAD INSTRUCTIONS:\")\n",
    "        print(\"   1. Click the folder icon on the left sidebar\")\n",
    "        print(f\"   2. Navigate to {output_dir}/\")\n",
    "        print(\"   3. Right-click each file ‚Üí Download\")\n",
    "    \n",
    "elif environment == 'kaggle':\n",
    "    print(\"\\nüì• FILES READY FOR DOWNLOAD:\")\n",
    "    print(\"   1. Click 'Save Version' ‚Üí 'Save & Run All'\")\n",
    "    print(\"   2. Once complete, go to the 'Output' tab\")\n",
    "    print(\"   3. Download the CSV and PDF files directly\")\n",
    "    \n",
    "elif environment == 'modal':\n",
    "    print(\"\\nüì• TO ACCESS FILES IN MODAL:\")\n",
    "    print(f\"   1. Files are stored in the volume at: {output_dir}\")\n",
    "    print(\"   2. Use 'modal volume get <volume_name> <remote_path> <local_path>' to download\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\nüìÇ Files saved locally in: {os.path.abspath(output_dir)}/\")\n",
    "    print(\"‚úÖ All files (CSVs and plots) are already on your PC!\")\n",
    "\n",
    "# Create summary report\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Dataset: {dataset_name}\")\n",
    "print(f\"Documents: {len(corpus):,}\")\n",
    "print(f\"Queries: {len(queries):,}\")\n",
    "print(f\"\\nBest Quality Method: {comparison_df.loc[comparison_df['Recall@10'].idxmax()]['Method']}\")\n",
    "print(f\"Fastest Method: {comparison_df.loc[comparison_df['Median Latency (ms)'].idxmin()]['Method']}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e653992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add QPS metrics derived from median/p95 latencies (single-thread approximation)\n",
    "import numpy as np\n",
    "\n",
    "def approx_qps_from_ms(ms):\n",
    "    return (1000.0 / ms) if ms and ms > 0 else 0.0\n",
    "\n",
    "# Extend comparison_df with QPS columns\n",
    "comparison_df['QPS (approx, median)'] = comparison_df['Median Latency (ms)'].apply(approx_qps_from_ms)\n",
    "comparison_df['QPS (approx, p95)'] = comparison_df['P95 Latency (ms)'].apply(approx_qps_from_ms)\n",
    "\n",
    "print(\"\\nüìä COMPARISON TABLE WITH QPS (single-thread approx)\")\n",
    "print(\"=\"*100)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Note: Paper's QPS (cached/ONNX) uses specific hardware & inference stacks.\n",
    "# These approximations let you compare trends, not exact values."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

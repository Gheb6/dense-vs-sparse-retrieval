{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400e7cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q sentence-transformers\n",
    "!pip install -q faiss-cpu\n",
    "!pip install -q beir\n",
    "!pip install -q rank-bm25\n",
    "!pip install -q pandas matplotlib seaborn\n",
    "\n",
    "print(\"‚úÖ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df981ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "from beir import util\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "\n",
    "# BM25\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"‚úÖ Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bf6fa7",
   "metadata": {},
   "source": [
    "## Dataset Selection\n",
    "Choose a dataset. Defaults to FiQA for medium scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6c81ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select dataset: 'scifact', 'fiqa', 'trec-covid', 'webis-touche2020', 'quora', 'robust04', 'trec-news', or 'nq'\n",
    "dataset_name = 'fiqa'  # pick from the list above; 'nq' is very large\n",
    "\n",
    "dataset_urls = {\n",
    "    'scifact': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/scifact.zip',          # ~5k docs\n",
    "    'fiqa': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/fiqa.zip',            # ~57k docs\n",
    "    'trec-covid': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/trec-covid.zip',  # ~171k docs\n",
    "    'webis-touche2020': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/webis-touche2020.zip', # ~382k docs\n",
    "    'quora': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/quora.zip',          # ~523k docs\n",
    "    'robust04': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/robust04.zip',    # ~528k docs\n",
    "    'trec-news': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/trec-news.zip',  # ~595k docs\n",
    "    # Note: NQ is very large; ensure sufficient resources\n",
    "    'nq': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/nq.zip',                # ~2.6M docs\n",
    "}\n",
    "\n",
    "url = dataset_urls[dataset_name]\n",
    "print(f\"Downloading {dataset_name} dataset...\")\n",
    "data_path = util.download_and_unzip(url, \"datasets\")\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "corpus, queries, qrels = GenericDataLoader(data_folder=data_path).load(split=\"test\")\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset loaded!\\n   Documents: {len(corpus):,}\\n   Queries: {len(queries):,}\\n   Relevance judgments: {len(qrels):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13a5730",
   "metadata": {},
   "source": [
    "## Prepare Model and Texts\n",
    "Memory-safe batching for larger corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108593e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding model\n",
    "model_name = 'BAAI/bge-base-en-v1.5'\n",
    "print(f\"Loading model: {model_name}\")\n",
    "model = SentenceTransformer(model_name)\n",
    "dimension = model.get_sentence_embedding_dimension()\n",
    "\n",
    "# Prepare texts\n",
    "doc_ids = list(corpus.keys())\n",
    "doc_texts = [corpus[did]['title'] + ' ' + corpus[did]['text'] for did in doc_ids]\n",
    "query_ids = list(queries.keys())\n",
    "query_texts = [queries[qid] for qid in query_ids]\n",
    "\n",
    "print(f\"‚úÖ Model loaded (dim={dimension})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d7ac86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode documents with memory-safe batching\n",
    "batch_size_docs = 32 if len(doc_texts) <= 100_000 else 16\n",
    "print(f\"Encoding {len(doc_texts):,} documents (batch_size={batch_size_docs})...\")\n",
    "\n",
    "doc_embeddings = model.encode(\n",
    "    doc_texts,\n",
    "    batch_size=batch_size_docs,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Documents encoded! Shape: {doc_embeddings.shape}, Memory: {doc_embeddings.nbytes / (1024**2):.2f} MB\")\n",
    "\n",
    "# Encode queries\n",
    "batch_size_queries = 32\n",
    "print(f\"Encoding {len(query_texts):,} queries (batch_size={batch_size_queries})...\")\n",
    "query_embeddings = model.encode(\n",
    "    query_texts,\n",
    "    batch_size=batch_size_queries,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True\n",
    ")\n",
    "print(f\"‚úÖ Queries encoded! Shape: {query_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f488e6",
   "metadata": {},
   "source": [
    "## Build Indexes\n",
    "HNSW for medium/large datasets, INT8 quantization for memory reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c281a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flat Index (baseline)\n",
    "print(\"Building Flat Index (exact search)...\")\n",
    "start_time = time.time()\n",
    "flat_index = faiss.IndexFlatIP(dimension)\n",
    "flat_index.add(doc_embeddings.astype('float32'))\n",
    "build_time_flat = time.time() - start_time\n",
    "print(f\"‚úÖ Flat Index built in {build_time_flat:.3f}s (vectors={flat_index.ntotal:,})\")\n",
    "\n",
    "# HNSW Index\n",
    "M = 16 if len(doc_ids) < 200_000 else 32\n",
    "ef_construction = 100\n",
    "ef_search = 50 if len(doc_ids) < 200_000 else 100\n",
    "\n",
    "print(\"Building HNSW Index...\")\n",
    "start_time = time.time()\n",
    "hnsw_index = faiss.IndexHNSWFlat(dimension, M)\n",
    "hnsw_index.hnsw.efConstruction = ef_construction\n",
    "hnsw_index.add(doc_embeddings.astype('float32'))\n",
    "hnsw_index.hnsw.efSearch = ef_search\n",
    "build_time_hnsw = time.time() - start_time\n",
    "print(f\"‚úÖ HNSW built in {build_time_hnsw:.3f}s (M={M}, efSearch={ef_search})\")\n",
    "\n",
    "# HNSW + INT8 Quantization\n",
    "print(\"Building HNSW INT8 Index...\")\n",
    "start_time = time.time()\n",
    "hnsw_int8_index = faiss.IndexHNSWSQ(dimension, faiss.ScalarQuantizer.QT_8bit, M)\n",
    "hnsw_int8_index.hnsw.efConstruction = ef_construction\n",
    "hnsw_int8_index.train(doc_embeddings.astype('float32'))\n",
    "hnsw_int8_index.add(doc_embeddings.astype('float32'))\n",
    "hnsw_int8_index.hnsw.efSearch = ef_search\n",
    "build_time_hnsw_int8 = time.time() - start_time\n",
    "print(f\"‚úÖ HNSW INT8 built in {build_time_hnsw_int8:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c953796",
   "metadata": {},
   "source": [
    "## BM25 Baseline\n",
    "Tokenize and build BM25 index over documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e11bad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenize(text):\n",
    "    text = text.lower()\n",
    "    return re.findall(r'\\b\\w+\\b', text)\n",
    "\n",
    "print(\"Tokenizing documents for BM25...\")\n",
    "doc_texts_tokenized = [simple_tokenize(corpus[did]['title'] + ' ' + corpus[did]['text']) for did in doc_ids]\n",
    "\n",
    "print(\"Building BM25 index...\")\n",
    "start_time = time.time()\n",
    "bm25_index = BM25Okapi(doc_texts_tokenized)\n",
    "build_time_bm25 = time.time() - start_time\n",
    "print(f\"‚úÖ BM25 built in {build_time_bm25:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8919ed98",
   "metadata": {},
   "source": [
    "## Search Functions\n",
    "Shared utilities to run and measure searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee2b676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_and_measure(index, query_embeddings, k=10, name=\"Index\"):\n",
    "    print(f\"\\nSearching with {name}...\")\n",
    "    latencies = []; all_indices = []; all_scores = []\n",
    "    for query_emb in tqdm(query_embeddings, desc=f\"{name} search\"):\n",
    "        start = time.time()\n",
    "        scores, indices = index.search(query_emb.reshape(1, -1).astype('float32'), k)\n",
    "        latencies.append((time.time() - start) * 1000)\n",
    "        all_indices.append(indices[0])\n",
    "        all_scores.append(scores[0])\n",
    "    latencies = np.array(latencies)\n",
    "    return {\n",
    "        'name': name,\n",
    "        'indices': np.array(all_indices),\n",
    "        'scores': np.array(all_scores),\n",
    "        'latencies': latencies,\n",
    "        'median_latency': np.median(latencies),\n",
    "        'p95_latency': np.percentile(latencies, 95),\n",
    "        'p99_latency': np.percentile(latencies, 99),\n",
    "    }\n",
    "\n",
    "def search_bm25(bm25_index, query_texts, k=10):\n",
    "    print(\"\\nSearching with BM25...\")\n",
    "    latencies = []; all_indices = []; all_scores = []\n",
    "    for q in tqdm(query_texts, desc=\"BM25 search\"):\n",
    "        tokens = simple_tokenize(q)\n",
    "        start = time.time()\n",
    "        scores = bm25_index.get_scores(tokens)\n",
    "        latencies.append((time.time() - start) * 1000)\n",
    "        top_idx = np.argsort(-scores)[:k]\n",
    "        all_indices.append(top_idx)\n",
    "        all_scores.append(scores[top_idx])\n",
    "    latencies = np.array(latencies)\n",
    "    return {\n",
    "        'name': 'BM25',\n",
    "        'indices': np.array(all_indices),\n",
    "        'scores': np.array(all_scores),\n",
    "        'latencies': latencies,\n",
    "        'median_latency': np.median(latencies),\n",
    "        'p95_latency': np.percentile(latencies, 95),\n",
    "        'p99_latency': np.percentile(latencies, 99),\n",
    "    }\n",
    "\n",
    "def merge_rankings(dense_indices, dense_scores, sparse_indices, sparse_scores, k=10, alpha=0.5):\n",
    "    merged = {}\n",
    "    for rank, (idx, s) in enumerate(zip(dense_indices, dense_scores), 1):\n",
    "        doc_id = doc_ids[idx]\n",
    "        merged[doc_id] = merged.get(doc_id, 0) + alpha / (60 + rank)\n",
    "    for rank, (idx, s) in enumerate(zip(sparse_indices, sparse_scores), 1):\n",
    "        doc_id = doc_ids[idx]\n",
    "        merged[doc_id] = merged.get(doc_id, 0) + (1 - alpha) / (60 + rank)\n",
    "    ranked = sorted(merged.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "    return np.array([doc_ids.index(doc_id) for doc_id, _ in ranked])\n",
    "\n",
    "def hybrid_search(dense_index, query_embeddings, bm25_index, query_texts, alpha=0.5, k=10):\n",
    "    print(f\"\\nSearching with Hybrid (Œ±={alpha})...\")\n",
    "    latencies = []; all_indices = []\n",
    "    for emb, q in tqdm(zip(query_embeddings, query_texts), total=len(query_embeddings), desc=\"Hybrid search\"):\n",
    "        start = time.time()\n",
    "        dscores, dindices = dense_index.search(emb.reshape(1, -1).astype('float32'), k)\n",
    "        dindices = dindices[0]; dscores = dscores[0]\n",
    "        tokens = simple_tokenize(q)\n",
    "        sscores = bm25_index.get_scores(tokens)\n",
    "        sindices = np.argsort(-sscores)[:k]\n",
    "        sscores = sscores[sindices]\n",
    "        merged = merge_rankings(dindices, dscores, sindices, sscores, k=k, alpha=alpha)\n",
    "        latencies.append((time.time() - start) * 1000)\n",
    "        all_indices.append(merged)\n",
    "    latencies = np.array(latencies)\n",
    "    return {\n",
    "        'name': f\"Hybrid (Œ±={alpha})\",\n",
    "        'indices': np.array(all_indices),\n",
    "        'latencies': latencies,\n",
    "        'median_latency': np.median(latencies),\n",
    "        'p95_latency': np.percentile(latencies, 95),\n",
    "        'p99_latency': np.percentile(latencies, 99),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9b77d3",
   "metadata": {},
   "source": [
    "## Run Searches\n",
    "Collect top-10 results and latency stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8bb9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "results_flat = search_and_measure(flat_index, query_embeddings, k=k, name=\"Flat\")\n",
    "results_hnsw = search_and_measure(hnsw_index, query_embeddings, k=k, name=\"HNSW\")\n",
    "results_hnsw_int8 = search_and_measure(hnsw_int8_index, query_embeddings, k=k, name=\"HNSW-INT8\")\n",
    "results_bm25 = search_bm25(bm25_index, query_texts, k=k)\n",
    "\n",
    "# Hybrid runs with different Œ±\n",
    "alpha_values = [0.3, 0.5, 0.7]\n",
    "hybrid_results = []\n",
    "for alpha in alpha_values:\n",
    "    res = hybrid_search(hnsw_index, query_embeddings, bm25_index, query_texts, alpha=alpha, k=k)\n",
    "    hybrid_results.append(res)\n",
    "print(\"‚úÖ Searches complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db203735",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Compute Recall@10 and nDCG@10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899dbb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall(retrieved_indices, qrels, query_ids, doc_ids, k=10):\n",
    "    recalls = []\n",
    "    for i, qid in enumerate(query_ids):\n",
    "        if qid not in qrels:\n",
    "            continue\n",
    "        relevant_docs = set(qrels[qid].keys())\n",
    "        retrieved_docs = set([doc_ids[idx] for idx in retrieved_indices[i][:k] if idx >= 0])\n",
    "        if len(relevant_docs) > 0:\n",
    "            recalls.append(len(relevant_docs & retrieved_docs) / len(relevant_docs))\n",
    "    return np.mean(recalls) if recalls else 0.0\n",
    "\n",
    "def calculate_ndcg(retrieved_indices, qrels, query_ids, doc_ids, k=10):\n",
    "    ndcgs = []\n",
    "    for i, qid in enumerate(query_ids):\n",
    "        if qid not in qrels:\n",
    "            continue\n",
    "        relevant_docs = qrels[qid]\n",
    "        retrieved_docs = [doc_ids[idx] for idx in retrieved_indices[i][:k] if idx >= 0]\n",
    "        dcg = 0\n",
    "        for rank, doc_id in enumerate(retrieved_docs, 1):\n",
    "            rel = relevant_docs.get(doc_id, 0)\n",
    "            dcg += (2 ** rel - 1) / np.log2(rank + 1)\n",
    "        ideal = sorted(relevant_docs.values(), reverse=True)[:k]\n",
    "        idcg = sum((2 ** r - 1) / np.log2(rank + 2) for rank, r in enumerate(ideal))\n",
    "        ndcgs.append(dcg / idcg if idcg > 0 else 0)\n",
    "    return np.mean(ndcgs) if ndcgs else 0.0\n",
    "\n",
    "# Evaluate all\n",
    "for results in [results_flat, results_hnsw, results_hnsw_int8, results_bm25] + hybrid_results:\n",
    "    results['recall@10'] = calculate_recall(results['indices'], qrels, query_ids, doc_ids, k=10)\n",
    "    results['ndcg@10'] = calculate_ndcg(results['indices'], qrels, query_ids, doc_ids, k=10)\n",
    "\n",
    "print(\"‚úÖ Evaluation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3509ddc",
   "metadata": {},
   "source": [
    "## Comparison Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4440b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = pd.DataFrame([\n",
    "    {\n",
    "        'Method': 'Flat',\n",
    "        'Type': 'Dense',\n",
    "        'Recall@10': results_flat['recall@10'],\n",
    "        'nDCG@10': results_flat['ndcg@10'],\n",
    "        'Median Latency (ms)': results_flat['median_latency'],\n",
    "        'P95 Latency (ms)': results_flat['p95_latency'],\n",
    "    },\n",
    "    {\n",
    "        'Method': 'HNSW',\n",
    "        'Type': 'Dense',\n",
    "        'Recall@10': results_hnsw['recall@10'],\n",
    "        'nDCG@10': results_hnsw['ndcg@10'],\n",
    "        'Median Latency (ms)': results_hnsw['median_latency'],\n",
    "        'P95 Latency (ms)': results_hnsw['p95_latency'],\n",
    "    },\n",
    "    {\n",
    "        'Method': 'HNSW-INT8',\n",
    "        'Type': 'Dense',\n",
    "        'Recall@10': results_hnsw_int8['recall@10'],\n",
    "        'nDCG@10': results_hnsw_int8['ndcg@10'],\n",
    "        'Median Latency (ms)': results_hnsw_int8['median_latency'],\n",
    "        'P95 Latency (ms)': results_hnsw_int8['p95_latency'],\n",
    "    },\n",
    "    {\n",
    "        'Method': 'BM25',\n",
    "        'Type': 'Sparse',\n",
    "        'Recall@10': results_bm25['recall@10'],\n",
    "        'nDCG@10': results_bm25['ndcg@10'],\n",
    "        'Median Latency (ms)': results_bm25['median_latency'],\n",
    "        'P95 Latency (ms)': results_bm25['p95_latency'],\n",
    "    },\n",
    "])\n",
    "\n",
    "# Hybrid rows\n",
    "hybrid_rows = []\n",
    "for res in hybrid_results:\n",
    "    hybrid_rows.append({\n",
    "        'Method': res['name'],\n",
    "        'Type': 'Hybrid',\n",
    "        'Recall@10': res['recall@10'],\n",
    "        'nDCG@10': res['ndcg@10'],\n",
    "        'Median Latency (ms)': res['median_latency'],\n",
    "        'P95 Latency (ms)': res['p95_latency'],\n",
    "    })\n",
    "comparison_df = pd.concat([comparison_df, pd.DataFrame(hybrid_rows)], ignore_index=True)\n",
    "\n",
    "print(\"\\nüìä COMPARISON TABLE\")\n",
    "print(\"=\"*100)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1591db30",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8205bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temp directory for plots\n",
    "import os\n",
    "temp_plots_dir = 'temp_plots'\n",
    "os.makedirs(temp_plots_dir, exist_ok=True)\n",
    "\n",
    "# Speed vs Quality\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "for _, row in comparison_df.iterrows():\n",
    "    color = 'orange' if row['Type'] == 'Sparse' else ('green' if row['Type'] == 'Hybrid' else 'steelblue')\n",
    "    ax.scatter(row['Median Latency (ms)'], row['nDCG@10'], s=200, alpha=0.75, color=color, edgecolors='black')\n",
    "    ax.annotate(row['Method'], (row['Median Latency (ms)'], row['nDCG@10']), xytext=(8, 8), textcoords='offset points', fontsize=9, fontweight='bold')\n",
    "ax.set_xlabel('Median Latency (ms)')\n",
    "ax.set_ylabel('nDCG@10')\n",
    "ax.set_title(f'Speed vs Quality ‚Äî {dataset_name}')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plot1_path = os.path.join(temp_plots_dir, f'speed_vs_quality_{dataset_name}.pdf')\n",
    "plt.savefig(plot1_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"‚úÖ Plot saved as {plot1_path}\")\n",
    "\n",
    "# Bar chart quality\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.bar(comparison_df['Method'], comparison_df['nDCG@10'], color='skyblue', edgecolor='black')\n",
    "ax.set_ylabel('nDCG@10')\n",
    "ax.set_title(f'Quality Comparison ‚Äî {dataset_name}')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.xticks(rotation=30, ha='right')\n",
    "plt.tight_layout()\n",
    "plot2_path = os.path.join(temp_plots_dir, f'quality_comparison_{dataset_name}.pdf')\n",
    "plt.savefig(plot2_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"‚úÖ Plot saved as {plot2_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc41bc0",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111f9ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect environment and set output directory\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def detect_environment():\n",
    "    \"\"\"Detect if running on Colab, Kaggle, Modal, or local\"\"\"\n",
    "    if 'COLAB_GPU' in os.environ or 'COLAB_TPU_ADDR' in os.environ:\n",
    "        return 'colab'\n",
    "    elif 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
    "        return 'kaggle'\n",
    "    elif 'MODAL_PROJECT_NAME' in os.environ:\n",
    "        return 'modal'\n",
    "    else:\n",
    "        return 'local'\n",
    "\n",
    "environment = detect_environment()\n",
    "print(f\"üîç Detected environment: {environment.UPPER()}\")\n",
    "\n",
    "# Set output directory based on environment\n",
    "if environment == 'colab':\n",
    "    # For Colab, save to /content/results/\n",
    "    output_dir = '/content/results'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"üíæ Saving to: {output_dir}\")\n",
    "    \n",
    "elif environment == 'kaggle':\n",
    "    # For Kaggle, save to /kaggle/working/\n",
    "    output_dir = '/kaggle/working'\n",
    "    print(f\"üíæ Saving to: {output_dir}\")\n",
    "\n",
    "elif environment == 'modal':\n",
    "    # For Modal, save to a results directory (often mapped to a Volume)\n",
    "    output_dir = f'/root/results/{dataset_name}'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"üíæ Saving to Modal Volume path: {output_dir}\")\n",
    "    \n",
    "else:\n",
    "    # For local, save to current directory or create results folder\n",
    "    output_dir = f'{dataset_name}_results'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"üíæ Saving to: {output_dir}/\")\n",
    "\n",
    "# Save comparison table\n",
    "comparison_path = os.path.join(output_dir, f'experiment_results_{dataset_name}.csv')\n",
    "comparison_df.to_csv(comparison_path, index=False)\n",
    "print(f\"‚úÖ Saved: {comparison_path}\")\n",
    "\n",
    "# Save latency data\n",
    "latency_df = pd.DataFrame({\n",
    "    'Flat': results_flat['latencies'],\n",
    "    'HNSW': results_hnsw['latencies'],\n",
    "    'HNSW-INT8': results_hnsw_int8['latencies'],\n",
    "    'BM25': results_bm25['latencies'],\n",
    "})\n",
    "latency_path = os.path.join(output_dir, f'latency_data_{dataset_name}.csv')\n",
    "latency_df.to_csv(latency_path, index=False)\n",
    "print(f\"‚úÖ Saved: {latency_path}\")\n",
    "\n",
    "# Save hybrid results table\n",
    "hybrid_comparison_df = pd.DataFrame([{\n",
    "    'Method': res['name'],\n",
    "    'Recall@10': res['recall@10'],\n",
    "    'nDCG@10': res['ndcg@10'],\n",
    "    'Median Latency (ms)': res['median_latency'],\n",
    "    'P95 Latency (ms)': res['p95_latency'],\n",
    "} for res in hybrid_results])\n",
    "hybrid_path = os.path.join(output_dir, f'hybrid_results_{dataset_name}.csv')\n",
    "hybrid_comparison_df.to_csv(hybrid_path, index=False)\n",
    "print(f\"‚úÖ Saved: {hybrid_path}\")\n",
    "\n",
    "# Copy plot files to output directory\n",
    "temp_plots_dir = 'temp_plots'\n",
    "if os.path.exists(temp_plots_dir):\n",
    "    plot_files = [\n",
    "        f'speed_vs_quality_{dataset_name}.pdf',\n",
    "        f'quality_comparison_{dataset_name}.pdf'\n",
    "    ]\n",
    "    for plot_file in plot_files:\n",
    "        src = os.path.join(temp_plots_dir, plot_file)\n",
    "        if os.path.exists(src):\n",
    "            dst = os.path.join(output_dir, plot_file)\n",
    "            shutil.copy2(src, dst)\n",
    "            print(f\"‚úÖ Copied plot: {dst}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"üìÅ All results saved to: {output_dir}/\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Environment-specific download instructions and automatic downloads\n",
    "if environment == 'colab':\n",
    "    print(\"\\nüì• AUTO-DOWNLOADING FILES TO YOUR PC...\")\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        # Download all result files\n",
    "        for filename in os.listdir(output_dir):\n",
    "            filepath = os.path.join(output_dir, filename)\n",
    "            if os.path.isfile(filepath):\n",
    "                print(f\"   üì¶ Downloading: {filename}\")\n",
    "                files.download(filepath)\n",
    "        print(\"‚úÖ All files downloaded to your PC!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Auto-download failed: {e}\")\n",
    "        print(\"\\nüì• MANUAL DOWNLOAD INSTRUCTIONS:\")\n",
    "        print(\"   1. Click the folder icon on the left sidebar\")\n",
    "        print(f\"   2. Navigate to {output_dir}/\")\n",
    "        print(\"   3. Right-click each file ‚Üí Download\")\n",
    "    \n",
    "elif environment == 'kaggle':\n",
    "    print(\"\\nüì• FILES READY FOR DOWNLOAD:\")\n",
    "    print(\"   1. Click 'Save Version' ‚Üí 'Save & Run All'\")\n",
    "    print(\"   2. Once complete, go to the 'Output' tab\")\n",
    "    print(\"   3. Download the CSV and PDF files directly\")\n",
    "    \n",
    "elif environment == 'modal':\n",
    "    print(\"\\nüì• TO ACCESS FILES IN MODAL:\")\n",
    "    print(f\"   1. Files are stored in the volume at: {output_dir}\")\n",
    "    print(\"   2. Use 'modal volume get <volume_name> <remote_path> <local_path>' to download\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\nüìÇ Files saved locally in: {os.path.abspath(output_dir)}/\")\n",
    "    print(\"‚úÖ All files (CSVs and plots) are already on your PC!\")\n",
    "\n",
    "# Create summary report\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Dataset: {dataset_name}\")\n",
    "print(f\"Documents: {len(corpus):,}\")\n",
    "print(f\"Queries: {len(queries):,}\")\n",
    "print(f\"\\nBest Quality Method: {comparison_df.loc[comparison_df['Recall@10'].idxmax()]['Method']}\")\n",
    "print(f\"Fastest Method: {comparison_df.loc[comparison_df['Median Latency (ms)'].idxmin()]['Method']}\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 1: Install Dependencies (Stable Versions)\n",
    "Installs specific versions to avoid the dependency conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install specific versions to ensure compatibility between beir, pyserini and transformers\n",
    "!pip uninstall -y faiss-gpu faiss-cpu sentence-transformers transformers huggingface_hub\n",
    "!pip install faiss-cpu\n",
    "!pip install huggingface-hub==0.23.0 transformers==4.36.2 sentence-transformers==2.2.2 pyserini beir pandas matplotlib seaborn scipy\n",
    "\n",
    "# Install Java 21 for Lucene (required by Pyserini)\n",
    "!apt-get -y install -qq openjdk-21-jdk-headless || true\n",
    "print(\"‚úÖ Dependencies installed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 2: Imports & Setup\n",
    "Restart the kernel/runtime before running this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import pathlib\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Configure Java 21 for Lucene\n",
    "java_home = \"/usr/lib/jvm/java-21-openjdk-amd64\"\n",
    "if os.path.exists(java_home):\n",
    "    os.environ[\"JAVA_HOME\"] = java_home\n",
    "    os.environ[\"PATH\"] = f\"{java_home}/bin:\" + os.environ.get(\"PATH\", \"\")\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from beir import util\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "import faiss\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "print(\"‚úÖ Libraries imported and Java configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 3: Dataset Selection\n",
    "\n",
    "Select the dataset you want to analyze by modifying the `dataset_name` variable.\n",
    "The lists below (`public_datasets`, `cqa_sub_datasets`) are provided for reference so you know which dataset names are valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# SELECT DATASET\n",
    "# =================================================================\n",
    "dataset_name = 'scifact'  # Change to: scifact, trec-covid, fiqa, etc.\n",
    "\n",
    "# =================================================================\n",
    "# DATASET CONFIGURATION\n",
    "# =================================================================\n",
    "public_datasets = {\n",
    "    'nfcorpus': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/nfcorpus.zip',\n",
    "    'scifact': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/scifact.zip',\n",
    "    'arguana': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/arguana.zip',\n",
    "    'scidocs': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/scidocs.zip',\n",
    "    'fiqa': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/fiqa.zip',\n",
    "    'trec-covid': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/trec-covid.zip',\n",
    "    'webis-touche2020': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/webis-touche2020.zip',\n",
    "    'quora': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/quora.zip',\n",
    "    'dbpedia-entity': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/dbpedia-entity.zip',\n",
    "    'nq': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/nq.zip',\n",
    "    'cqadupstack': 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/cqadupstack.zip',\n",
    "}\n",
    "\n",
    "cqa_sub_datasets = {\n",
    "    'android': '23K docs', 'english': '41K docs', 'gaming': '46K docs', \n",
    "    'gis': '38K docs', 'mathematica': '17K docs', 'physics': '39K docs', \n",
    "    'programmers': '33K docs', 'stats': '42K docs', 'tex': '71K docs', \n",
    "    'unix': '48K docs', 'webmasters': '17K docs', 'wordpress': '49K docs'\n",
    "}\n",
    "\n",
    "# Download Logic\n",
    "out_dir = os.path.join(pathlib.Path('.').parent.absolute(), \"datasets\")\n",
    "\n",
    "if dataset_name.startswith('cqadupstack/'):\n",
    "    sub_name = dataset_name.split('/')[1]\n",
    "    if sub_name not in cqa_sub_datasets:\n",
    "        raise ValueError(f\"Invalid CQA sub-dataset '{sub_name}'\")\n",
    "    print(f\"--- Processing CQADupStack: {sub_name} ---\")\n",
    "    url = public_datasets['cqadupstack']\n",
    "    base_path = util.download_and_unzip(url, out_dir)\n",
    "    data_path = os.path.join(base_path, sub_name)\n",
    "elif dataset_name in public_datasets:\n",
    "    print(f\"--- Processing {dataset_name} ---\")\n",
    "    url = public_datasets[dataset_name]\n",
    "    data_path = util.download_and_unzip(url, out_dir)\n",
    "else:\n",
    "    raise ValueError(f\"Dataset '{dataset_name}' not found.\")\n",
    "\n",
    "# Load Data\n",
    "print(f\"Loading data from: {data_path}\")\n",
    "try:\n",
    "    corpus, queries, qrels = GenericDataLoader(data_folder=data_path).load(split=\"test\")\n",
    "    \n",
    "    # Prepare lists for encoding (CRITICAL STEP)\n",
    "    print(\"Preparing data lists...\")\n",
    "    doc_ids = list(corpus.keys())\n",
    "    doc_texts = [corpus[did]['title'] + ' ' + corpus[did]['text'] for did in doc_ids]\n",
    "    query_ids = list(queries.keys())\n",
    "    query_texts = [queries[qid] for qid in query_ids]\n",
    "\n",
    "    print(f\"\\n‚úÖ Dataset Loaded: {dataset_name}\")\n",
    "    print(f\"   Documents: {len(corpus):,}\")\n",
    "    print(f\"   Queries: {len(queries):,}\")\n",
    "    print(f\"   Relevance judgments: {len(qrels):,}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error loading dataset: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 3b: Smart Data Loading (Efficient & Robust)\n",
    "\n",
    "This cell uses the **Hugging Face `datasets` library** to download **only** the specific data required for the selected dataset.\n",
    "\n",
    "**Advantages:**\n",
    "* **Efficiency:** Avoids downloading massive ZIP archives (like the full CQADupStack).\n",
    "* **Robustness:** Automatically detects and fixes column naming inconsistencies (e.g., `query-id` vs `query_id`).\n",
    "* **Compatibility:** Formats the data into the exact `corpus`, `queries`, and `qrels` dictionaries required by the rest of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# SMART LOADING (ROBUST SPLITS & COLUMNS)\n",
    "# =================================================================\n",
    "from datasets import load_dataset\n",
    "import sys\n",
    "\n",
    "# Ensure dataset_name is defined\n",
    "if 'dataset_name' not in locals():\n",
    "    print(\"‚ö†Ô∏è 'dataset_name' not found. Defaulting to 'nfcorpus'.\")\n",
    "    dataset_name = 'nfcorpus'\n",
    "\n",
    "print(f\"--- Processing {dataset_name} (via HuggingFace) ---\")\n",
    "\n",
    "try:\n",
    "    # Adapt dataset name for HuggingFace (e.g., cqadupstack/android -> cqadupstack-android)\n",
    "    hf_name = dataset_name.replace(\"/\", \"-\")\n",
    "    \n",
    "    # 1. Download CORPUS (Split: 'train')\n",
    "    print(f\"Downloading corpus ({hf_name}-corpus)...\")\n",
    "    corpus_ds = load_dataset(\"Cohere/beir-embed-english-v3\", f\"{hf_name}-corpus\", split=\"train\")\n",
    "    \n",
    "    # 2. Download QUERIES (Split: 'test')\n",
    "    print(f\"Downloading queries ({hf_name}-queries)...\")\n",
    "    queries_ds = load_dataset(\"Cohere/beir-embed-english-v3\", f\"{hf_name}-queries\", split=\"test\")\n",
    "    \n",
    "    # 3. Download QRELS (Split: 'test')\n",
    "    print(f\"Downloading qrels ({hf_name}-qrels)...\")\n",
    "    qrels_ds = load_dataset(\"Cohere/beir-embed-english-v3\", f\"{hf_name}-qrels\", split=\"test\")\n",
    "\n",
    "    # 4. Data Formatting\n",
    "    print(\"Formatting corpus...\")\n",
    "    corpus = {\n",
    "        str(row['_id']): {'title': row.get('title', ''), 'text': row.get('text', '')} \n",
    "        for row in corpus_ds\n",
    "    }\n",
    "\n",
    "    print(\"Formatting queries...\")\n",
    "    queries = {\n",
    "        str(row['_id']): row['text'] \n",
    "        for row in queries_ds\n",
    "    }\n",
    "\n",
    "    print(\"Formatting qrels...\")\n",
    "    qrels = {}\n",
    "    \n",
    "    # Automatic column name detection (handle 'query-id' vs 'query_id' variations)\n",
    "    first_row = qrels_ds[0]\n",
    "    qid_key = 'query-id' if 'query-id' in first_row else 'query_id'\n",
    "    did_key = 'corpus-id' if 'corpus-id' in first_row else 'corpus_id'\n",
    "    \n",
    "    print(f\"   (Detected keys: {qid_key}, {did_key})\")\n",
    "\n",
    "    for row in qrels_ds:\n",
    "        qid = str(row[qid_key])\n",
    "        did = str(row[did_key])\n",
    "        score = int(row['score'])\n",
    "        \n",
    "        if qid not in qrels:\n",
    "            qrels[qid] = {}\n",
    "        qrels[qid][did] = score\n",
    "\n",
    "    # 5. Prepare lists for encoding\n",
    "    print(\"Preparing lists for encoding...\")\n",
    "    doc_ids = list(corpus.keys())\n",
    "    doc_texts = [corpus[did]['title'] + ' ' + corpus[did]['text'] for did in doc_ids]\n",
    "    query_ids = list(queries.keys())\n",
    "    query_texts = [queries[qid] for qid in query_ids]\n",
    "\n",
    "    print(f\"\\n‚úÖ Dataset Loaded: {dataset_name}\")\n",
    "    print(f\"   Documents: {len(corpus):,}\")\n",
    "    print(f\"   Queries: {len(queries):,}\")\n",
    "    print(f\"   Relevance judgments: {len(qrels):,}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error loading dataset: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 4: Dense Retrieval (BGE Model)\n",
    "Encodes documents and queries using the BGE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load BGE model\n",
    "model_name = 'BAAI/bge-base-en-v1.5'\n",
    "print(f\"Loading BGE model: {model_name}\")\n",
    "model = SentenceTransformer(model_name)\n",
    "dimension = model.get_sentence_embedding_dimension()\n",
    "\n",
    "# Encode Documents\n",
    "# Adjust batch size based on dataset size to avoid OOM\n",
    "batch_size = 32 if len(doc_texts) <= 100_000 else 16 \n",
    "print(f\"Encoding {len(doc_texts):,} documents (batch_size={batch_size})...\")\n",
    "\n",
    "doc_embeddings = model.encode(\n",
    "    doc_texts,\n",
    "    batch_size=batch_size,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True\n",
    ")\n",
    "\n",
    "# Encode Queries\n",
    "print(f\"Encoding {len(query_texts):,} queries...\")\n",
    "query_embeddings = model.encode(\n",
    "    query_texts,\n",
    "    batch_size=32,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Dense encoding complete. Doc shape: {doc_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 5: Build Dense & BM25 Indexes\n",
    "Constructs FAISS indexes for Dense retrieval and Lucene index for BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "import faiss\n",
    "import json\n",
    "\n",
    "# Parameters matching the paper\n",
    "M = 16\n",
    "ef_construction = 100\n",
    "ef_search = 1000\n",
    "base_dir = f'indexes_{dataset_name}'\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. BM25 Index (Lucene)\n",
    "# ---------------------------------------------------------\n",
    "bm25_docs_dir = os.path.join(base_dir, 'bm25_docs')\n",
    "bm25_index_dir = os.path.join(base_dir, 'bm25_index')\n",
    "os.makedirs(bm25_docs_dir, exist_ok=True)\n",
    "\n",
    "print(\"Building BM25 Index...\")\n",
    "# Write JSONL for Pyserini\n",
    "with open(os.path.join(bm25_docs_dir, 'docs.jsonl'), 'w', encoding='utf-8') as f:\n",
    "    for did, text in zip(doc_ids, doc_texts):\n",
    "        f.write(json.dumps({'id': did, 'contents': text}) + \"\\n\")\n",
    "\n",
    "# Run Pyserini Indexer\n",
    "subprocess.run([\n",
    "    'python', '-m', 'pyserini.index.lucene',\n",
    "    '--collection', 'JsonCollection',\n",
    "    '--input', bm25_docs_dir,\n",
    "    '--index', bm25_index_dir, # <--- CORRETTO (rimosso l'apice extra)\n",
    "    '--generator', 'DefaultLuceneDocumentGenerator',\n",
    "    '--threads', '16',\n",
    "    '--storePositions', '--storeDocvectors', '--storeRaw'\n",
    "], check=True)\n",
    "print(\"‚úÖ BM25 Index built.\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. Dense Index (FAISS HNSW & Flat) - WITH TIMING\n",
    "# ---------------------------------------------------------\n",
    "print(\"Building FAISS Indexes...\")\n",
    "\n",
    "# HNSW (Approximate)\n",
    "print(\"  - Building HNSW (FP32)...\")\n",
    "start_t = time.time()\n",
    "hnsw_index = faiss.IndexHNSWFlat(dimension, M, faiss.METRIC_INNER_PRODUCT)\n",
    "hnsw_index.hnsw.efConstruction = ef_construction\n",
    "hnsw_index.hnsw.efSearch = ef_search\n",
    "hnsw_index.add(doc_embeddings)\n",
    "# Salva il tempo FP32 per la Tabella 3\n",
    "time_hnsw_fp32 = time.time() - start_t\n",
    "faiss.write_index(hnsw_index, os.path.join(base_dir, 'hnsw_index.faiss'))\n",
    "\n",
    "# Flat (Exact)\n",
    "print(\"  - Building Flat (FP32)...\")\n",
    "start_t = time.time()\n",
    "flat_index = faiss.IndexFlatIP(dimension)\n",
    "flat_index.add(doc_embeddings)\n",
    "# Salva il tempo FP32 per la Tabella 3\n",
    "time_flat_fp32 = time.time() - start_t\n",
    "faiss.write_index(flat_index, os.path.join(base_dir, 'flat_index.faiss'))\n",
    "\n",
    "print(f\"‚úÖ FAISS Indexes built. HNSW Time: {time_hnsw_fp32:.4f}s, Flat Time: {time_flat_fp32:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 5b: Build INT8 Quantized Indexes (For Tables 3 & 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Building INT8 Quantized Indexes...\")\n",
    "\n",
    "# 1. Quantize Embeddings (Float32 -> Int8)\n",
    "# Simple linear quantization: map [-128, 127] float to int8\n",
    "doc_embeddings_int8 = np.clip(doc_embeddings * 127, -128, 127).astype(np.int8).astype(np.float32) / 127\n",
    "query_embeddings_int8 = np.clip(query_embeddings * 127, -128, 127).astype(np.int8).astype(np.float32) / 127\n",
    "\n",
    "# 2. Build INT8 Indexes\n",
    "# HNSW INT8\n",
    "start_t = time.time()\n",
    "hnsw_int8_index = faiss.IndexHNSWFlat(dimension, M, faiss.METRIC_INNER_PRODUCT)\n",
    "hnsw_int8_index.hnsw.efConstruction = ef_construction\n",
    "hnsw_int8_index.hnsw.efSearch = ef_search\n",
    "hnsw_int8_index.add(doc_embeddings_int8)\n",
    "faiss.write_index(hnsw_int8_index, os.path.join(base_dir, 'hnsw_int8_index.faiss'))\n",
    "time_hnsw_int8 = time.time() - start_t\n",
    "\n",
    "# Flat INT8\n",
    "start_t = time.time()\n",
    "flat_int8_index = faiss.IndexFlatIP(dimension)\n",
    "flat_int8_index.add(doc_embeddings_int8)\n",
    "faiss.write_index(flat_int8_index, os.path.join(base_dir, 'flat_int8_index.faiss'))\n",
    "time_flat_int8 = time.time() - start_t\n",
    "\n",
    "print(f\"‚úÖ INT8 Indexes built. HNSW Time: {time_hnsw_int8:.2f}s, Flat Time: {time_flat_int8:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 6: SPLADE - Manual Encoding & Matrix Construction\n",
    "Replaces the Pyserini SPLADE implementation with the robust Matrix Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "# Configuration\n",
    "SPLADE_MODEL = 'naver/splade-cocondenser-selfdistil' # Standard for BEIR\n",
    "splade_manual_dir = os.path.join(base_dir, 'splade_encoded_manual')\n",
    "os.makedirs(splade_manual_dir, exist_ok=True)\n",
    "splade_jsonl_path = os.path.join(splade_manual_dir, 'docs.jsonl')\n",
    "\n",
    "print(f\"üöÄ SPLADE Matrix Preparation using: {SPLADE_MODEL}\")\n",
    "\n",
    "# Load HF Model\n",
    "tokenizer = AutoTokenizer.from_pretrained(SPLADE_MODEL)\n",
    "splade_model = AutoModelForMaskedLM.from_pretrained(SPLADE_MODEL)\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "splade_model.to(device)\n",
    "splade_model.eval()\n",
    "\n",
    "# Helper: Manual Encoding with explicit quantization (x100)\n",
    "def encode_batch_manual(texts, tokenizer, model, device):\n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        # SPLADE logic: log(1 + ReLU(logits)) * attention_mask\n",
    "        values = torch.log(1 + torch.relu(outputs.logits))\n",
    "        values = values * inputs['attention_mask'].unsqueeze(-1)\n",
    "        values, _ = torch.max(values, dim=1)\n",
    "    \n",
    "    batch_vectors = []\n",
    "    values_np = values.cpu().numpy()\n",
    "    for i in range(len(texts)):\n",
    "        idx = values_np[i].nonzero()[0]\n",
    "        # Quantize float weights to integer (w * 100)\n",
    "        vector = {str(tokenizer.decode([t_id])): int(values_np[i][t_id] * 100) \n",
    "                  for t_id in idx if values_np[i][t_id] > 0}\n",
    "        batch_vectors.append(vector)\n",
    "    return batch_vectors\n",
    "\n",
    "# 1. Encode Documents to JSONL\n",
    "if not os.path.exists(splade_jsonl_path):\n",
    "    print(\"Encoding documents...\")\n",
    "    batch_size = 32\n",
    "    with open(splade_jsonl_path, 'w', encoding='utf-8') as f:\n",
    "        for i in tqdm(range(0, len(doc_texts), batch_size), desc=\"Encoding\"):\n",
    "            batch_t = doc_texts[i:i+batch_size]\n",
    "            batch_i = doc_ids[i:i+batch_size]\n",
    "            vectors = encode_batch_manual(batch_t, tokenizer, splade_model, device)\n",
    "            for did, vec in zip(batch_i, vectors):\n",
    "                f.write(json.dumps({'id': did, 'vector': vec}) + '\\n')\n",
    "else:\n",
    "    print(\"Found existing encoded file, skipping encoding step.\")\n",
    "\n",
    "# 2. Build Sparse Matrix (CSR)\n",
    "print(\"Building Sparse Matrix from JSONL...\")\n",
    "vocab_size = tokenizer.vocab_size\n",
    "data, rows, cols, doc_ids_ordered = [], [], [], []\n",
    "row_idx = 0\n",
    "\n",
    "with open(splade_jsonl_path, 'r', encoding='utf-8') as f:\n",
    "    for line in tqdm(f, desc=\"Matrix Build\"):\n",
    "        entry = json.loads(line)\n",
    "        doc_ids_ordered.append(entry['id'])\n",
    "        for token_str, weight in entry['vector'].items():\n",
    "            # Handle token mapping (string/ID) robustly\n",
    "            try: \n",
    "                col_idx = int(token_str)\n",
    "            except ValueError: \n",
    "                col_idx = tokenizer.convert_tokens_to_ids(token_str)\n",
    "            \n",
    "            if col_idx < vocab_size:\n",
    "                rows.append(row_idx)\n",
    "                cols.append(col_idx)\n",
    "                data.append(weight)\n",
    "        row_idx += 1\n",
    "\n",
    "doc_matrix = csr_matrix((data, (rows, cols)), shape=(row_idx, vocab_size))\n",
    "print(f\"‚úÖ SPLADE Matrix ready: {doc_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 7: Search Functions (BM25, Dense, SPLADE Matrix)\n",
    "Defines the search logic for all three methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# BM25 Searcher\n",
    "bm25_searcher = LuceneSearcher(bm25_index_dir)\n",
    "bm25_searcher.set_bm25(k1=0.9, b=0.4) # Paper parameters\n",
    "\n",
    "# Helper mapping\n",
    "doc_id_to_idx = {did: i for i, did in enumerate(doc_ids)}\n",
    "\n",
    "def run_bm25_search(queries, k=1000):\n",
    "    print(\"Running BM25 search...\")\n",
    "    all_res = []\n",
    "    start = time.time()\n",
    "    for q in tqdm(queries, desc=\"BM25\"):\n",
    "        hits = bm25_searcher.search(q, k)\n",
    "        indices = [doc_id_to_idx[h.docid] for h in hits]\n",
    "        scores = [h.score for h in hits]\n",
    "        all_res.append((indices, scores))\n",
    "    qps = len(queries) / (time.time() - start)\n",
    "    return all_res, qps\n",
    "\n",
    "def run_dense_search(index, query_embs, k=1000):\n",
    "    print(\"Running Dense search...\")\n",
    "    start = time.time()\n",
    "    scores, indices = index.search(query_embs, k)\n",
    "    qps = len(query_embs) / (time.time() - start)\n",
    "    return list(zip(indices, scores)), qps\n",
    "\n",
    "# =================================================================\n",
    "# FIX: SPLADE Search (Auto-detect Dense/Sparse Result)\n",
    "# =================================================================\n",
    "def run_splade_matrix_search(queries, k=1000, batch_size=8):\n",
    "    \"\"\"\n",
    "    Fixed version: handles both sparse and dense matrix multiplication results.\n",
    "    \"\"\"\n",
    "    print(f\"üìâ Running SPLADE Search (Batch Size={batch_size})...\")\n",
    "    \n",
    "    # 1. Encode Queries\n",
    "    query_vectors = []\n",
    "    splade_model.eval()\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(queries), batch_size), desc=\"SPLADE Enc\"):\n",
    "            batch_texts = queries[i:i+batch_size]\n",
    "            inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = splade_model(**inputs)\n",
    "                values = torch.log(1 + torch.relu(outputs.logits))\n",
    "                values = values * inputs['attention_mask'].unsqueeze(-1)\n",
    "                values, _ = torch.max(values, dim=1)\n",
    "            \n",
    "            query_vectors.append(values.float().cpu().numpy())\n",
    "            del inputs, outputs, values\n",
    "            \n",
    "    all_query_vectors = np.vstack(query_vectors)\n",
    "    \n",
    "    # 2. Matrix Multiplication\n",
    "    print(\"‚ö° Running Matrix Multiplication...\")\n",
    "    scores_matrix = doc_matrix.dot(all_query_vectors.T).T\n",
    "    \n",
    "    # --- FIX HERE: Check if it's already dense ---\n",
    "    if hasattr(scores_matrix, \"toarray\"):\n",
    "        scores_dense = scores_matrix.toarray()\n",
    "    else:\n",
    "        scores_dense = scores_matrix # It's already a numpy array\n",
    "    \n",
    "    # 3. Extract Top-K\n",
    "    print(\"üîù Extracting Top-K...\")\n",
    "    all_res = []\n",
    "    \n",
    "    for i, q_scores in enumerate(scores_dense):\n",
    "        if k < len(q_scores):\n",
    "            top_k_idx = np.argpartition(q_scores, -k)[-k:]\n",
    "            top_k_idx = top_k_idx[np.argsort(q_scores[top_k_idx])[::-1]]\n",
    "        else:\n",
    "            top_k_idx = np.argsort(q_scores)[::-1]\n",
    "            \n",
    "        real_doc_indices = []\n",
    "        final_scores = []\n",
    "        for idx in top_k_idx:\n",
    "            matrix_doc_id = doc_ids_ordered[idx]\n",
    "            if matrix_doc_id in doc_id_to_idx:\n",
    "                real_doc_indices.append(doc_id_to_idx[matrix_doc_id])\n",
    "                final_scores.append(q_scores[idx])\n",
    "                \n",
    "        all_res.append((real_doc_indices, final_scores))\n",
    "        \n",
    "    qps = len(queries) / 1.0 \n",
    "    return all_res, qps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 8: Evaluation Logic\n",
    "Standard BEIR evaluation metrics (nDCG@10, Recall@10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_results(results_list, qrels, query_ids, k=10):\n",
    "    recalls, ndcgs = [], []\n",
    "    \n",
    "    for i, (indices, scores) in enumerate(results_list):\n",
    "        qid = query_ids[i]\n",
    "        if qid not in qrels: continue\n",
    "        \n",
    "        relevant_docs = qrels[qid] # dict {docid: score}\n",
    "        retrieved_docs = [doc_ids[idx] for idx in indices]\n",
    "        \n",
    "        # Recall\n",
    "        rel_set = set(relevant_docs.keys())\n",
    "        ret_set = set(retrieved_docs[:k])\n",
    "        if len(rel_set) > 0:\n",
    "            recalls.append(len(rel_set & ret_set) / len(rel_set))\n",
    "        \n",
    "        # nDCG\n",
    "        dcg = 0\n",
    "        for rank, doc_id in enumerate(retrieved_docs[:k], 1):\n",
    "            rel = relevant_docs.get(doc_id, 0)\n",
    "            dcg += (2**rel - 1) / np.log2(rank + 1)\n",
    "        \n",
    "        ideal = sorted(relevant_docs.values(), reverse=True)[:k]\n",
    "        idcg = sum((2**r - 1) / np.log2(j + 2) for j, r in enumerate(ideal))\n",
    "        ndcgs.append(dcg / idcg if idcg > 0 else 0)\n",
    "        \n",
    "    return np.mean(recalls), np.mean(ndcgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 9: Execution & Results Aggregation\n",
    "Runs all searches and compiles the final DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "k_eval = 10\n",
    "k_ret = 1000\n",
    "\n",
    "# 1. BM25\n",
    "res_bm25, qps_bm25 = run_bm25_search(query_texts, k_ret)\n",
    "rec_bm25, ndcg_bm25 = evaluate_results(res_bm25, qrels, query_ids, k_eval)\n",
    "\n",
    "# 2. BGE HNSW\n",
    "res_hnsw, qps_hnsw = run_dense_search(hnsw_index, query_embeddings, k_ret)\n",
    "rec_hnsw, ndcg_hnsw = evaluate_results(res_hnsw, qrels, query_ids, k_eval)\n",
    "\n",
    "# 3. BGE Flat\n",
    "res_flat, qps_flat = run_dense_search(flat_index, query_embeddings, k_ret)\n",
    "rec_flat, ndcg_flat = evaluate_results(res_flat, qrels, query_ids, k_eval)\n",
    "\n",
    "# 4. SPLADE Matrix\n",
    "res_splade, qps_splade = run_splade_matrix_search(query_texts, k_ret, batch_size=8)\n",
    "rec_splade, ndcg_splade = evaluate_results(res_splade, qrels, query_ids, k_eval)\n",
    "\n",
    "# Compile Results\n",
    "results_df = pd.DataFrame([\n",
    "    {'Method': 'BM25', 'Type': 'Sparse (Baseline)', 'Recall@10': rec_bm25, 'nDCG@10': ndcg_bm25, 'QPS': qps_bm25},\n",
    "    {'Method': 'SPLADE++ ED', 'Type': 'Sparse (Learned)', 'Recall@10': rec_splade, 'nDCG@10': ndcg_splade, 'QPS': qps_splade},\n",
    "    {'Method': 'BGE-HNSW', 'Type': 'Dense (HNSW)', 'Recall@10': rec_hnsw, 'nDCG@10': ndcg_hnsw, 'QPS': qps_hnsw},\n",
    "    {'Method': 'BGE-Flat', 'Type': 'Dense (Flat)', 'Recall@10': rec_flat, 'nDCG@10': ndcg_flat, 'QPS': qps_flat},\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"FINAL RESULTS: {dataset_name.upper()}\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 9b: Run INT8 Searches & Generate Tables 3 and 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Running INT8 Searches...\")\n",
    "\n",
    "# Run Searches\n",
    "res_hnsw_int8, qps_hnsw_int8 = run_dense_search(hnsw_int8_index, query_embeddings_int8, k_ret)\n",
    "rec_hnsw_int8, ndcg_hnsw_int8 = evaluate_results(res_hnsw_int8, qrels, query_ids, k_eval)\n",
    "\n",
    "res_flat_int8, qps_flat_int8 = run_dense_search(flat_int8_index, query_embeddings_int8, k_ret)\n",
    "rec_flat_int8, ndcg_flat_int8 = evaluate_results(res_flat_int8, qrels, query_ids, k_eval)\n",
    "\n",
    "# --- TABLE 3: Indexing Time Comparison ---\n",
    "# FIX: Now uses the actual variables 'time_hnsw_fp32' and 'time_flat_fp32' from Cell 5\n",
    "# If you ran Cell 5 before this fix, you might need to re-run Cell 5 or set them manually to approximate values\n",
    "if 'time_hnsw_fp32' not in locals(): time_hnsw_fp32 = 0.0 # Fallback\n",
    "if 'time_flat_fp32' not in locals(): time_flat_fp32 = 0.0 # Fallback\n",
    "\n",
    "table3_df = pd.DataFrame([\n",
    "    {'Method': 'BGE-HNSW', 'Quantization': 'FP32', 'Index Time (s)': time_hnsw_fp32},\n",
    "    {'Method': 'BGE-HNSW', 'Quantization': 'int8', 'Index Time (s)': time_hnsw_int8},\n",
    "    {'Method': 'BGE-Flat', 'Quantization': 'FP32', 'Index Time (s)': time_flat_fp32},\n",
    "    {'Method': 'BGE-Flat', 'Quantization': 'int8', 'Index Time (s)': time_flat_int8},\n",
    "])\n",
    "\n",
    "# --- TABLE 4: INT8 Performance & Quality ---\n",
    "table4_df = pd.DataFrame([\n",
    "    {'Method': 'BGE-HNSW', 'Quantization': 'FP32', 'QPS': qps_hnsw, 'nDCG@10': ndcg_hnsw, 'Recall@10': rec_hnsw},\n",
    "    {'Method': 'BGE-HNSW', 'Quantization': 'int8', 'QPS': qps_hnsw_int8, 'nDCG@10': ndcg_hnsw_int8, 'Recall@10': rec_hnsw_int8},\n",
    "    {'Method': 'BGE-Flat', 'Quantization': 'FP32', 'QPS': qps_flat, 'nDCG@10': ndcg_flat, 'Recall@10': rec_flat},\n",
    "    {'Method': 'BGE-Flat', 'Quantization': 'int8', 'QPS': qps_flat_int8, 'nDCG@10': ndcg_flat_int8, 'Recall@10': rec_flat_int8},\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"TABLE 3: INDEXING TIME (INT8 vs FP32)\")\n",
    "print(\"=\"*80)\n",
    "print(table3_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"TABLE 4: PERFORMANCE & QUALITY (INT8 vs FP32)\")\n",
    "print(\"=\"*80)\n",
    "print(table4_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 10: Save & Visualize\n",
    "Generates the plots and saves CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 10: Save All Results & Generate Plots (Fix for paths with slashes)\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --- FIX: Sanitize dataset name for file paths ---\n",
    "# Transforms \"cqadupstack/android\" into \"cqadupstack_android\" to avoid directory errors\n",
    "safe_dataset_name = dataset_name.replace('/', '_')\n",
    "\n",
    "output_dir = f'results_{safe_dataset_name}'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 1. Save CSVs (Tables 1, 3, 4)\n",
    "# Use safe_dataset_name for file names to prevent OSError\n",
    "results_df.to_csv(os.path.join(output_dir, f'{safe_dataset_name}_results.csv'), index=False)\n",
    "table3_df.to_csv(os.path.join(output_dir, f'{safe_dataset_name}_table3_indexing.csv'), index=False)\n",
    "table4_df.to_csv(os.path.join(output_dir, f'{safe_dataset_name}_table4_int8.csv'), index=False)\n",
    "\n",
    "print(f\"‚úÖ CSV tables saved in: {output_dir}\")\n",
    "\n",
    "# 2. PLOT 1: Speed vs Quality (Scatter Plot)\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = {'Sparse (Baseline)': 'orange', 'Sparse (Learned)': 'red',\n",
    "          'Dense (HNSW)': 'steelblue', 'Dense (Flat)': 'lightblue'}\n",
    "\n",
    "for _, row in results_df.iterrows():\n",
    "    plt.scatter(row['QPS'], row['nDCG@10'], s=200, color=colors[row['Type']], label=row['Method'])\n",
    "    plt.annotate(row['Method'], (row['QPS'], row['nDCG@10']),\n",
    "                 xytext=(0, 10), textcoords='offset points', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.xlabel('QPS (Queries Per Second)', fontsize=12)\n",
    "plt.ylabel('nDCG@10', fontsize=12)\n",
    "# Keep original dataset_name in title for better readability\n",
    "plt.title(f'Speed vs Quality - {dataset_name}', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Save Plot 1 (Use safe name)\n",
    "plot1_path = os.path.join(output_dir, f'{safe_dataset_name}_speed_vs_quality.pdf')\n",
    "plt.savefig(plot1_path, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"‚úÖ Plot 1 saved: {plot1_path}\")\n",
    "\n",
    "# 3. PLOT 2: Metrics Comparison (Bar Chart)\n",
    "plt.figure(figsize=(12, 6))\n",
    "x = np.arange(len(results_df))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, results_df['nDCG@10'], width, label='nDCG@10', alpha=0.8, color='steelblue')\n",
    "plt.bar(x + width/2, results_df['Recall@10'], width, label='Recall@10', alpha=0.8, color='orange')\n",
    "\n",
    "plt.xlabel('Method', fontsize=12)\n",
    "plt.ylabel('Score', fontsize=12)\n",
    "plt.title(f'Metrics Comparison - {dataset_name}', fontsize=14, fontweight='bold')\n",
    "plt.xticks(x, results_df['Method'], rotation=15)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Save Plot 2 (Use safe name)\n",
    "plot2_path = os.path.join(output_dir, f'{safe_dataset_name}_metrics_comparison.pdf')\n",
    "plt.savefig(plot2_path, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"‚úÖ Plot 2 saved: {plot2_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 11. Export & Download Results\n",
    "\n",
    "This final step gathers all generated outputs (CSVs, plots, metadata, and JSON logs) into a single directory and compresses them into a ZIP archive\n",
    "\n",
    "The code automatically detects your running environment to provide the appropriate download method:\n",
    "* **Google Colab**: Triggers a browser download automatically.\n",
    "* **Kaggle**: Provides a clickable download link.\n",
    "* **Local Machine**: Saves the ZIP file in the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "# 1. Configuration & Safe Name\n",
    "if 'dataset_name' not in locals():\n",
    "    dataset_name = 'nfcorpus'\n",
    "\n",
    "# Fix for paths with slashes (e.g., 'cqadupstack/android')\n",
    "safe_dataset_name = dataset_name.replace('/', '_')\n",
    "    \n",
    "source_dir = f'results_{safe_dataset_name}'\n",
    "export_dir = f'FINAL_OUTPUT_{safe_dataset_name}'\n",
    "zip_filename = f'{export_dir}' # shutil adds .zip automatically\n",
    "\n",
    "print(f\"üì¶ PREPARING EXPORT FOR: {dataset_name}\")\n",
    "\n",
    "# 2. Create clean export directory\n",
    "if os.path.exists(export_dir):\n",
    "    shutil.rmtree(export_dir)\n",
    "os.makedirs(export_dir)\n",
    "\n",
    "# 3. Copy files\n",
    "file_count = 0\n",
    "if os.path.exists(source_dir):\n",
    "    for filename in os.listdir(source_dir):\n",
    "        if filename.endswith(('.csv', '.pdf', '.png', '.json', '.txt')):\n",
    "            shutil.copy2(os.path.join(source_dir, filename), os.path.join(export_dir, filename))\n",
    "            file_count += 1\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Warning: Source folder '{source_dir}' not found.\")\n",
    "\n",
    "print(f\"‚úÖ Collected {file_count} files.\")\n",
    "\n",
    "# 4. Create ZIP\n",
    "print(f\"üìö Compressing to {zip_filename}.zip...\")\n",
    "shutil.make_archive(zip_filename, 'zip', export_dir)\n",
    "zip_full_path = f\"{zip_filename}.zip\"\n",
    "\n",
    "# 5. Robust Environment Detection & Download\n",
    "print(\"\\nüåç DETECTING ENVIRONMENT...\")\n",
    "\n",
    "# Check specifically for Kaggle environment variables\n",
    "is_kaggle = 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
    "# Check for Colab\n",
    "is_colab = 'google.colab' in sys.modules or os.path.exists('/content')\n",
    "\n",
    "if is_kaggle:\n",
    "    # --- KAGGLE METHOD ---\n",
    "    from IPython.display import FileLink, display\n",
    "    print(\"   Detected: Kaggle\")\n",
    "    print(\"   ‚úÖ Click the link below to download:\")\n",
    "    \n",
    "    # Kaggle needs a local path relative to the working directory\n",
    "    local_zip_name = os.path.basename(zip_full_path)\n",
    "    display(FileLink(local_zip_name))\n",
    "    \n",
    "    print(f\"\\n   (If the link doesn't appear, look in the 'Output' tab on the right)\")\n",
    "\n",
    "elif is_colab:\n",
    "    # --- GOOGLE COLAB METHOD ---\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        print(\"   Detected: Google Colab\")\n",
    "        print(\"   ‚¨áÔ∏è Triggering browser download...\")\n",
    "        files.download(zip_full_path)\n",
    "    except ImportError:\n",
    "        print(\"   ‚ö†Ô∏è Error: Detected Colab but failed to import google.colab\")\n",
    "\n",
    "else:\n",
    "    # --- LOCAL / GENERIC JUPYTER ---\n",
    "    print(\"   Detected: Local Machine / Generic Jupyter\")\n",
    "    print(f\"   üíæ File saved at: {os.path.abspath(zip_full_path)}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "979c2e78",
   "metadata": {},
   "source": [
    "## üì¶ Step 1: Install Required Libraries\n",
    "\n",
    "First, install all dependencies. This takes ~2-3 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9472d6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sentence-transformers\n",
    "!pip install -q faiss-cpu\n",
    "!pip install -q beir\n",
    "!pip install -q pandas matplotlib seaborn\n",
    "\n",
    "print(\"‚úÖ All libraries installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a481b8",
   "metadata": {},
   "source": [
    "## üì• Step 2: Import Libraries and Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877f6bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Sentence Transformers for embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# FAISS for indexes\n",
    "import faiss\n",
    "\n",
    "# BEIR for dataset\n",
    "from beir import util\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "\n",
    "# Set style for plots\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2821dd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download SciFact dataset (smallest BEIR dataset - only 5K docs)\n",
    "dataset_name = \"scifact\"\n",
    "url = f\"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{dataset_name}.zip\"\n",
    "\n",
    "print(f\"Downloading {dataset_name} dataset...\")\n",
    "data_path = util.download_and_unzip(url, \"datasets\")\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "corpus, queries, qrels = GenericDataLoader(data_folder=data_path).load(split=\"test\")\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset loaded!\")\n",
    "print(f\"   Documents: {len(corpus):,}\")\n",
    "print(f\"   Queries: {len(queries):,}\")\n",
    "print(f\"   Relevance judgments: {len(qrels):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa14d562",
   "metadata": {},
   "source": [
    "## üîç Step 3: Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5471a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example document\n",
    "doc_id = list(corpus.keys())[0]\n",
    "print(\"Example Document:\")\n",
    "print(f\"ID: {doc_id}\")\n",
    "print(f\"Title: {corpus[doc_id]['title']}\")\n",
    "print(f\"Text (first 200 chars): {corpus[doc_id]['text'][:200]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Show example query\n",
    "query_id = list(queries.keys())[0]\n",
    "print(\"Example Query:\")\n",
    "print(f\"ID: {query_id}\")\n",
    "print(f\"Text: {queries[query_id]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Show example relevance judgment\n",
    "print(\"Example Relevance Judgment:\")\n",
    "print(f\"Query {query_id} relevant documents: {qrels[query_id]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad6ead1",
   "metadata": {},
   "source": [
    "## üß† Step 4: Load Embedding Model and Encode Documents\n",
    "\n",
    "We use **BGE-base-en-v1.5** - the same model used in the paper.\n",
    "This produces 768-dimensional vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c032a6",
   "metadata": {},
   "source": [
    "### ‚ö° IMPORTANT: Speed Up Encoding by 10-20x!\n",
    "\n",
    "**If encoding is too slow (>3 minutes), enable GPU:**\n",
    "\n",
    "1. Click: **Runtime** ‚Üí **Change runtime type**\n",
    "2. Select: **Hardware accelerator** ‚Üí **T4 GPU**  \n",
    "3. Click: **Save**\n",
    "4. Re-run from the beginning\n",
    "\n",
    "**Encoding speed comparison:**\n",
    "- CPU: ~3-5 minutes for SciFact (5K docs)\n",
    "- GPU (T4): ~20-30 seconds ‚ö°\n",
    "\n",
    "The optimizations below (batch_size=128) already make it 4x faster, but GPU gives another 10x boost!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac9ab58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model (same as paper)\n",
    "model_name = 'BAAI/bge-base-en-v1.5'\n",
    "print(f\"Loading model: {model_name}\")\n",
    "print(\"This may take 1-2 minutes on first run...\")\n",
    "\n",
    "model = SentenceTransformer(model_name)\n",
    "dimension = model.get_sentence_embedding_dimension()\n",
    "\n",
    "print(f\"\\n‚úÖ Model loaded!\")\n",
    "print(f\"   Embedding dimension: {dimension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d16b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare documents for encoding\n",
    "doc_ids = list(corpus.keys())\n",
    "doc_texts = [corpus[did]['title'] + ' ' + corpus[did]['text'] for did in doc_ids]\n",
    "\n",
    "print(f\"Encoding {len(doc_texts):,} documents...\")\n",
    "print(\"This takes ~2-3 minutes for SciFact...\")\n",
    "\n",
    "# Encode all documents\n",
    "doc_embeddings = model.encode(\n",
    "    doc_texts,\n",
    "    batch_size=32,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True  # Important for cosine similarity!\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Documents encoded!\")\n",
    "print(f\"   Shape: {doc_embeddings.shape}\")\n",
    "print(f\"   Memory: {doc_embeddings.nbytes / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dd6e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode queries\n",
    "query_ids = list(queries.keys())\n",
    "query_texts = [queries[qid] for qid in query_ids]\n",
    "\n",
    "print(f\"Encoding {len(query_texts):,} queries...\")\n",
    "\n",
    "query_embeddings = model.encode(\n",
    "    query_texts,\n",
    "    batch_size=32,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Queries encoded!\")\n",
    "print(f\"   Shape: {query_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b38112",
   "metadata": {},
   "source": [
    "## üóÇÔ∏è Step 5: Build Indexes\n",
    "\n",
    "### 5.1: Flat Index (Baseline - Exact Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c31a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building Flat Index (exact search)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Create flat index (uses inner product for cosine similarity after normalization)\n",
    "flat_index = faiss.IndexFlatIP(dimension)\n",
    "flat_index.add(doc_embeddings.astype('float32'))\n",
    "\n",
    "build_time_flat = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ Flat Index built!\")\n",
    "print(f\"   Build time: {build_time_flat:.3f} seconds\")\n",
    "print(f\"   Number of vectors: {flat_index.ntotal:,}\")\n",
    "print(f\"   Index type: Exact search (100% recall)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4824cc3e",
   "metadata": {},
   "source": [
    "### 5.2: HNSW Index (Approximate Search - Fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57cb063",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building HNSW Index (approximate search)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# HNSW parameters (from paper)\n",
    "M = 16                  # Number of connections per node\n",
    "ef_construction = 100   # Build quality\n",
    "ef_search = 50          # Search quality (adjustable)\n",
    "\n",
    "# Create HNSW index\n",
    "hnsw_index = faiss.IndexHNSWFlat(dimension, M)\n",
    "hnsw_index.hnsw.efConstruction = ef_construction\n",
    "hnsw_index.add(doc_embeddings.astype('float32'))\n",
    "\n",
    "# Set search parameter\n",
    "hnsw_index.hnsw.efSearch = ef_search\n",
    "\n",
    "build_time_hnsw = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ HNSW Index built!\")\n",
    "print(f\"   Build time: {build_time_hnsw:.3f} seconds\")\n",
    "print(f\"   Number of vectors: {hnsw_index.ntotal:,}\")\n",
    "print(f\"   Parameters: M={M}, efConstruction={ef_construction}, efSearch={ef_search}\")\n",
    "print(f\"   Build time vs Flat: {build_time_hnsw/build_time_flat:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b46a319",
   "metadata": {},
   "source": [
    "### 5.3: HNSW with INT8 Quantization (Memory Efficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d8ea2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building HNSW Index with INT8 quantization...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Create quantized HNSW index (4x memory reduction!)\n",
    "hnsw_int8_index = faiss.IndexHNSWSQ(dimension, faiss.ScalarQuantizer.QT_8bit, M)\n",
    "hnsw_int8_index.hnsw.efConstruction = ef_construction\n",
    "\n",
    "# IMPORTANT: Train the quantizer first!\n",
    "print(\"   Training quantizer on document embeddings...\")\n",
    "hnsw_int8_index.train(doc_embeddings.astype('float32'))\n",
    "\n",
    "# Now add the vectors\n",
    "print(\"   Adding vectors to index...\")\n",
    "hnsw_int8_index.add(doc_embeddings.astype('float32'))\n",
    "\n",
    "# Set search parameter\n",
    "hnsw_int8_index.hnsw.efSearch = ef_search\n",
    "\n",
    "build_time_hnsw_int8 = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ HNSW INT8 Index built!\")\n",
    "print(f\"   Build time: {build_time_hnsw_int8:.3f} seconds\")\n",
    "print(f\"   Parameters: M={M}, efConstruction={ef_construction}, efSearch={ef_search}, Quantization=INT8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d705e9e1",
   "metadata": {},
   "source": [
    "## üî¨ Step 6: Run Search Experiments\n",
    "\n",
    "Now we search with all three indexes and measure performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26553a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_and_measure(index, query_embeddings, k=10, name=\"Index\"):\n",
    "    \"\"\"\n",
    "    Search with an index and measure latency.\n",
    "    \"\"\"\n",
    "    print(f\"\\nSearching with {name}...\")\n",
    "    \n",
    "    latencies = []\n",
    "    all_indices = []\n",
    "    all_scores = []\n",
    "    \n",
    "    # Search each query individually to measure latency\n",
    "    for i, query_emb in enumerate(tqdm(query_embeddings, desc=f\"{name} search\")):\n",
    "        start = time.time()\n",
    "        scores, indices = index.search(query_emb.reshape(1, -1).astype('float32'), k)\n",
    "        latency = (time.time() - start) * 1000  # Convert to ms\n",
    "        \n",
    "        latencies.append(latency)\n",
    "        all_indices.append(indices[0])\n",
    "        all_scores.append(scores[0])\n",
    "    \n",
    "    latencies = np.array(latencies)\n",
    "    \n",
    "    print(f\"\\n‚úÖ {name} Search Complete!\")\n",
    "    print(f\"   Total queries: {len(latencies):,}\")\n",
    "    print(f\"   Latency (median): {np.median(latencies):.3f} ms\")\n",
    "    print(f\"   Latency (p95): {np.percentile(latencies, 95):.3f} ms\")\n",
    "    print(f\"   Latency (p99): {np.percentile(latencies, 99):.3f} ms\")\n",
    "    print(f\"   QPS (approx): {1000 / np.median(latencies):.1f} queries/second\")\n",
    "    \n",
    "    return {\n",
    "        'name': name,\n",
    "        'indices': np.array(all_indices),\n",
    "        'scores': np.array(all_scores),\n",
    "        'latencies': latencies,\n",
    "        'median_latency': np.median(latencies),\n",
    "        'p95_latency': np.percentile(latencies, 95),\n",
    "        'p99_latency': np.percentile(latencies, 99),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28622a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search with all indexes\n",
    "k = 10  # Retrieve top-10 results\n",
    "\n",
    "results_flat = search_and_measure(flat_index, query_embeddings, k=k, name=\"Flat\")\n",
    "results_hnsw = search_and_measure(hnsw_index, query_embeddings, k=k, name=\"HNSW\")\n",
    "results_hnsw_int8 = search_and_measure(hnsw_int8_index, query_embeddings, k=k, name=\"HNSW-INT8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c25adc",
   "metadata": {},
   "source": [
    "## üìä Step 7: Evaluate Retrieval Quality\n",
    "\n",
    "Calculate Recall@10 and nDCG@10 for each index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10925de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall(retrieved_indices, qrels, query_ids, doc_ids, k=10):\n",
    "    \"\"\"\n",
    "    Calculate Recall@k: fraction of relevant docs found in top-k.\n",
    "    \"\"\"\n",
    "    recalls = []\n",
    "    \n",
    "    for i, qid in enumerate(query_ids):\n",
    "        if qid not in qrels:\n",
    "            continue\n",
    "        \n",
    "        # Get relevant documents for this query\n",
    "        relevant_docs = set(qrels[qid].keys())\n",
    "        \n",
    "        # Get retrieved documents (convert indices to doc_ids)\n",
    "        retrieved_docs = set([doc_ids[idx] for idx in retrieved_indices[i][:k] if idx >= 0])\n",
    "        \n",
    "        # Calculate recall\n",
    "        if len(relevant_docs) > 0:\n",
    "            recall = len(relevant_docs & retrieved_docs) / len(relevant_docs)\n",
    "            recalls.append(recall)\n",
    "    \n",
    "    return np.mean(recalls)\n",
    "\n",
    "\n",
    "def calculate_ndcg(retrieved_indices, qrels, query_ids, doc_ids, k=10):\n",
    "    \"\"\"\n",
    "    Calculate nDCG@k: normalized discounted cumulative gain.\n",
    "    \"\"\"\n",
    "    ndcgs = []\n",
    "    \n",
    "    for i, qid in enumerate(query_ids):\n",
    "        if qid not in qrels:\n",
    "            continue\n",
    "        \n",
    "        relevant_docs = qrels[qid]\n",
    "        retrieved_docs = [doc_ids[idx] for idx in retrieved_indices[i][:k] if idx >= 0]\n",
    "        \n",
    "        # Calculate DCG\n",
    "        dcg = 0\n",
    "        for rank, doc_id in enumerate(retrieved_docs, 1):\n",
    "            relevance = relevant_docs.get(doc_id, 0)\n",
    "            dcg += (2 ** relevance - 1) / np.log2(rank + 1)\n",
    "        \n",
    "        # Calculate ideal DCG (perfect ranking)\n",
    "        ideal_relevances = sorted(relevant_docs.values(), reverse=True)[:k]\n",
    "        idcg = sum((2 ** rel - 1) / np.log2(rank + 2) for rank, rel in enumerate(ideal_relevances))\n",
    "        \n",
    "        # Calculate nDCG\n",
    "        if idcg > 0:\n",
    "            ndcg = dcg / idcg\n",
    "            ndcgs.append(ndcg)\n",
    "    \n",
    "    return np.mean(ndcgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab10cacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for all indexes\n",
    "print(\"Calculating quality metrics...\\n\")\n",
    "\n",
    "for results in [results_flat, results_hnsw, results_hnsw_int8]:\n",
    "    recall = calculate_recall(results['indices'], qrels, query_ids, doc_ids, k=10)\n",
    "    ndcg = calculate_ndcg(results['indices'], qrels, query_ids, doc_ids, k=10)\n",
    "    \n",
    "    results['recall@10'] = recall\n",
    "    results['ndcg@10'] = ndcg\n",
    "    \n",
    "    print(f\"{results['name']}:\")\n",
    "    print(f\"   Recall@10: {recall:.4f}\")\n",
    "    print(f\"   nDCG@10: {ndcg:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51dd218",
   "metadata": {},
   "source": [
    "## üìà Step 8: Visualize Results\n",
    "\n",
    "Create comparison charts to understand the trade-offs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ecf81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame([\n",
    "    {\n",
    "        'Index': results['name'],\n",
    "        'Recall@10': results['recall@10'],\n",
    "        'nDCG@10': results['ndcg@10'],\n",
    "        'Median Latency (ms)': results['median_latency'],\n",
    "        'P95 Latency (ms)': results['p95_latency'],\n",
    "        'P99 Latency (ms)': results['p99_latency'],\n",
    "    }\n",
    "    for results in [results_flat, results_hnsw, results_hnsw_int8]\n",
    "])\n",
    "\n",
    "print(\"\\nüìä COMPARISON TABLE\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8673001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Speed vs Quality Trade-off\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for results in [results_flat, results_hnsw, results_hnsw_int8]:\n",
    "    ax.scatter(\n",
    "        results['median_latency'],\n",
    "        results['ndcg@10'],\n",
    "        s=200,\n",
    "        alpha=0.7,\n",
    "        label=results['name']\n",
    "    )\n",
    "    ax.annotate(\n",
    "        results['name'],\n",
    "        (results['median_latency'], results['ndcg@10']),\n",
    "        xytext=(10, 10),\n",
    "        textcoords='offset points',\n",
    "        fontsize=10,\n",
    "        bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.3)\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('Median Latency (ms)', fontsize=12)\n",
    "ax.set_ylabel('nDCG@10', fontsize=12)\n",
    "ax.set_title('Speed vs Quality Trade-off (SciFact Dataset)', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('speed_vs_quality.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Plot saved as 'speed_vs_quality.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1310c954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Latency Distribution Comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4), sharey=True)\n",
    "\n",
    "for ax, results in zip(axes, [results_flat, results_hnsw, results_hnsw_int8]):\n",
    "    ax.hist(results['latencies'], bins=30, alpha=0.7, edgecolor='black')\n",
    "    ax.axvline(results['median_latency'], color='red', linestyle='--', linewidth=2, label='Median')\n",
    "    ax.axvline(results['p95_latency'], color='orange', linestyle='--', linewidth=2, label='P95')\n",
    "    ax.set_xlabel('Latency (ms)', fontsize=10)\n",
    "    ax.set_title(f\"{results['name']}\\n(median: {results['median_latency']:.2f}ms)\", fontsize=11, fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes[0].set_ylabel('Frequency', fontsize=10)\n",
    "fig.suptitle('Query Latency Distributions', fontsize=14, fontweight='bold', y=1.02)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('latency_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Plot saved as 'latency_distributions.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193b1092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3: Bar Chart Comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Quality metrics\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, comparison_df['Recall@10'], width, label='Recall@10', alpha=0.8)\n",
    "axes[0].bar(x + width/2, comparison_df['nDCG@10'], width, label='nDCG@10', alpha=0.8)\n",
    "axes[0].set_ylabel('Score', fontsize=11)\n",
    "axes[0].set_title('Retrieval Quality Comparison', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(comparison_df['Index'])\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "axes[0].set_ylim([0, 1.0])\n",
    "\n",
    "# Latency metrics\n",
    "axes[1].bar(x, comparison_df['Median Latency (ms)'], alpha=0.8, color='steelblue')\n",
    "axes[1].set_ylabel('Latency (ms)', fontsize=11)\n",
    "axes[1].set_title('Query Latency Comparison (Median)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(comparison_df['Index'])\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for ax in axes:\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, fmt='%.3f', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('comparison_bars.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Plot saved as 'comparison_bars.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c16b975",
   "metadata": {},
   "source": [
    "## üìù Step 9: Key Findings Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166048de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate speedups and quality preservation\n",
    "speedup_hnsw = results_flat['median_latency'] / results_hnsw['median_latency']\n",
    "speedup_hnsw_int8 = results_flat['median_latency'] / results_hnsw_int8['median_latency']\n",
    "\n",
    "quality_loss_hnsw = (1 - results_hnsw['ndcg@10'] / results_flat['ndcg@10']) * 100\n",
    "quality_loss_hnsw_int8 = (1 - results_hnsw_int8['ndcg@10'] / results_flat['ndcg@10']) * 100\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ KEY FINDINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1. SPEED:\")\n",
    "print(f\"   ‚Ä¢ HNSW is {speedup_hnsw:.2f}x faster than Flat index\")\n",
    "print(f\"   ‚Ä¢ HNSW-INT8 is {speedup_hnsw_int8:.2f}x faster than Flat index\")\n",
    "\n",
    "print(f\"\\n2. QUALITY:\")\n",
    "print(f\"   ‚Ä¢ HNSW preserves {100-quality_loss_hnsw:.2f}% of Flat quality (loss: {quality_loss_hnsw:.2f}%)\")\n",
    "print(f\"   ‚Ä¢ HNSW-INT8 preserves {100-quality_loss_hnsw_int8:.2f}% of Flat quality (loss: {quality_loss_hnsw_int8:.2f}%)\")\n",
    "\n",
    "print(f\"\\n3. BUILD TIME:\")\n",
    "print(f\"   ‚Ä¢ Flat: {build_time_flat:.3f}s (fastest to build)\")\n",
    "print(f\"   ‚Ä¢ HNSW: {build_time_hnsw:.3f}s ({build_time_hnsw/build_time_flat:.2f}x slower)\")\n",
    "print(f\"   ‚Ä¢ HNSW-INT8: {build_time_hnsw_int8:.3f}s ({build_time_hnsw_int8/build_time_flat:.2f}x slower)\")\n",
    "\n",
    "print(f\"\\n4. MEMORY:\")\n",
    "print(f\"   ‚Ä¢ Documents: {doc_embeddings.nbytes / (1024**2):.2f} MB (float32)\")\n",
    "print(f\"   ‚Ä¢ INT8 would use ~{doc_embeddings.nbytes / (1024**2) / 4:.2f} MB (4x compression)\")\n",
    "\n",
    "print(f\"\\n5. RECOMMENDATION for SciFact (5K docs):\")\n",
    "if speedup_hnsw < 2:\n",
    "    print(f\"   ‚û°Ô∏è Flat index is sufficient (speedup not significant)\")\n",
    "else:\n",
    "    print(f\"   ‚û°Ô∏è HNSW recommended (significant speedup with minimal quality loss)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4514088e",
   "metadata": {},
   "source": [
    "## üíæ Step 10: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09456d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comparison table\n",
    "comparison_df.to_csv('experiment_results.csv', index=False)\n",
    "print(\"‚úÖ Results saved to 'experiment_results.csv'\")\n",
    "\n",
    "# Save detailed latency data\n",
    "latency_df = pd.DataFrame({\n",
    "    'Flat': results_flat['latencies'],\n",
    "    'HNSW': results_hnsw['latencies'],\n",
    "    'HNSW-INT8': results_hnsw_int8['latencies']\n",
    "})\n",
    "latency_df.to_csv('latency_data.csv', index=False)\n",
    "print(\"‚úÖ Latency data saved to 'latency_data.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2a9a36",
   "metadata": {},
   "source": [
    "## üöÄ Next Steps\n",
    "\n",
    "Congratulations! You've completed your first experiments. Here's what you can do next:\n",
    "\n",
    "### Option 1: Test Different Parameters\n",
    "- Try different `efSearch` values: [10, 30, 50, 100, 200]\n",
    "- See how it affects speed vs quality trade-off\n",
    "\n",
    "### Option 2: Test on Larger Dataset\n",
    "- Try FiQA (57K docs) or NQ (2.7M docs)\n",
    "- See how speedup increases with dataset size\n",
    "\n",
    "### Option 3: Add BM25 Baseline\n",
    "- Compare dense (neural) vs sparse (BM25) retrieval\n",
    "- Implement hybrid search (combine both)\n",
    "\n",
    "### Option 4: Memory Constraints Study\n",
    "- Test different quantization levels\n",
    "- Measure quality vs memory trade-offs\n",
    "\n",
    "---\n",
    "\n",
    "**Great job!** üéâ You've successfully replicated the core experiments from the paper!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41ad324",
   "metadata": {},
   "source": [
    "## üîß BONUS: Parameter Tuning - Testing Different efSearch Values\n",
    "\n",
    "Let's explore how the `efSearch` parameter affects the speed vs quality trade-off for HNSW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389dec35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different efSearch values\n",
    "efSearch_values = [10, 20, 30, 50, 75, 100, 150, 200]\n",
    "\n",
    "print(\"üî¨ Testing different efSearch parameters...\")\n",
    "print(f\"Values to test: {efSearch_values}\")\n",
    "print(f\"This will take ~{len(efSearch_values)} minutes...\\n\")\n",
    "\n",
    "results_by_efSearch = []\n",
    "\n",
    "for ef in tqdm(efSearch_values, desc=\"Testing efSearch values\"):\n",
    "    # Set the efSearch parameter\n",
    "    hnsw_index.hnsw.efSearch = ef\n",
    "    \n",
    "    # Run searches and measure performance\n",
    "    latencies = []\n",
    "    all_indices = []\n",
    "    \n",
    "    for query_emb in query_embeddings:\n",
    "        start = time.time()\n",
    "        scores, indices = hnsw_index.search(query_emb.reshape(1, -1).astype('float32'), k=10)\n",
    "        latency = (time.time() - start) * 1000  # Convert to milliseconds\n",
    "        latencies.append(latency)\n",
    "        all_indices.append(indices[0])\n",
    "    \n",
    "    latencies = np.array(latencies)\n",
    "    all_indices = np.array(all_indices)\n",
    "    \n",
    "    # Calculate quality metrics\n",
    "    recall = calculate_recall(all_indices, qrels, query_ids, doc_ids, k=10)\n",
    "    ndcg = calculate_ndcg(all_indices, qrels, query_ids, doc_ids, k=10)\n",
    "    \n",
    "    # Store results for this efSearch value\n",
    "    results_by_efSearch.append({\n",
    "        'efSearch': ef,\n",
    "        'median_latency': np.median(latencies),\n",
    "        'p95_latency': np.percentile(latencies, 95),\n",
    "        'recall@10': recall,\n",
    "        'ndcg@10': ndcg,\n",
    "        'qps': 1000 / np.median(latencies)  # Queries per second\n",
    "    })\n",
    "    \n",
    "    print(f\"efSearch={ef:3d}: Latency={np.median(latencies):6.2f}ms, nDCG@10={ndcg:.4f}, Recall@10={recall:.4f}\")\n",
    "\n",
    "# Convert to DataFrame for easy analysis\n",
    "efSearch_df = pd.DataFrame(results_by_efSearch)\n",
    "\n",
    "print(\"\\n‚úÖ Parameter tuning complete!\")\n",
    "print(\"\\nüìä RESULTS TABLE:\")\n",
    "print(\"=\"*90)\n",
    "print(efSearch_df.to_string(index=False))\n",
    "print(\"=\"*90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fead9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Speed vs Quality Trade-off for different efSearch values\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "# Plot the curve\n",
    "ax.plot(efSearch_df['median_latency'], efSearch_df['ndcg@10'], \n",
    "        marker='o', markersize=8, linewidth=2, color='steelblue', label='HNSW')\n",
    "\n",
    "# Annotate each point with efSearch value\n",
    "for idx, row in efSearch_df.iterrows():\n",
    "    ax.annotate(f\"ef={row['efSearch']}\", \n",
    "                (row['median_latency'], row['ndcg@10']),\n",
    "                xytext=(5, 5), textcoords='offset points',\n",
    "                fontsize=9, alpha=0.8)\n",
    "\n",
    "# Add reference line for Flat index\n",
    "ax.axhline(y=results_flat['ndcg@10'], color='red', linestyle='--', \n",
    "           linewidth=2, label=f\"Flat Index (nDCG={results_flat['ndcg@10']:.4f})\", alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Median Latency (ms)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('nDCG@10', fontsize=12, fontweight='bold')\n",
    "ax.set_title('HNSW: Speed vs Quality Trade-off for Different efSearch Values', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(fontsize=11, loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('efSearch_speed_vs_quality.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Plot saved as 'efSearch_speed_vs_quality.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f69dafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Multi-metric comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 2a: Latency vs efSearch\n",
    "axes[0, 0].plot(efSearch_df['efSearch'], efSearch_df['median_latency'], \n",
    "                marker='o', linewidth=2, color='steelblue', markersize=8)\n",
    "axes[0, 0].fill_between(efSearch_df['efSearch'], \n",
    "                         efSearch_df['median_latency'], \n",
    "                         efSearch_df['p95_latency'], \n",
    "                         alpha=0.3, label='p50-p95 range')\n",
    "axes[0, 0].set_xlabel('efSearch', fontsize=11, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Latency (ms)', fontsize=11, fontweight='bold')\n",
    "axes[0, 0].set_title('Query Latency vs efSearch', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Plot 2b: nDCG vs efSearch\n",
    "axes[0, 1].plot(efSearch_df['efSearch'], efSearch_df['ndcg@10'], \n",
    "                marker='s', linewidth=2, color='green', markersize=8)\n",
    "axes[0, 1].axhline(y=results_flat['ndcg@10'], color='red', linestyle='--', \n",
    "                   linewidth=2, label='Flat Index', alpha=0.7)\n",
    "axes[0, 1].set_xlabel('efSearch', fontsize=11, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('nDCG@10', fontsize=11, fontweight='bold')\n",
    "axes[0, 1].set_title('Retrieval Quality vs efSearch', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Plot 2c: Recall vs efSearch\n",
    "axes[1, 0].plot(efSearch_df['efSearch'], efSearch_df['recall@10'], \n",
    "                marker='^', linewidth=2, color='orange', markersize=8)\n",
    "axes[1, 0].axhline(y=results_flat['recall@10'], color='red', linestyle='--', \n",
    "                   linewidth=2, label='Flat Index', alpha=0.7)\n",
    "axes[1, 0].set_xlabel('efSearch', fontsize=11, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Recall@10', fontsize=11, fontweight='bold')\n",
    "axes[1, 0].set_title('Recall vs efSearch', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Plot 2d: QPS vs efSearch\n",
    "axes[1, 1].plot(efSearch_df['efSearch'], efSearch_df['qps'], \n",
    "                marker='D', linewidth=2, color='purple', markersize=8)\n",
    "axes[1, 1].set_xlabel('efSearch', fontsize=11, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Queries Per Second (QPS)', fontsize=11, fontweight='bold')\n",
    "axes[1, 1].set_title('Throughput vs efSearch', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "fig.suptitle('HNSW Parameter Analysis: Impact of efSearch', \n",
    "             fontsize=15, fontweight='bold', y=0.995)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('efSearch_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Plot saved as 'efSearch_analysis.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e25d740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the optimal efSearch value (best quality/speed balance)\n",
    "# Calculate efficiency score: quality / latency (higher is better)\n",
    "efSearch_df['efficiency_score'] = efSearch_df['ndcg@10'] / (efSearch_df['median_latency'] / 1000)\n",
    "\n",
    "optimal_idx = efSearch_df['efficiency_score'].idxmax()\n",
    "optimal_efSearch = efSearch_df.loc[optimal_idx]\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"üéØ OPTIMAL efSearch VALUE\")\n",
    "print(\"=\"*90)\n",
    "print(f\"\\nBest efficiency score at efSearch = {optimal_efSearch['efSearch']}\")\n",
    "print(f\"\\nMetrics:\")\n",
    "print(f\"   ‚Ä¢ Median Latency: {optimal_efSearch['median_latency']:.2f} ms\")\n",
    "print(f\"   ‚Ä¢ P95 Latency: {optimal_efSearch['p95_latency']:.2f} ms\")\n",
    "print(f\"   ‚Ä¢ nDCG@10: {optimal_efSearch['ndcg@10']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Recall@10: {optimal_efSearch['recall@10']:.4f}\")\n",
    "print(f\"   ‚Ä¢ QPS: {optimal_efSearch['qps']:.1f} queries/second\")\n",
    "print(f\"   ‚Ä¢ Efficiency Score: {optimal_efSearch['efficiency_score']:.2f}\")\n",
    "\n",
    "print(f\"\\nüìä Comparison with extremes:\")\n",
    "print(f\"\\nLowest efSearch ({efSearch_df.iloc[0]['efSearch']}):\")\n",
    "print(f\"   ‚Ä¢ {efSearch_df.iloc[0]['median_latency']:.2f}ms (fastest)\")\n",
    "print(f\"   ‚Ä¢ nDCG@10: {efSearch_df.iloc[0]['ndcg@10']:.4f} ({(1-efSearch_df.iloc[0]['ndcg@10']/results_flat['ndcg@10'])*100:.1f}% quality loss vs Flat)\")\n",
    "\n",
    "print(f\"\\nHighest efSearch ({efSearch_df.iloc[-1]['efSearch']}):\")\n",
    "print(f\"   ‚Ä¢ {efSearch_df.iloc[-1]['median_latency']:.2f}ms (slowest)\")\n",
    "print(f\"   ‚Ä¢ nDCG@10: {efSearch_df.iloc[-1]['ndcg@10']:.4f} ({(1-efSearch_df.iloc[-1]['ndcg@10']/results_flat['ndcg@10'])*100:.1f}% quality loss vs Flat)\")\n",
    "\n",
    "print(f\"\\nOptimal efSearch ({optimal_efSearch['efSearch']}):\")\n",
    "print(f\"   ‚Ä¢ {optimal_efSearch['median_latency']:.2f}ms (balanced)\")\n",
    "print(f\"   ‚Ä¢ nDCG@10: {optimal_efSearch['ndcg@10']:.4f} ({(1-optimal_efSearch['ndcg@10']/results_flat['ndcg@10'])*100:.1f}% quality loss vs Flat)\")\n",
    "\n",
    "print(f\"\\nüí° RECOMMENDATION:\")\n",
    "print(f\"   Use efSearch = {optimal_efSearch['efSearch']} for best quality/speed balance\")\n",
    "print(f\"   This gives {optimal_efSearch['ndcg@10']/results_flat['ndcg@10']*100:.1f}% of Flat quality\")\n",
    "print(f\"   at {results_flat['median_latency']/optimal_efSearch['median_latency']:.1f}x speedup!\")\n",
    "print(\"\\n\" + \"=\"*90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfe959f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save efSearch tuning results\n",
    "efSearch_df.to_csv('efSearch_tuning_results.csv', index=False)\n",
    "print(\"‚úÖ Results saved to 'efSearch_tuning_results.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ec31d3",
   "metadata": {},
   "source": [
    "## üîß BONUS 2: M Parameter Tuning - Testing Different Graph Connectivity\n",
    "\n",
    "Now let's explore how the `M` parameter affects HNSW performance. Unlike `efSearch`, changing `M` requires rebuilding the entire index for each value.\n",
    "\n",
    "**What is M?**\n",
    "- M = number of bidirectional connections per node in the graph\n",
    "- Higher M = better quality but more memory and slower builds\n",
    "- Typical range: 8-64 (paper uses M=16 as baseline)\n",
    "\n",
    "**Trade-offs:**\n",
    "- **Low M (8-12)**: Faster builds, less memory, lower quality\n",
    "- **Medium M (16-24)**: Balanced (recommended)\n",
    "- **High M (32-64)**: Best quality, but expensive builds and memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62afb4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different M values (graph connectivity)\n",
    "M_values = [8, 12, 16, 24, 32]\n",
    "\n",
    "print(\"üî¨ Testing different M parameters...\")\n",
    "print(f\"Values to test: {M_values}\")\n",
    "print(f\"‚ö†Ô∏è  WARNING: This requires rebuilding the index for each M value!\")\n",
    "print(f\"This will take ~{len(M_values) * 2} minutes (longer than efSearch tuning)...\\n\")\n",
    "\n",
    "results_by_M = []\n",
    "\n",
    "for M_test in tqdm(M_values, desc=\"Testing M values\"):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Building HNSW index with M={M_test}...\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Build a new index with this M value\n",
    "    start_build = time.time()\n",
    "    index_M = faiss.IndexHNSWFlat(dimension, M_test)\n",
    "    index_M.hnsw.efConstruction = ef_construction  # Use same efConstruction as baseline\n",
    "    index_M.add(doc_embeddings.astype('float32'))\n",
    "    index_M.hnsw.efSearch = ef_search  # Use same efSearch as baseline\n",
    "    build_time = time.time() - start_build\n",
    "    \n",
    "    # Run searches and measure performance\n",
    "    latencies = []\n",
    "    all_indices = []\n",
    "    \n",
    "    for query_emb in query_embeddings:\n",
    "        start = time.time()\n",
    "        scores, indices = index_M.search(query_emb.reshape(1, -1).astype('float32'), k=10)\n",
    "        latency = (time.time() - start) * 1000  # Convert to milliseconds\n",
    "        latencies.append(latency)\n",
    "        all_indices.append(indices[0])\n",
    "    \n",
    "    latencies = np.array(latencies)\n",
    "    all_indices = np.array(all_indices)\n",
    "    \n",
    "    # Calculate quality metrics\n",
    "    recall = calculate_recall(all_indices, qrels, query_ids, doc_ids, k=10)\n",
    "    ndcg = calculate_ndcg(all_indices, qrels, query_ids, doc_ids, k=10)\n",
    "    \n",
    "    # Store results for this M value\n",
    "    results_by_M.append({\n",
    "        'M': M_test,\n",
    "        'build_time': build_time,\n",
    "        'median_latency': np.median(latencies),\n",
    "        'p95_latency': np.percentile(latencies, 95),\n",
    "        'recall@10': recall,\n",
    "        'ndcg@10': ndcg,\n",
    "        'qps': 1000 / np.median(latencies)  # Queries per second\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n‚úÖ M={M_test} complete:\")\n",
    "    print(f\"   Build time: {build_time:.3f}s\")\n",
    "    print(f\"   Latency: {np.median(latencies):.2f}ms (median), {np.percentile(latencies, 95):.2f}ms (p95)\")\n",
    "    print(f\"   nDCG@10: {ndcg:.4f}, Recall@10: {recall:.4f}\")\n",
    "\n",
    "# Convert to DataFrame for easy analysis\n",
    "M_df = pd.DataFrame(results_by_M)\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"‚úÖ M Parameter tuning complete!\")\n",
    "print(\"=\"*90)\n",
    "print(\"\\nüìä RESULTS TABLE:\")\n",
    "print(\"=\"*90)\n",
    "print(M_df.to_string(index=False))\n",
    "print(\"=\"*90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab89b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: M Parameter Impact - Speed vs Quality\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "# Plot the curve\n",
    "ax.plot(M_df['median_latency'], M_df['ndcg@10'], \n",
    "        marker='o', markersize=10, linewidth=2, color='darkgreen', label='HNSW (varying M)')\n",
    "\n",
    "# Annotate each point with M value\n",
    "for idx, row in M_df.iterrows():\n",
    "    ax.annotate(f\"M={row['M']}\", \n",
    "                (row['median_latency'], row['ndcg@10']),\n",
    "                xytext=(8, 8), textcoords='offset points',\n",
    "                fontsize=10, fontweight='bold', alpha=0.8)\n",
    "\n",
    "# Add reference line for Flat index\n",
    "ax.axhline(y=results_flat['ndcg@10'], color='red', linestyle='--', \n",
    "           linewidth=2, label=f\"Flat Index (nDCG={results_flat['ndcg@10']:.4f})\", alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Median Latency (ms)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('nDCG@10', fontsize=12, fontweight='bold')\n",
    "ax.set_title('HNSW: Speed vs Quality Trade-off for Different M Values', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(fontsize=11, loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('M_speed_vs_quality.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Plot saved as 'M_speed_vs_quality.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa188d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: M Parameter Multi-metric Analysis\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "# Plot 2a: Build Time vs M\n",
    "axes[0, 0].plot(M_df['M'], M_df['build_time'], \n",
    "                marker='o', linewidth=2, color='red', markersize=8)\n",
    "axes[0, 0].set_xlabel('M (Graph Connectivity)', fontsize=11, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Build Time (seconds)', fontsize=11, fontweight='bold')\n",
    "axes[0, 0].set_title('Index Build Time vs M', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2b: Query Latency vs M\n",
    "axes[0, 1].plot(M_df['M'], M_df['median_latency'], \n",
    "                marker='s', linewidth=2, color='steelblue', markersize=8)\n",
    "axes[0, 1].fill_between(M_df['M'], \n",
    "                         M_df['median_latency'], \n",
    "                         M_df['p95_latency'], \n",
    "                         alpha=0.3, label='p50-p95 range')\n",
    "axes[0, 1].set_xlabel('M (Graph Connectivity)', fontsize=11, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Latency (ms)', fontsize=11, fontweight='bold')\n",
    "axes[0, 1].set_title('Query Latency vs M', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Plot 2c: nDCG vs M\n",
    "axes[0, 2].plot(M_df['M'], M_df['ndcg@10'], \n",
    "                marker='^', linewidth=2, color='green', markersize=8)\n",
    "axes[0, 2].axhline(y=results_flat['ndcg@10'], color='red', linestyle='--', \n",
    "                   linewidth=2, label='Flat Index', alpha=0.7)\n",
    "axes[0, 2].set_xlabel('M (Graph Connectivity)', fontsize=11, fontweight='bold')\n",
    "axes[0, 2].set_ylabel('nDCG@10', fontsize=11, fontweight='bold')\n",
    "axes[0, 2].set_title('Retrieval Quality vs M', fontsize=12, fontweight='bold')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "axes[0, 2].legend()\n",
    "\n",
    "# Plot 2d: Recall vs M\n",
    "axes[1, 0].plot(M_df['M'], M_df['recall@10'], \n",
    "                marker='D', linewidth=2, color='orange', markersize=8)\n",
    "axes[1, 0].axhline(y=results_flat['recall@10'], color='red', linestyle='--', \n",
    "                   linewidth=2, label='Flat Index', alpha=0.7)\n",
    "axes[1, 0].set_xlabel('M (Graph Connectivity)', fontsize=11, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Recall@10', fontsize=11, fontweight='bold')\n",
    "axes[1, 0].set_title('Recall vs M', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Plot 2e: QPS vs M\n",
    "axes[1, 1].plot(M_df['M'], M_df['qps'], \n",
    "                marker='p', linewidth=2, color='purple', markersize=8)\n",
    "axes[1, 1].set_xlabel('M (Graph Connectivity)', fontsize=11, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Queries Per Second (QPS)', fontsize=11, fontweight='bold')\n",
    "axes[1, 1].set_title('Throughput vs M', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2f: Build Time vs Quality (Trade-off)\n",
    "axes[1, 2].scatter(M_df['build_time'], M_df['ndcg@10'], \n",
    "                   s=100, c=M_df['M'], cmap='viridis', alpha=0.7, edgecolors='black')\n",
    "for idx, row in M_df.iterrows():\n",
    "    axes[1, 2].annotate(f\"M={row['M']}\", \n",
    "                        (row['build_time'], row['ndcg@10']),\n",
    "                        xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "axes[1, 2].set_xlabel('Build Time (seconds)', fontsize=11, fontweight='bold')\n",
    "axes[1, 2].set_ylabel('nDCG@10', fontsize=11, fontweight='bold')\n",
    "axes[1, 2].set_title('Build Cost vs Quality', fontsize=12, fontweight='bold')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "fig.suptitle('HNSW Parameter Analysis: Impact of M (Graph Connectivity)', \n",
    "             fontsize=15, fontweight='bold', y=0.995)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('M_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Plot saved as 'M_analysis.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2588faf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the optimal M value (best overall balance)\n",
    "# Calculate efficiency score: quality / (latency * build_time)\n",
    "# This balances search quality, search speed, AND build cost\n",
    "M_df['efficiency_score'] = M_df['ndcg@10'] / ((M_df['median_latency'] / 1000) * M_df['build_time'])\n",
    "\n",
    "optimal_idx = M_df['efficiency_score'].idxmax()\n",
    "optimal_M = M_df.loc[optimal_idx]\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"üéØ OPTIMAL M VALUE (Best Overall Balance)\")\n",
    "print(\"=\"*90)\n",
    "print(f\"\\nBest efficiency score at M = {optimal_M['M']}\")\n",
    "print(f\"\\nMetrics:\")\n",
    "print(f\"   ‚Ä¢ Build Time: {optimal_M['build_time']:.3f} seconds\")\n",
    "print(f\"   ‚Ä¢ Median Latency: {optimal_M['median_latency']:.2f} ms\")\n",
    "print(f\"   ‚Ä¢ P95 Latency: {optimal_M['p95_latency']:.2f} ms\")\n",
    "print(f\"   ‚Ä¢ nDCG@10: {optimal_M['ndcg@10']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Recall@10: {optimal_M['recall@10']:.4f}\")\n",
    "print(f\"   ‚Ä¢ QPS: {optimal_M['qps']:.1f} queries/second\")\n",
    "print(f\"   ‚Ä¢ Efficiency Score: {optimal_M['efficiency_score']:.4f}\")\n",
    "\n",
    "print(f\"\\nüìä Comparison across all M values:\")\n",
    "print(f\"\\nLowest M ({M_df.iloc[0]['M']}):\")\n",
    "print(f\"   ‚Ä¢ Build: {M_df.iloc[0]['build_time']:.3f}s (fastest build)\")\n",
    "print(f\"   ‚Ä¢ Query: {M_df.iloc[0]['median_latency']:.2f}ms\")\n",
    "print(f\"   ‚Ä¢ nDCG@10: {M_df.iloc[0]['ndcg@10']:.4f} ({(1-M_df.iloc[0]['ndcg@10']/results_flat['ndcg@10'])*100:.1f}% quality loss)\")\n",
    "\n",
    "print(f\"\\nBaseline M ({M_df[M_df['M']==16].iloc[0]['M']} - paper's choice):\")\n",
    "baseline_M = M_df[M_df['M']==16].iloc[0]\n",
    "print(f\"   ‚Ä¢ Build: {baseline_M['build_time']:.3f}s\")\n",
    "print(f\"   ‚Ä¢ Query: {baseline_M['median_latency']:.2f}ms\")\n",
    "print(f\"   ‚Ä¢ nDCG@10: {baseline_M['ndcg@10']:.4f} ({(1-baseline_M['ndcg@10']/results_flat['ndcg@10'])*100:.1f}% quality loss)\")\n",
    "\n",
    "print(f\"\\nHighest M ({M_df.iloc[-1]['M']}):\")\n",
    "print(f\"   ‚Ä¢ Build: {M_df.iloc[-1]['build_time']:.3f}s (slowest build)\")\n",
    "print(f\"   ‚Ä¢ Query: {M_df.iloc[-1]['median_latency']:.2f}ms\")\n",
    "print(f\"   ‚Ä¢ nDCG@10: {M_df.iloc[-1]['ndcg@10']:.4f} ({(1-M_df.iloc[-1]['ndcg@10']/results_flat['ndcg@10'])*100:.1f}% quality loss)\")\n",
    "\n",
    "print(f\"\\nOptimal M ({optimal_M['M']}):\")\n",
    "print(f\"   ‚Ä¢ Build: {optimal_M['build_time']:.3f}s\")\n",
    "print(f\"   ‚Ä¢ Query: {optimal_M['median_latency']:.2f}ms\")\n",
    "print(f\"   ‚Ä¢ nDCG@10: {optimal_M['ndcg@10']:.4f} ({(1-optimal_M['ndcg@10']/results_flat['ndcg@10'])*100:.1f}% quality loss)\")\n",
    "\n",
    "# Key insights\n",
    "print(f\"\\nüí° KEY INSIGHTS:\")\n",
    "\n",
    "# Quality improvement from M=8 to M=16\n",
    "quality_gain = (M_df[M_df['M']==16].iloc[0]['ndcg@10'] - M_df[M_df['M']==8].iloc[0]['ndcg@10']) / M_df[M_df['M']==8].iloc[0]['ndcg@10'] * 100\n",
    "build_cost = (M_df[M_df['M']==16].iloc[0]['build_time'] - M_df[M_df['M']==8].iloc[0]['build_time']) / M_df[M_df['M']==8].iloc[0]['build_time'] * 100\n",
    "print(f\"   ‚Ä¢ Doubling M from 8‚Üí16 improves quality by {quality_gain:.1f}% but increases build time by {build_cost:.1f}%\")\n",
    "\n",
    "# Diminishing returns from M=16 to M=32\n",
    "if len(M_df[M_df['M']==32]) > 0:\n",
    "    quality_gain_high = (M_df[M_df['M']==32].iloc[0]['ndcg@10'] - M_df[M_df['M']==16].iloc[0]['ndcg@10']) / M_df[M_df['M']==16].iloc[0]['ndcg@10'] * 100\n",
    "    build_cost_high = (M_df[M_df['M']==32].iloc[0]['build_time'] - M_df[M_df['M']==16].iloc[0]['build_time']) / M_df[M_df['M']==16].iloc[0]['build_time'] * 100\n",
    "    print(f\"   ‚Ä¢ Doubling M from 16‚Üí32 improves quality by only {quality_gain_high:.1f}% but increases build time by {build_cost_high:.1f}%\")\n",
    "    print(f\"   ‚Ä¢ ‚ö†Ô∏è  Diminishing returns: Higher M values are not cost-effective for this dataset\")\n",
    "\n",
    "print(f\"\\nüéØ RECOMMENDATION:\")\n",
    "print(f\"   ‚Ä¢ For production: Use M={optimal_M['M']} (best balance of build cost, speed, and quality)\")\n",
    "print(f\"   ‚Ä¢ Paper's choice (M=16) is {'optimal' if optimal_M['M'] == 16 else 'close to optimal'} for this dataset\")\n",
    "print(f\"   ‚Ä¢ M < 16: Only if build time is critical and slight quality loss is acceptable\")\n",
    "print(f\"   ‚Ä¢ M > 16: Only if you need maximum quality and can afford longer builds\")\n",
    "print(\"\\n\" + \"=\"*90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d83f76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save M tuning results\n",
    "M_df.to_csv('M_tuning_results.csv', index=False)\n",
    "print(\"‚úÖ M parameter tuning results saved to 'M_tuning_results.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de3c3f0",
   "metadata": {},
   "source": [
    "## üéì Final Comparison: efSearch vs M Parameter Impact\n",
    "\n",
    "Let's compare the impact of both parameters side-by-side to understand which one matters more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6e9928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare parameter impacts\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Left plot: Quality range for each parameter\n",
    "axes[0].plot(efSearch_df['efSearch'], efSearch_df['ndcg@10'], \n",
    "             marker='o', linewidth=2, markersize=6, label='efSearch (runtime)', color='steelblue')\n",
    "axes[0].set_xlabel('efSearch Value', fontsize=11, fontweight='bold')\n",
    "axes[0].set_ylabel('nDCG@10', fontsize=11, fontweight='bold')\n",
    "axes[0].set_title('efSearch Impact on Quality (No Rebuild Required)', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axhline(y=results_flat['ndcg@10'], color='red', linestyle='--', linewidth=1.5, alpha=0.5, label='Flat Index')\n",
    "axes[0].legend()\n",
    "\n",
    "# Right plot: M parameter\n",
    "axes[1].plot(M_df['M'], M_df['ndcg@10'], \n",
    "             marker='s', linewidth=2, markersize=8, label='M (build-time)', color='darkgreen')\n",
    "axes[1].set_xlabel('M Value (Graph Connectivity)', fontsize=11, fontweight='bold')\n",
    "axes[1].set_ylabel('nDCG@10', fontsize=11, fontweight='bold')\n",
    "axes[1].set_title('M Impact on Quality (Rebuild Required)', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axhline(y=results_flat['ndcg@10'], color='red', linestyle='--', linewidth=1.5, alpha=0.5, label='Flat Index')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('parameter_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Plot saved as 'parameter_comparison.png'\")\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"üìä PARAMETER IMPACT SUMMARY\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "efSearch_quality_range = efSearch_df['ndcg@10'].max() - efSearch_df['ndcg@10'].min()\n",
    "M_quality_range = M_df['ndcg@10'].max() - M_df['ndcg@10'].min()\n",
    "\n",
    "print(f\"\\nefSearch Parameter (Runtime - No Rebuild):\")\n",
    "print(f\"   ‚Ä¢ Quality range: {efSearch_quality_range:.4f} ({efSearch_quality_range/results_flat['ndcg@10']*100:.1f}% of Flat performance)\")\n",
    "print(f\"   ‚Ä¢ Latency range: {efSearch_df['median_latency'].min():.2f}ms - {efSearch_df['median_latency'].max():.2f}ms\")\n",
    "print(f\"   ‚Ä¢ Speedup range: {results_flat['median_latency']/efSearch_df['median_latency'].max():.1f}x - {results_flat['median_latency']/efSearch_df['median_latency'].min():.1f}x vs Flat\")\n",
    "print(f\"   ‚Ä¢ Advantage: ‚úÖ Fast to test, no rebuild needed\")\n",
    "\n",
    "print(f\"\\nM Parameter (Build-Time - Rebuild Required):\")\n",
    "print(f\"   ‚Ä¢ Quality range: {M_quality_range:.4f} ({M_quality_range/results_flat['ndcg@10']*100:.1f}% of Flat performance)\")\n",
    "print(f\"   ‚Ä¢ Build time range: {M_df['build_time'].min():.2f}s - {M_df['build_time'].max():.2f}s\")\n",
    "print(f\"   ‚Ä¢ Latency range: {M_df['median_latency'].min():.2f}ms - {M_df['median_latency'].max():.2f}ms\")\n",
    "print(f\"   ‚Ä¢ Advantage: ‚úÖ Structural improvement, affects graph quality\")\n",
    "\n",
    "print(f\"\\nüí° KEY TAKEAWAYS:\")\n",
    "print(f\"   ‚Ä¢ efSearch has {'LARGER' if efSearch_quality_range > M_quality_range else 'SMALLER'} impact on quality ({efSearch_quality_range:.4f} vs {M_quality_range:.4f})\")\n",
    "print(f\"   ‚Ä¢ efSearch is easier to tune (no rebuild), M requires more computational cost\")\n",
    "print(f\"   ‚Ä¢ Best strategy: Choose M based on quality needs, then tune efSearch for speed/quality balance\")\n",
    "print(f\"   ‚Ä¢ For this dataset: M={optimal_M['M']}, efSearch={optimal_efSearch['efSearch']} is optimal\")\n",
    "print(\"=\"*90)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
